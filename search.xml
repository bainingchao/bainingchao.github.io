<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[笔记10：PostgreSQL学习篇]]></title>
    <url>%2F2019%2F06%2F14%2F%E7%AC%94%E8%AE%B010%EF%BC%9APGSQL%E5%AD%A6%E4%B9%A0%E7%AF%87%2F</url>
    <content type="text"><![CDATA[摘要：日常学习中对一些知识点进行总结得出该系列文章。学习笔记内容包括前端技术，Django web开发技术，数据库技术如MySQL，MongoDB，PGSQL等等。此外还有一些工具如Dock，ES等等。（本文原创，转载必须注明出处.） PGSQLPostgreSQL是一个开源对象关系数据库管理系统(ORDBMS)。包括PostgreSQL语言的所有主题，如创建数据库，创建表，删除数据库，删除表，选择数据库，选择表，插入记录，更新记录，删除记录，触发器，功能，过程，游标等。帮助您更好地了解PostgreSQL语言和使用PostgreSQL数据库。PostgreSQL是跨平台的，可以在许多操作系统上运行，如Linux，FreeBSD，OS X，Solaris和Microsoft Windows等。 1 PGSQL特点 PostgreSQL可在所有主要操作系统(即Linux，UNIX(AIX，BSD，HP-UX，SGI IRIX，Mac OS X，Solaris，Tru64)和Windows等)上运行。 PostgreSQL支持文本，图像，声音和视频，并包括用于C/C++，Java，Perl，Python，Ruby，Tcl和开放数据库连接(ODBC)的编程接口。 PostgreSQL支持SQL的许多功能，例如复杂SQL查询，SQL子选择，外键，触发器，视图，事务，多进程并发控制(MVCC)，流式复制(9.0)，热备(9.0))。 在PostgreSQL中，表可以设置为从“父”表继承其特征。 可以安装多个扩展以向PostgreSQL添加附加功能。 （2）基本操作 创建数据库：CREATE DATABASE database_name; 查看数据库：postgres=# 删除数据库：DROP DATABASE 创建表： 1234567891011121314CREATE TABLE public.student2( id integer NOT NULL, name character(100), subjects character(1), CONSTRAINT student2_pkey PRIMARY KEY (id))WITH ( OIDS=FALSE);ALTER TABLE public.student2 OWNER TO postgres;COMMENT ON TABLE public.student2 IS '这是一个学生信息表2'; 删除表 1234postgres=#postgres=# drop table student2;DROP TABLEpostgres=# 模式(也叫架构)是指定的表集合。 它还可以包含视图，索引，序列，数据类型，运算符和函数。 1CREATE SCHEMA schema_name; 在架构中创建表 12345678910111213-- Table: myschema.tb_test-- DROP TABLE myschema.tb_test;CREATE TABLE myschema.tb_test( id integer, name character(254))WITH ( OIDS=FALSE);ALTER TABLE myschema.tb_test OWNER TO postgres; 使用架构的优点： 模式有助于多用户使用一个数据库，而不会互相干扰。 它将数据库对象组织成逻辑组，使其更易于管理。 可以将第三方模式放入单独的模式中，以避免与其他对象的名称相冲突。 2 查询（1）插入语句 12345678910INSERT INTO TABLE_NAME (column1, column2, column3,...columnN) VALUES (value1, value2, value3,...valueN);INSERT INTO EMPLOYEES( ID, NAME, AGE, ADDRESS, SALARY) VALUES (1, 'Maxsu', 25, '海口市人民大道2880号', 109990.00 ), (2, 'minsu', 25, '广州中山大道 ', 125000.00 ), (3, '李洋', 21, '北京市朝阳区', 185000.00), (4, 'Manisha', 24, 'Mumbai', 65000.00), (5, 'Larry', 21, 'Paris', 85000.00); （2）基本数据库操作查询，修改，删除，添加，order by，分组group，条件查询，and，or，like，not，in，between，全连接，内连接，右连接，左连接等跟mysql数据库操作基本一样 3 高级操作（1）视图 12345CREATE VIEW current_employees AS SELECT NAME, ID, SALARY FROM EMPLOYEES;drop view view_name （2）存储过程 1234567891011CREATE OR REPLACE FUNCTION totalRecords () RETURNS integer AS $total$ declare total integer; BEGIN SELECT count(*) into total FROM EMPLOYEES; RETURN total; END; $total$ LANGUAGE plpgsql;select totalRecords () (3) 触发器，可以使用某个表插入一条信息之后，触发器将记录写进日志表中，配合存储过程调用触发器例子 12CREATE TRIGGER example_trigger AFTER INSERT ON COMPANY FOR EACH ROW EXECUTE PROCEDURE auditlogfunc(); PostgreSQL触发器可用于以下目的： 验证输入数据。 执行业务规则。 为不同文件中新插入的行生成唯一值。 写入其他文件以进行审计跟踪。 从其他文件查询交叉引用目的。 访问系统函数。 将数据复制到不同的文件以实现数据一致性。 使用触发器的优点 它提高了应用程序的开发速度。 因为数据库存储触发器，所以您不必将触发器操作编码到每个数据库应用程序中。 全局执法业务规则。定义触发器一次，然后将其重用于使用数据库的任何应用程序。 更容易维护 如果业务策略发生变化，则只需更改相应的触发程序，而不是每个应用程序。 提高客户/服务器环境的性能。 所有规则在结果返回之前在服务器中运行。 （4）索引 数据库索引的重要特点 索引使用SELECT查询和WHERE子句加速数据输出，但是会减慢使用INSERT和UPDATE语句输入的数据。 您可以在不影响数据的情况下创建或删除索引。 可以通过使用CREATE INDEX语句创建索引，指定创建索引的索引名称和表或列名称。 还可以创建一个唯一索引，类似于唯一约束，该索引防止列或列的组合上有一个索引重复的项。 1234567891011# 单列索引CREATE INDEX index_name ON table_name (column_name);# 多列索引CREATE INDEX multicolumn_index ON EMPLOYEES (name, salary);# 唯一索引CREATE UNIQUE INDEX index_name on table_name (column_name);# 删除索引DROP INDEX index_name; 避免使用索引？ 应该避免在小表上使用索引。 不要为具有频繁，大批量更新或插入操作的表创建索引。 索引不应用于包含大量NULL值的列。 不要在经常操作(修改)的列上创建索引。 （5）UNIONS PostgreSQL UNION子句/运算符用于组合两个或多个SELECT语句的结果，而不返回任何重复的行。要使用UNION，每个SELECT必须具有相同的列数，相同数量的列表达式，相同的数据类型，并且具有相同的顺序，但不一定要相同。 1234567SELECT EMP_ID, NAME, DEPT FROM COMPANY INNER JOIN DEPARTMENT ON COMPANY.ID = DEPARTMENT.EMP_IDUNIONSELECT EMP_ID, NAME, DEPT FROM COMPANY LEFT OUTER JOIN DEPARTMENT ON COMPANY.ID = DEPARTMENT.EMP_ID; UNION ALL运算符用于组合两个SELECT语句(包括重复行)的结果。 （6）修改表 123456789101112131415# 添加新列alter table table_name add column_name datatype;# 删除列alter table table_name drop column_name;# 修改列不为空ALTER TABLE table_name MODIFY column_name datatype NOT NULL;# 添加唯一约束ALTER TABLE table_nameADD CONSTRAINT MyUniqueConstraint UNIQUE(column1, column2...);# 添加主键ALTER TABLE table_nameADD CONSTRAINT MyPrimaryKey PRIMARY KEY (column1, column2...);# 删除主键ALTER TABLE table_nameDROP CONSTRAINT MyPrimaryKey; (7) 事务 以下命令用于控制事务： BEGIN TRANSACTION：开始事务。 COMMIT：保存更改，或者您可以使用END TRANSACTION命令。 ROLLBACK：回滚更改。 事务控制命令仅用于DML命令INSERT，UPDATE和DELETE。 创建表或删除它们时不能使用它们，因为这些操作会在数据库中自动提交。 123BEGIN;DELETE FROM COMPANY WHERE AGE = 25;COMMIT; (8) 锁 锁或独占锁或写锁阻止用户修改行或整个表。 在UPDATE和DELETE修改的行在事务的持续时间内被自动独占锁定。 这将阻止其他用户更改行，直到事务被提交或回退。用户必须等待其他用户当他们都尝试修改同一行时。 如果他们修改不同的行，不需要等待。 SELECT查询不必等待。数据库自动执行锁定。 然而，在某些情况下，必须手动控制锁定。 手动锁定可以通过使用LOCK命令完成。 它允许指定事务的锁类型和范围。 LOCK命令的基本语法如下： 1234567LOCK [ TABLE ]name INlock_modeSQL name：要锁定的现有表的锁名称(可选模式限定)。 如果在表名之前指定了ONLY，则仅该表被锁定 如果未指定ONLY，则表及其所有后代表(如果有)被锁定。 lock_mode：锁模式指定此锁与之冲突的锁。 如果未指定锁定模式，则使用最严格的访问模式ACCESS EXCLUSIVE。 可能的值是：ACCESS SHARE，ROW SHARE，ROW EXCLUSIVE，SHARE UPDATE EXCLUSIVE，SHARE，SHARE ROW EXCLUSIVE，EXCLUSIVE，ACCESS EXCLUSIVE。 （9）自动增长：REAL 1234567CREATE TABLE COMPANY( ID SERIAL PRIMARY KEY, NAME TEXT NOT NULL, AGE INT NOT NULL, ADDRESS CHAR(50), SALARY REAL); 4 Python连接PGSQLPostgreSQL可以使用psycopg2模块与Python集成。sycopg2是用于Python编程语言的PostgreSQL数据库适配器。 psycopg2是非常小，快速，稳定的。 您不需要单独安装此模块，因为默认情况下它会随着Python 2.5.x版本一起发布。 如果还没有在您的机器上安装它，那么可以使用yum命令安装它，如下所示： 123$yum install python-psycopg2Shell 要使用psycopg2模块，必须首先创建一个表示数据库的Connection对象，然后可以选择创建可以帮助您执行所有SQL语句的游标对象。 （1）连接到数据库 123456#!/usr/bin/pythonimport psycopg2conn = psycopg2.connect(database="testdb", user="postgres", password="pass123", host="127.0.0.1", port="5432")print "Opened database successfully" （2）创建表 以下Python程序将用于在先前创建的数据库(testdb)中创建一个表： 123456789101112131415161718#!/usr/bin/pythonimport psycopg2conn = psycopg2.connect(database="testdb", user="postgres", password="pass123", host="127.0.0.1", port="5432")print "Opened database successfully"cur = conn.cursor()cur.execute('''CREATE TABLE COMPANY (ID INT PRIMARY KEY NOT NULL, NAME TEXT NOT NULL, AGE INT NOT NULL, ADDRESS CHAR(50), SALARY REAL);''')print "Table created successfully"conn.commit()conn.close() （3）插入操作 在上述示例中创建的COMPANY表中创建记录： 123456789101112131415#!/usr/bin/pythonimport psycopg2conn = psycopg2.connect(database="testdb", user="postgres", password="pass123", host="127.0.0.1", port="5432")print "Opened database successfully"cur = conn.cursor()cur.execute("INSERT INTO COMPANY (ID,NAME,AGE,ADDRESS,SALARY) \ VALUES (1, 'Paul', 32, 'California', 20000.00 )");conn.commit()print "Records created successfully";conn.close() （4）SELECT操作 以下Python程序显示了如何从上述示例中创建的COMPANY表中获取和显示记录： 12345678910111213141516171819#!/usr/bin/pythonimport psycopg2conn = psycopg2.connect(database="testdb", user="postgres", password="pass123", host="127.0.0.1", port="5432")print "Opened database successfully"cur = conn.cursor()cur.execute("SELECT id, name, address, salary from COMPANY")rows = cur.fetchall()for row in rows: print "ID = ", row[0] print "NAME = ", row[1] print "ADDRESS = ", row[2] print "SALARY = ", row[3], "\n"print "Operation done successfully";conn.close() （5）更新操作 使用UPDATE语句来更新任何记录，然后从COMPANY表中获取并显示更新的记录： 1234567891011121314151617181920212223#!/usr/bin/pythonimport psycopg2conn = psycopg2.connect(database="testdb", user="postgres", password="pass123", host="127.0.0.1", port="5432")print "Opened database successfully"cur = conn.cursor()cur.execute("UPDATE COMPANY set SALARY = 25000.00 where ID=1")conn.commitprint "Total number of rows updated :", cur.rowcountcur.execute("SELECT id, name, address, salary from COMPANY")rows = cur.fetchall()for row in rows: print "ID = ", row[0] print "NAME = ", row[1] print "ADDRESS = ", row[2] print "SALARY = ", row[3], "\n"print "Operation done successfully";conn.close() （6）删除操作 使用DELETE语句来删除记录，然后从COMPANY表中获取并显示剩余的记录： 1234567891011121314151617181920212223#!/usr/bin/pythonimport psycopg2conn = psycopg2.connect(database="testdb", user="postgres", password="pass123", host="127.0.0.1", port="5432")print "Opened database successfully"cur = conn.cursor()cur.execute("DELETE from COMPANY where ID=2;")conn.commitprint "Total number of rows deleted :", cur.rowcountcur.execute("SELECT id, name, address, salary from COMPANY")rows = cur.fetchall()for row in rows: print "ID = ", row[0] print "NAME = ", row[1] print "ADDRESS = ", row[2] print "SALARY = ", row[3], "\n"print "Operation done successfully";conn.close() 技术交流共享QQ群【机器学习和自然语言QQ群：436303759】： 机器学习和自然语言（QQ群号：436303759）是一个研究深度学习、机器学习、自然语言处理、数据挖掘、图像处理、目标检测、数据科学等AI相关领域的技术群。其宗旨是纯粹的AI技术圈子、绿色的交流环境。本群禁止有违背法律法规和道德的言谈举止。群成员备注格式：城市-自命名。微信订阅号：datathinks]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>PGSQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[笔记9:Django提升篇]]></title>
    <url>%2F2019%2F06%2F13%2F%E7%AC%94%E8%AE%B09-Django%E6%8F%90%E5%8D%87%E7%AF%87%2F</url>
    <content type="text"><![CDATA[摘要：日常学习中对一些知识点进行总结得出该系列文章。学习笔记内容包括前端技术，Django web开发技术，数据库技术如MySQL，MongoDB，PGSQL等等。此外还有一些工具如Dock，ES等等。（本文原创，转载必须注明出处.） （1）打包应用程序：可重用性 打包 Python 程序需要工具：setuptools 。打包时候建议使用django-appname 1 在你的 Django 项目目录外创建一个名为 django-polls 的文件夹，用于盛放 polls。 2 将 polls 目录移入 django-polls 目录。 3 创建一个名为 django-polls/README.rst 的文件，包含以下内容： 1234567891011121314151617181920212223242526272829=====Polls=====Polls is a simple Django app to conduct Web-based polls. For eachquestion, visitors can choose between a fixed number of answers.Detailed documentation is in the "docs" directory.Quick start-----------1. Add "polls" to your INSTALLED_APPS setting like this:: INSTALLED_APPS = [ ... 'polls', ]2. Include the polls URLconf in your project urls.py like this:: path('polls/', include('polls.urls')),3. Run `python manage.py migrate` to create the polls models.4. Start the development server and visit http://127.0.0.1:8000/admin/ to create a poll (you'll need the Admin app enabled).5. Visit http://127.0.0.1:8000/polls/ to participate in the poll. 4 创建一个 django-polls/LICENSE 文件。选择一个非本教程使用的授权协议，但是要足以说明发布代码没有授权证书是 不可能的 。Django 和很多兼容 Django 的应用是以 BSD 授权协议发布的；不过，你可以自己选择一个授权协议。只要确定你选择的协议能够限制未来会使用你的代码的人。 下一步我们将创建 setup.py 用于说明如何构建和安装应用的细节。关于此文件的完整介绍超出了此教程的范围，但是 setuptools docs 有详细的介绍。创建文件 django-polls/setup.py 包含以下内容： 12345678910111213141516171819202122232425262728293031323334import osfrom setuptools import find_packages, setupwith open(os.path.join(os.path.dirname(__file__), 'README.rst')) as readme: README = readme.read()# allow setup.py to be run from any pathos.chdir(os.path.normpath(os.path.join(os.path.abspath(__file__), os.pardir)))setup( name='django-polls', version='0.1', packages=find_packages(), include_package_data=True, license='BSD License', # example license description='A simple Django app to conduct Web-based polls.', long_description=README, url='https://www.example.com/', author='Your Name', author_email='yourname@example.com', classifiers=[ 'Environment :: Web Environment', 'Framework :: Django', 'Framework :: Django :: X.Y', # replace "X.Y" as appropriate 'Intended Audience :: Developers', 'License :: OSI Approved :: BSD License', # example license 'Operating System :: OS Independent', 'Programming Language :: Python', 'Programming Language :: Python :: 3.5', 'Programming Language :: Python :: 3.6', 'Topic :: Internet :: WWW/HTTP', 'Topic :: Internet :: WWW/HTTP :: Dynamic Content', ],) 6 默认包中只包含 Python 模块和包。为了包含额外文件，我们需要创建一个名为 MANIFEST.in 的文件。上一步中关于 setuptools 的文档详细介绍了这个文件。为了包含模板、README.rst 和我们的 LICENSE 文件，创建文件 django-polls/MANIFEST.in 包含以下内容： 1234include LICENSEinclude README.rstrecursive-include polls/static *recursive-include polls/templates * 7 在应用中包含详细文档是可选的，但我们推荐你这样做。创建一个空目录 django-polls/docs 用于未来编写文档。额外添加一行至 django-polls/MANIFEST.in 1recursive-include docs * 8 试着构建你自己的应用包通过 ptyhon setup.py sdist （在 django-polls``目录内）。这将创建一个名为 ``dist 的目录并构建你自己的应用包， django-polls-0.1.tar.gz。 1C:\Users\Administrator\Desktop\django-polls&gt; python setup.py sdist （2）使用自己的包名 由于我们把 polls 目录移出了项目，所以它无法工作了。我们现在要通过安装我们的新 django-polls 应用来修复这个问题。 1C:\Users\Administrator\Desktop\django-polls\dist&gt;pip install --user django-polls-0.1.tar.gz （3）发布应用，可以邮件，github等等方式上传 （4）模型 字段类型详解：&lt;https://docs.djangoproject.com/zh-hans/2.2/ref/models/fields/#model-field-types&gt; （5）查询 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859&gt;&gt;&gt; from blog.models import Blog&gt;&gt;&gt; b = Blog(name='Beatles Blog', tagline='All the latest Beatles news.')&gt;&gt;&gt; b.save()&gt;&gt;&gt; from blog.models import Author&gt;&gt;&gt; joe = Author.objects.create(name="Joe")&gt;&gt;&gt; entry.authors.add(joe)&gt;&gt;&gt; all_entries = Entry.objects.all()Entry.objects.filter(pub_date__year=2006)Entry.objects.all().filter(pub_date__year=2006)&gt;&gt;&gt; q1 = Entry.objects.filter(headline__startswith="What")&gt;&gt;&gt; q2 = q1.exclude(pub_date__gte=datetime.date.today())&gt;&gt;&gt; q3 = q1.filter(pub_date__gte=datetime.date.today())&gt;&gt;&gt; one_entry = Entry.objects.get(pk=1)&gt;&gt;&gt; Entry.objects.all()[5:10] # OFFSET 5 LIMIT 5&gt;&gt;&gt; Entry.objects.order_by('headline')[0]SELECT * FROM blog_entry WHERE pub_date &lt;= '2006-01-01';Entry.objects.get(headline__exact="Cat bites dog")&gt;&gt;&gt; Blog.objects.get(name__iexact="beatles blog")Entry.objects.get(headline__contains='Lennon')SELECT ... WHERE headline LIKE '%Lennon%';Blog.objects.filter(entry__authors__isnull=False, entry__authors__name__isnull=True)Blog.objects.filter(entry__headline__contains='Lennon', entry__pub_date__year=2008)&gt;&gt;&gt; Entry.objects.filter(mod_date__gt=F('pub_date') + timedelta(days=3))&gt;&gt;&gt; print([e.headline for e in Entry.objects.all()])SELECT * from polls WHERE question LIKE 'Who%' AND (pub_date = '2005-05-02' OR pub_date = '2005-05-06')&gt;&gt;&gt; e.delete()&gt;&gt;&gt; Entry.objects.filter(pub_date__year=2005).delete()e = Entry.objects.get(id=3)e.authors.all() # Returns all Author objects for this Entry.e.authors.count()e.authors.filter(name__contains='John')a = Author.objects.get(id=5)a.entry_set.all() # Returns all Entry objects for this Author.e = Entry.objects.get(id=2)e.entrydetail # returns the related EntryDetail objecte.entrydetail = ed （5）聚合 123456789101112131415161718192021from django.db import modelsclass Author(models.Model): name = models.CharField(max_length=100) age = models.IntegerField()class Publisher(models.Model): name = models.CharField(max_length=300)class Book(models.Model): name = models.CharField(max_length=300) pages = models.IntegerField() price = models.DecimalField(max_digits=10, decimal_places=2) rating = models.FloatField() authors = models.ManyToManyField(Author) publisher = models.ForeignKey(Publisher, on_delete=models.CASCADE) pubdate = models.DateField()class Store(models.Model): name = models.CharField(max_length=300) books = models.ManyToManyField(Book) 123456789101112131415161718192021# Total number of books.&gt;&gt;&gt; Book.objects.count()&gt;&gt;&gt; Book.objects.filter(publisher__name='BaloneyPress').count()# Average price across all books.&gt;&gt;&gt; from django.db.models import Avg, Max, Min&gt;&gt;&gt; Book.objects.aggregate(Avg('price'), Max('price'), Min('price'))# Each publisher, each with a count of books as a "num_books" attribute.&gt;&gt;&gt; pubs = Publisher.objects.annotate(num_books=Count('book'))# The top 5 publishers, in order by number of books.&gt;&gt;&gt; pubs = Publisher.objects.annotate(num_books=Count('book')).order_by('-num_books')[:5]&gt;&gt;&gt; pubs[0].num_books&gt;&gt;&gt; Book.objects.filter(name__startswith="Django").annotate(num_authors=Count('authors'))&gt;&gt;&gt; Book.objects.annotate(num_authors=Count('authors')).filter(num_authors__gt=1)&gt;&gt;&gt; Book.objects.annotate(num_authors=Count('authors')).order_by('num_authors') （6）搜索 12345&gt;&gt;&gt; Entry.objects.filter(body_text__search='cheese')&gt;&gt;&gt; Entry.objects.annotate(... search=SearchVector('blog__tagline', 'body_text'),... ).filter(search='cheese') （7）Manager 12345678910111213141516171819202122232425262728from django.db import modelsclass PollManager(models.Manager): def with_counts(self): from django.db import connection with connection.cursor() as cursor: cursor.execute(""" SELECT p.id, p.question, p.poll_date, COUNT(*) FROM polls_opinionpoll p, polls_response r WHERE p.id = r.poll_id GROUP BY p.id, p.question, p.poll_date ORDER BY p.poll_date DESC""") result_list = [] for row in cursor.fetchall(): p = self.model(id=row[0], question=row[1], poll_date=row[2]) p.num_responses = row[3] result_list.append(p) return result_listclass OpinionPoll(models.Model): question = models.CharField(max_length=200) poll_date = models.DateField() objects = PollManager()class Response(models.Model): poll = models.ForeignKey(OpinionPoll, on_delete=models.CASCADE) person_name = models.CharField(max_length=50) response = models.TextField() 1234567891011class ExtraManager(models.Model): extra_manager = OtherManager() class Meta: abstract = Trueclass ChildC(AbstractBase, ExtraManager): # ... # Default manager is CustomManager, but OtherManager is # also available via the "extra_manager" attribute. pass （8）raw SQL queries 1234class Person(models.Model): first_name = models.CharField(...) last_name = models.CharField(...) birth_date = models.DateField(...) 1234567891011121314151617181920212223242526272829303132333435&gt;&gt;&gt; for p in Person.objects.raw('SELECT * FROM myapp_person'):... print(p)&gt;&gt;&gt; Person.objects.raw('SELECT last_name, birth_date, first_name, id FROM myapp_person')&gt;&gt;&gt; name_map = &#123;'first': 'first_name', 'last': 'last_name', 'bd': 'birth_date', 'pk': 'id'&#125;&gt;&gt;&gt; Person.objects.raw('SELECT * FROM some_other_table', translations=name_map)&gt;&gt;&gt; first_person = Person.objects.raw('SELECT * FROM myapp_person LIMIT 1')[0]&gt;&gt;&gt; people = Person.objects.raw('SELECT *, age(birth_date) AS age FROM myapp_person')&gt;&gt;&gt; for p in people:... print("%s is %s." % (p.first_name, p.age))&gt;&gt;&gt; lname = 'Doe'&gt;&gt;&gt; Person.objects.raw('SELECT * FROM myapp_person WHERE last_name = %s', [lname])from django.db import connectiondef my_custom_sql(self): with connection.cursor() as cursor: cursor.execute("UPDATE bar SET foo = 1 WHERE baz = %s", [self.baz]) cursor.execute("SELECT foo FROM bar WHERE baz = %s", [self.baz]) row = cursor.fetchone() return rowcursor.execute("SELECT foo FROM bar WHERE baz = '30%%' AND id = %s", [self.id])&gt;&gt;&gt; cursor.fetchall()with connection.cursor() as c: c.execute(...)with connection.cursor() as cursor: cursor.callproc('test_procedure', [1, 'test']) (9) 数据事务 123456a.save() # Succeeds, but may be undone by transaction rollbacktry: b.save() # Could throw exceptionexcept IntegrityError: transaction.rollback()c.save() # Succeeds, but a.save() may have been undone （10）多数据库 1234567891011121314DATABASES = &#123; 'default': &#123; 'NAME': 'app_data', 'ENGINE': 'django.db.backends.postgresql', 'USER': 'postgres_user', 'PASSWORD': 's3krit' &#125;, 'users': &#123; 'NAME': 'user_data', 'ENGINE': 'django.db.backends.mysql', 'USER': 'mysql_user', 'PASSWORD': 'priv4te' &#125;&#125; 123$ ./manage.py migrate --database=users$ ./manage.py migrate --database=users$ ./manage.py migrate --database=customers 1234567891011121314from django.contrib import admin# Specialize the multi-db admin objects for use with specific models.class BookInline(MultiDBTabularInline): model = Bookclass PublisherAdmin(MultiDBModelAdmin): inlines = [BookInline]admin.site.register(Author, MultiDBModelAdmin)admin.site.register(Publisher, PublisherAdmin)othersite = admin.AdminSite('othersite')othersite.register(Publisher, MultiDBModelAdmin) 123from django.db import connectionswith connections['my_db_alias'].cursor() as cursor: ... (11) URL调度器 123456789from django.urls import pathfrom . import viewsurlpatterns = [ path('articles/2003/', views.special_case_2003), path('articles/&lt;int:year&gt;/', views.year_archive), path('articles/&lt;int:year&gt;/&lt;int:month&gt;/', views.month_archive), path('articles/&lt;int:year&gt;/&lt;int:month&gt;/&lt;slug:slug&gt;/', views.article_detail),] 123456789from django.urls import path, re_pathfrom . import viewsurlpatterns = [ path('articles/2003/', views.special_case_2003), re_path(r'^articles/(?P&lt;year&gt;[0-9]&#123;4&#125;)/$', views.year_archive), re_path(r'^articles/(?P&lt;year&gt;[0-9]&#123;4&#125;)/(?P&lt;month&gt;[0-9]&#123;2&#125;)/$', views.month_archive), re_path(r'^articles/(?P&lt;year&gt;[0-9]&#123;4&#125;)/(?P&lt;month&gt;[0-9]&#123;2&#125;)/(?P&lt;slug&gt;[\w-]+)/$', views.article_detail),] 123456from django.urls import re_pathurlpatterns = [ re_path(r'^blog/(page-(\d+)/)?$', blog_articles), # bad re_path(r'^comments/(?:page-(?P&lt;page_number&gt;\d+)/)?$', comments), # good] 12345678from django.urls import include, pathurlpatterns = [ # ... snip ... path('community/', include('aggregator.urls')), path('contact/', include('contact.urls')), # ... snip ...] 123456789101112131415# In settings/urls/main.pyfrom django.urls import include, pathurlpatterns = [ path('&lt;username&gt;/blog/', include('foo.urls.blog')),]# In foo/urls/blog.pyfrom django.urls import pathfrom . import viewsurlpatterns = [ path('', views.blog.index), path('archive/', views.blog.archive),] 示例 12345678from django.urls import pathfrom . import viewsurlpatterns = [ #... path('articles/&lt;int:year&gt;/', views.year_archive, name='news-year-archive'), #...] 1234567&lt;a href="&#123;% url 'news-year-archive' 2012 %&#125;"&gt;2012 Archive&lt;/a&gt;&#123;# Or with the year in a template context variable: #&#125;&lt;ul&gt;&#123;% for yearvar in year_list %&#125;&lt;li&gt;&lt;a href="&#123;% url 'news-year-archive' yearvar %&#125;"&gt;&#123;&#123; yearvar &#125;&#125; Archive&lt;/a&gt;&lt;/li&gt;&#123;% endfor %&#125;&lt;/ul&gt; 12345678from django.http import HttpResponseRedirectfrom django.urls import reversedef redirect_to_year(request): # ... year = 2006 # ... return HttpResponseRedirect(reverse('news-year-archive', args=(year,))) （12）文件上传 forms.py 12345from django import formsclass UploadFileForm(forms.Form): title = forms.CharField(max_length=50) file = forms.FileField() views.py 1234567891011121314from django.http import HttpResponseRedirectfrom django.shortcuts import renderfrom .forms import ModelFormWithFileFielddef upload_file(request): if request.method == 'POST': form = ModelFormWithFileField(request.POST, request.FILES) if form.is_valid(): # file is saved form.save() return HttpResponseRedirect('/success/url/') else: form = ModelFormWithFileField() return render(request, 'upload.html', &#123;'form': form&#125;) 1234def handle_uploaded_file(f): with open('some/file/name.txt', 'wb+') as destination: for chunk in f.chunks(): destination.write(chunk) 使用 UploadedFile.chunks() 而不是 read() 是为了确保即使是大文件又不会将我们系统的内存占满。 （13）快捷键函数 render(request, template_name, context=None**, content_type=None, status=None,** using=None) 1234567from django.shortcuts import renderdef my_view(request): # View code here... return render(request, 'myapp/index.html', &#123; 'foo': 'bar', &#125;, content_type='application/xhtml+xml') redirect(to, args, permanent=False**, *kwargs*) 123456from django.shortcuts import redirectdef my_view(request): ... obj = MyModel.objects.get(...) return redirect(obj) （14）中间件 中间件是 Django 请求/响应处理的钩子框架。它是一个轻量级的、低级的“插件”系统，用于全局改变 Django 的输入或输出。 中间件工厂是一个可调用的程序，它接受 get_response 可调用并返回中间件。中间件是可调用的，它接受请求并返回响应，就像视图一样。 123456789101112131415def simple_middleware(get_response): # One-time configuration and initialization. def middleware(request): # Code to be executed for each request before # the view (and later middleware) are called. response = get_response(request) # Code to be executed for each request/response after # the view is called. return response return middleware 或者它可以写成一个类，它的实例是可调用的，如下： 123456789101112131415class SimpleMiddleware: def __init__(self, get_response): self.get_response = get_response # One-time configuration and initialization. def __call__(self, request): # Code to be executed for each request before # the view (and later middleware) are called. response = self.get_response(request) # Code to be executed for each request/response after # the view is called. return response 中间件工厂必须接受 get_response 参数。还可以初始化中间件的一些全局状态。记住两个注意事项： Django仅用 get_response 参数初始化您的中间件，因此不能定义 __init__()，因为需要其他参数。 与每次请求调用 __call__() 方法不同，当 Web 服务器启动时，__init__() 只被称为一次。 激活中间件 若要激活中间件组件，请将其添加到 Django 设置中的 MIDDLEWARE 列表中。在 MIDDLEWARE 中，每个中间件组件由字符串表示：指向中间件工厂的类或函数名的完整 Python 路径。例如，这里创建的默认值是 django-admin startproject： 123456789MIDDLEWARE = [ 'django.middleware.security.SecurityMiddleware', 'django.contrib.sessions.middleware.SessionMiddleware', 'django.middleware.common.CommonMiddleware', 'django.middleware.csrf.CsrfViewMiddleware', 'django.contrib.auth.middleware.AuthenticationMiddleware', 'django.contrib.messages.middleware.MessageMiddleware', 'django.middleware.clickjacking.XFrameOptionsMiddleware',] MIDDLEWARE 的顺序很重要，因为中间件会依赖其他中间件。例如：类 AuthenticationMiddleware 在会话中存储经过身份验证的用户；因此，它必须在 SessionMiddleware 后面运行 。中间件。Session中间件。请参阅 Middleware ordering ，用于一些关于 Django 中间件类排序的常见提示。 (15) 会话 1234567def post_comment(request, new_comment): if request.session.get('has_commented', False): return HttpResponse("You've already commented.") c = comments.Comment(comment=new_comment) c.save() request.session['has_commented'] = True return HttpResponse('Thanks for your comment!') This simplistic view logs in a “member” of the site: 1234567def login(request): m = Member.objects.get(username=request.POST['username']) if m.password == request.POST['password']: request.session['member_id'] = m.id return HttpResponse("You're logged in.") else: return HttpResponse("Your username and password didn't match.") …And this one logs a member out, according to login() above: 123456def logout(request): try: del request.session['member_id'] except KeyError: pass return HttpResponse("You're logged out.") （16）表单 16.1 Get与POST 处理表单时只会用到 GET 和 POST 两种HTTP方法。Django的登录表单使用 POST 方法传输数据，在这个方法中浏览器会封装表单数据，为了传输会进行编码，然后发送到服务端并接收它的响应。相比之下，GET 方法将提交的数据捆绑到一个字符串中，并用它来组成一个URL。该URL包含了数据要发送的地址以及一些键值对应的数据。如果您在Django文档中进行一次搜索，就会看到这点，它会生成一个形似 https://docs.djangoproject.com/search/?q=forms&amp;release=1 的URL。 GET 和 POST 通常用于不同的目的。 任何可能用于更改系统状态的请求应该使用 POST —— 比如一个更改数据库的请求。GET 应该只被用于不会影响系统状态的请求。 GET 方法也不适合密码表单，因为密码会出现在URL中，于是也会出现在浏览器的历史记录以及服务器的日志中，而且都是以纯文本的形式。它也不适合处理大量的数据或者二进制数据，比如一张图片。在WEB应用的管理表单中使用 GET 请求具有安全隐患：攻击者很容易通过模拟请求来访问系统的敏感数据。 POST 方法通过与其他像Django的 CSRF protection 这样的保护措施配合使用，能对访问提供更多的控制。另一方面， GET 方法适用于诸如网页搜索表单这样的内容，因为这类呈现为一个 GET请求的URL很容易被存为书签、分享或重新提交。 12345&lt;form action="/your-name/" method="post"&gt; &lt;label for="your_name"&gt;Your name: &lt;/label&gt; &lt;input id="your_name" type="text" name="your_name" value="&#123;&#123; current_name &#125;&#125;"&gt; &lt;input type="submit" value="OK"&gt;&lt;/form&gt; forms.py 1234from django import formsclass NameForm(forms.Form): your_name = forms.CharField(label='Your name', max_length=100) views.py 12345678910111213141516171819202122from django.http import HttpResponseRedirectfrom django.shortcuts import renderfrom .forms import NameFormdef get_name(request): # if this is a POST request we need to process the form data if request.method == 'POST': # create a form instance and populate it with data from the request: form = NameForm(request.POST) # check whether it's valid: if form.is_valid(): # process the data in form.cleaned_data as required # ... # redirect to a new URL: return HttpResponseRedirect('/thanks/') # if a GET (or any other method) we'll create a blank form else: form = NameForm() return render(request, 'name.html', &#123;'form': form&#125;) 视图将再次创建一个表单实例并使用请求中的数据填充它： form = NameForm(request.POST) 这叫“绑定数据到表单” （现在它是一张 绑定的 表单）。调用表单的 is_valid() 方法；如果不为 True ，我们带着表单返回到模板。这次表单不再为空（ 未绑定 ），所以HTML表单将用之前提交的数据进行填充，放到可以根据需要进行编辑和修正的位置。如果 is_valid() 为 True ，我们就能在其 cleaned_data 属性中找到所有通过验证的表单数据。我们可以在发送一个HTTP重定向告诉浏览器下一步去向之前用这些数据更新数据库或者做其他处理。 name.html 12345&lt;form action="/your-name/" method="post"&gt; &#123;% csrf_token %&#125; &#123;&#123; form &#125;&#125; &lt;input type="submit" value="Submit"&gt;&lt;/form&gt; 16.2 详解Django Form类 forms.py 1234567from django import formsclass ContactForm(forms.Form): subject = forms.CharField(max_length=100) message = forms.CharField(widget=forms.Textarea) sender = forms.EmailField() cc_myself = forms.BooleanField(required=False) views.py 1234567891011121314from django.core.mail import send_mailif form.is_valid(): subject = form.cleaned_data['subject'] message = form.cleaned_data['message'] sender = form.cleaned_data['sender'] cc_myself = form.cleaned_data['cc_myself'] recipients = ['info@example.com'] if cc_myself: recipients.append(sender) send_mail(subject, message, sender, recipients) return HttpResponseRedirect('/thanks/') 12345678&lt;p&gt;&lt;label for="id_subject"&gt;Subject:&lt;/label&gt; &lt;input id="id_subject" type="text" name="subject" maxlength="100" required&gt;&lt;/p&gt;&lt;p&gt;&lt;label for="id_message"&gt;Message:&lt;/label&gt; &lt;textarea name="message" id="id_message" required&gt;&lt;/textarea&gt;&lt;/p&gt;&lt;p&gt;&lt;label for="id_sender"&gt;Sender:&lt;/label&gt; &lt;input type="email" name="sender" id="id_sender" required&gt;&lt;/p&gt;&lt;p&gt;&lt;label for="id_cc_myself"&gt;Cc myself:&lt;/label&gt; &lt;input type="checkbox" name="cc_myself" id="id_cc_myself"&gt;&lt;/p&gt; （17）用户认证 Django 自带一个用户验证系统。它负责处理用户账号、组、权限和基于cookie的用户会话。文档的这部分解释了默认的实现如何开箱即用，以及如何扩展和自定义以满足你的项目需求。 （18）验证系统 创建用户 12345678&gt;&gt;&gt; from django.contrib.auth.models import User&gt;&gt;&gt; user = User.objects.create_user('john', 'lennon@thebeatles.com', 'johnpassword')# At this point, user is a User object that has already been saved# to the database. You can continue to change its attributes# if you want to change other fields.&gt;&gt;&gt; user.last_name = 'Lennon'&gt;&gt;&gt; user.save() 创建超级用户 1$ python manage.py createsuperuser --username=joe --email=joe@example.com 更改密码 1234&gt;&gt;&gt; from django.contrib.auth.models import User&gt;&gt;&gt; u = User.objects.get(username='john')&gt;&gt;&gt; u.set_password('new password')&gt;&gt;&gt; u.save() 验证用户 123456from django.contrib.auth import authenticateuser = authenticate(username='john', password='secret')if user is not None: # A backend authenticated the credentialselse: # No backend authenticated the credentials 登出 12345from django.contrib.auth import logoutdef logout_view(request): logout(request) # Redirect to a success page. 限制对未登录用户的访问：装饰器 12345from django.contrib.auth.decorators import login_required@login_requireddef my_view(request): ... （19）Django缓存框架 动态网站存在一个基本权衡是——它们是动态的。每次用户请求一个页面，web 服务器需要提供各种各样的计算——从数据库查询到模板渲染再到业务逻辑——最后建立页面呈现给用户。从处理开销的角度来看，这比标准读取文件系统服务安排的开销要高得多。下面是一些伪代码解释了动态网站生成页面的时候，缓存是怎么工作的： 1234567given a URL, try finding that page in the cacheif the page is in the cache: return the cached pageelse: generate the page save the generated page in the cache (for next time) return the generated page 19.1 Memcached内存缓存 Memcached 是一个完全基于内存的缓存服务器，是 Django 原生支持的最快、最高效的缓存类型，最初被开发出来用于处理 LiveJournal.com 的高负载，随后由 Danga Interactive 开源。Facebook 和 Wikipedia 等网站使用它来减少数据库访问并显著提高网站性能。 Memcached 以一个守护进程的形式运行，并且被分配了指定数量的 RAM。它所做的就是提供一个快速接口用于在缓存中添加，检索和删除数据。所有数据都直接存储在内存中，因此不会产生数据库或文件系统使用的开销。 在安装 Memcached 本身后，你还需要安装一个 Memcached 绑定。有许多可用的 Python Memcached 绑定，最常见的两个是 python-memcached 和pylibmc 在 Django 中使用 Memcached ： 将 BACKEND 设置为 django.core.cache.backends.memcached.MemcachedCache 或者 django.core.cache.backends.memcached.PyLibMCCache （取决于你所选择的 memcached 绑定） 将 LOCATION设置为 ip:port 值，其中 ip 是 Memcached 守护进程的 IP 地址，port 是运行 Memcached 的端口；或者设置为一个 unix:path 值，其中 path 是 Memcached Unix 套接字文件的路径。 在这个示例中，Memcached 使用 python-memcached 绑定，在 localhost (127.0.0.1) 端口 11211 上运行： 123456CACHES = &#123; 'default': &#123; 'BACKEND': 'django.core.cache.backends.memcached.MemcachedCache', 'LOCATION': '127.0.0.1:11211', &#125;&#125; 在这个示例中， Memcached 可通过本地 Unix 套接字文件 /tmp/memcached.sock 使用 python-memcached 绑定得到： 123456CACHES = &#123; 'default': &#123; 'BACKEND': 'django.core.cache.backends.memcached.MemcachedCache', 'LOCATION': 'unix:/tmp/memcached.sock', &#125;&#125; 当使用 pylibmc 绑定时，不要包含 unix:/ 前缀： 123456CACHES = &#123; 'default': &#123; 'BACKEND': 'django.core.cache.backends.memcached.PyLibMCCache', 'LOCATION': '/tmp/memcached.sock', &#125;&#125; Memcached 的一个出色功能是它能够在多个服务器上共享缓存。这意味着您可以在多台计算机上运行 Memcached 守护程序，程序会视这组计算机为单个缓存，而无需在每台机器上复制缓存值。要使用此功能，需要在 LOCATION 中包含所有服务器的地址，可以是分号或者逗号分隔的字符串，也可以是一个列表。 在这个示例中，缓存通过端口 11211 的 IP 地址 172.19.26.240 、 172.19.26.242 运行的 Memcached 实例共享： 123456789CACHES = &#123; 'default': &#123; 'BACKEND': 'django.core.cache.backends.memcached.MemcachedCache', 'LOCATION': [ '172.19.26.240:11211', '172.19.26.242:11211', ] &#125;&#125; 在以下示例中，缓存通过在 IP 地址 172.19.26.240（端口号 11211），172.19.26.242（端口号 11212）和 172.19.26.244（端口号 11213）上运行的 Memcached 实例共享： 12345678910CACHES = &#123; 'default': &#123; 'BACKEND': 'django.core.cache.backends.memcached.MemcachedCache', 'LOCATION': [ '172.19.26.240:11211', '172.19.26.242:11212', '172.19.26.244:11213', ] &#125;&#125; 关于 Memcached 的最后一点是，基于内存的缓存有一个缺点：因为缓存的数据存储在内存中，如果服务器崩溃，那么数据将会丢失。显然，内存不适用于持久数据存储，因此不要依赖基于内存的缓存作为你唯一的数据存储。毫无疑问，没有任何 Django 缓存后端应该被用于持久存储——它们都是适用于缓存的解决方案，而不是存储——我们在这里指出这一点是因为基于内存的缓存是格外临时的。 19.2 数据库缓存 Django 可以在数据库中存储缓存数据。如果你有一个快速、索引正常的数据库服务器，这种缓存效果最好。 用数据库表作为你的缓存后端： 将 BACKEND 设置为 django.core.cache.backends.db.DatabaseCache 将 LOCATION 设置为 数据库表的表名。这个表名可以是没有使用过的任何符合要求的名称。 在这个例子中，缓存表的名称是 my_cache_table ： 123456CACHES = &#123; 'default': &#123; 'BACKEND': 'django.core.cache.backends.db.DatabaseCache', 'LOCATION': 'my_cache_table', &#125;&#125; 创建缓存表 使用数据库缓存之前，必须通过下面的命令创建缓存表： 1python manage.py createcachetable 这将在数据库中创建一个表，该表的格式与 Django 数据库缓存系统期望的一致。该表的表名取自 LOCATION 如果你正在使用多数据库缓存， createcachetable 会对每个缓存创建一个表。 如果你正在使用多数据库， createcachetable 将遵循数据库路由的 allow_migrate() 方法。 像 migrate 一样， createcachetable 不会影响已经存在的表，它只创建缺失的表。 要打印即将运行的 SQL，而不是运行它，请使用 createcachetable --dry-run 选项。 多数据库 如果在多数据库中使用缓存，你也需要设置数据库缓存表的路由指令。因为路由的原因，数据库缓存表在 django_cache 应用程序中显示为 CacheEntry 的模型名。这个模型不会出现在模型缓存中，但模型详情可用于路由目的。 比如，下面的路由可以将所有缓存读取操作指向 cache_replica ，并且所有的写操作指向 cache_primary。缓存表将会只同步到 cache_primary。 1234567891011121314151617181920class CacheRouter: """A router to control all database cache operations""" def db_for_read(self, model, **hints): "All cache read operations go to the replica" if model._meta.app_label == 'django_cache': return 'cache_replica' return None def db_for_write(self, model, **hints): "All cache write operations go to primary" if model._meta.app_label == 'django_cache': return 'cache_primary' return None def allow_migrate(self, db, app_label, model_name=None, **hints): "Only install the cache model on primary" if app_label == 'django_cache': return db == 'cache_primary' return None 如果你没有指定路由指向数据库缓存模型，缓存后端将使用 默认 的数据库。当然，如果没使用数据库缓存后端，则无需担心为数据库缓存模型提供路由指令。 19.3 文件系统缓存 基于文件的后端序列化并保存每个缓存值作为单独的文件。要使用此后端，可将 BACKEND 设置为 &quot;django.core.cache.backends.filebased.FileBasedCache&quot; 并将 LOCATION 设置为一个合适的路径。比如，在 /var/tmp/django_cache 存储缓存数据，使用以下配置： 123456CACHES = &#123; 'default': &#123; 'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache', 'LOCATION': '/var/tmp/django_cache', &#125;&#125; 如果使用 Windows 系统，将驱动器号放在路径开头，如下： 123456CACHES = &#123; 'default': &#123; 'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache', 'LOCATION': 'c:/foo/bar', &#125;&#125; 目录路径应该是绝对路径——因此，它应该以文件系统根目录开始。无需担心是否需要以斜杠结尾。 确保这个配置指向的目录存在，并且可由运行 Web 服务器的系统用户读写。继续上面的例子，如果服务器被用户 apache 运行，确保目录 /var/tmp/django_cache 存在并且可被用户 apache 读写。 19.4 本地内存缓存 如果在配置文件中没有指定缓存，那么将默认使用本地内存缓存。如果你想要内存缓存的速度优势，但又没有条件使用 Memcached，那么可以考虑本地内存缓存后端。 123456CACHES = &#123; 'default': &#123; 'BACKEND': 'django.core.cache.backends.locmem.LocMemCache', 'LOCATION': 'unique-snowflake', &#125;&#125; LOCATION 被用于标识各个内存存储。如果只有一个 locmem 缓存，你可以忽略 LOCATION 。但是如果你有多个本地内存缓存，那么你至少要为其中一个起个名字，以便将它们区分开。这种缓存使用最近最少使用（LRU）的淘汰策略。 注意，每个进程将有它们自己的私有缓存实例，这意味着不存在跨进程的缓存。这也同样意味着本地内存缓存不是特别节省内存，因此它或许不是生成环境的好选择，不过它在开发环境中表现很好。 19.5 虚拟缓存（用于开发模式） Django 带有一个实际上不是缓存的 “虚拟” 缓存，它只是实现缓存接口，并不做其他操作。如果你有一个正式网站在不同地方使用了重型缓存，但你不想在开发环境使用缓存，而且不想为这个特殊场景而修改代码的时候，这将非常有用。要激活虚拟缓存，像这样设置 BACKEND 。 12345CACHES = &#123; 'default': &#123; 'BACKEND': 'django.core.cache.backends.dummy.DummyCache', &#125;&#125; 19.6 使用自定义的缓存后台 虽然 Django 自带一些缓存后端，但有时你也想使用自定义的缓存后端。当使用第三方缓存后端时，使用 Python 导入路径作为 Cache 设置的后端，像这样： 12345CACHES = &#123; 'default': &#123; 'BACKEND': 'path.to.backend', &#125;&#125; 如果你正在创建自己的后端，你可以使用标准缓存作为参考实现。你在 Django 源代码的 django/core/cache/backends/ 目录找到代码。 注意：除非是令人信服的理由，诸如服务器不支持缓存，否则你应该使用 Django 附带的缓存后端。他们经过了良好的测试并易于使用。 19.7 缓存参数 每个缓存后端可以通过额外的参数来控制缓存行为。这些参数在 CACHES 设置中作为附加键提供。有效参数如下： 缓存:setting:TIMEOUT ：用于缓存的默认超时时间（以秒为单位）。这个参数默认为 300 秒（5分钟）。你可以设置 TIMEOUT 为 None，因此，默认情况下缓存键永不过时。值为 0 会导致键立刻过期（实际上就是不缓存）。 OPTIONS ：任何选项应该传递到缓存后端。有效选项列表将随着每个后端变化，并且由第三方库缓存支持的后端直接传递它们的选项到底层缓存库。 实现自有的淘汰策略的缓存后端（比如 locmem, filesystem 和 database 后端）将遵循以下选项： MAX_ENTRIES ：删除旧值之前允许缓存的最大条目。默认是 300 。 CULL_FREQUENCY ：当达到 MAX_ENTRIES 时被淘汰的部分条目。实际比率为 1 / CULL_FREQUENCY ，当达到 MAX_ENTRIES 时，设置为2就会淘汰一半的条目。这个参数应该是一个整数，默认为3。 CULL_FREQUENCY 的值为 0 意味着当达到 MAX_ENTRIES 缓存时，整个缓存都会被清空。在一些后端（尤其是 database ），这会使以更多的缓存未命中为代价来更快的进行淘汰。 Memcached 后端传递 OPTIONS 的内容作为键参数到客户端构造函数，从而允许对客户端行为进行更高级的控制。参见下文： KEY_PREFIX ：将自动包含（默认预先添加）到Django 服务器使用的所有缓存键的字符串。 查看 cache documentation 获取更多信息。 VERSION ：通过 Django 服务器生成的缓存键的默认版本号。 查看 cache documentation 获取更多信息。 KEY_FUNCTION ：一个包含指向函数的路径的字符串，该函数定义将如何前缀、版本和键组成最终的缓存键。 查看 cache documentation 获取更多信息。 在这个例子中，文件系统后端正被设置成60秒超时时间，并且最大容量是1000条。 12345678910CACHES = &#123; 'default': &#123; 'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache', 'LOCATION': '/var/tmp/django_cache', 'TIMEOUT': 60, 'OPTIONS': &#123; 'MAX_ENTRIES': 1000 &#125; &#125;&#125; 这个的例子是基于 python-memcached 后端的设置，对象大小限制在 2MB ： 123456789CACHES = &#123; 'default': &#123; 'BACKEND': 'django.core.cache.backends.memcached.MemcachedCache', 'LOCATION': '127.0.0.1:11211', 'OPTIONS': &#123; 'server_max_value_length': 1024 * 1024 * 2, &#125; &#125;&#125; 这个例子是基于 pylibmc 后端的设置，改设置支持二进制协议、SASL 验证和 ketama行为模式： 1234567891011121314CACHES = &#123; 'default': &#123; 'BACKEND': 'django.core.cache.backends.memcached.PyLibMCCache', 'LOCATION': '127.0.0.1:11211', 'OPTIONS': &#123; 'binary': True, 'username': 'user', 'password': 'pass', 'behaviors': &#123; 'ketama': True, &#125; &#125; &#125;&#125; 更多缓存知识访问&gt;&gt; https://docs.djangoproject.com/zh-hans/2.2/topics/cache/ （20）发送邮件 20.1 快速上手,仅需两行代码: 123456789from django.core.mail import send_mailsend_mail( 'Subject here', 'Here is the message.', 'from@example.com', ['to@example.com'], fail_silently=False,) 邮件是通过 SMTP 主机和端口发送的，由配置项 EMAIL_HOST 和 EMAIL_PORT 指定。如果配置了 EMAIL_HOST_USER 和 EMAIL_HOST_PASSWORD ，那么它们将被用来验证 SMTP 服务器。配置项 EMAIL_USE_TLS 和 EMAIL_USE_SSL 控制是否使用安全连接。发送邮件最简单的方式就是使用 django.core.mail.send_mail()。 send_mail():send_mail(subject, message, from_email, recipient_list, fail_silently=False, auth_user=None, auth_password=None, connection=None, html_message=None) 参数 subject, message, from_email 和 recipient_list 是必须的。 recipient_list: 一个字符串列表，每项都是一个邮箱地址。recipient_list中的每个成员都可以在邮件的 “收件人:” 中看到其他的收件人。 fail_silently: 为 False， send_mail() 发生错误时抛出 smtplib.SMTPException 。可在 smtplib 文档找到一系列可能的异常，它们都是 SMTPException 的子类。 auth_user: 可选的用户名，用于验证登陆 SMTP 服务器。 若未提供，Django 会使用 EMAIL_HOST_USER 指定的值。 auth_password: 可选的密码，用于验证登陆 SMTP 服务器。若未提供， Django 会使用 EMAIL_HOST_PASSWORD 指定的值。 connection: 可选参数，发送邮件使用的后端。若未指定，则使用默认的后端。查询 邮件后端 文档获取更多细节。 html_message: 若提供了 html_message，会使邮件成为 multipart/alternative 的实例， message 的内容类型则是 text/plain ，并且 html_message 的内容类型是 text/html 。 返回值会是成功发送的信息的数量（只能是 0 或 1 ，因为同时只能发送一条消息）。 20.2 批量发送邮件 django.core.mail.send_mass_mail() 用于批量发送邮件。 send_mass_mail():(datatuple, fail_silently=False, auth_user=None, auth_password=None, connection=None) datatuple 是一个元组，形式如下: 1(subject, message, from_email, recipient_list) datatuple 参数的每个元素会生成一份独立的邮件内容。就像 send_mail() 中的一样， recipient_list 中的每个收件人会在邮件的 “收件人:” 中看到其他收件人的地址一样.举个例子，以下代码会向两个不同的收件人列表发送两封不同的邮件，却复用了同一条连接:返回值是成功发送的消息的数量。 123message1 = ('Subject here', 'Here is the message', 'from@example.com', ['first@example.com', 'other@example.com'])message2 = ('Another Subject', 'Here is another message', 'from@example.com', ['second@test.com'])send_mass_mail((message1, message2), fail_silently=False) 20.3 显示全部收件人与独立收件人 以下发送了一封邮件给 john@example.com 和 jane@example.com，他们都出现在 “收件人:”: 123456send_mail( 'Subject', 'Message.', 'from@example.com', ['john@example.com', 'jane@example.com'],) 以下分别发送了一封邮件给 john@example.com 和 jane@example.com，他们收到了独立的邮件: 12345datatuple = ( ('Subject', 'Message.', 'from@example.com', ['john@example.com']), ('Subject', 'Message.', 'from@example.com', ['jane@example.com']),)send_mass_mail(datatuple) 20.4 防止头注入 Header injection 是一个开发漏洞，攻击者可以利用它在邮件头插入额外信息，以控制脚本生成的邮件中的 “收件人:” 和 “发件人:” 内容。Django 的邮件函数包含了以上所有的反头注入功能，通过在头中禁止新的行。如果 subject， from_email 或 recipient_list 包含了新行（不管是 Unix，Windows 或 Mac 格式中的哪一种），邮件函数（比如 send_mail() ）都会抛出一个 django.core.mail.BadHeaderError （ ValueError 的子类），这会中断邮件发送。你需要在将参数传给邮件函数前确保数据的有效性和合法性。 如果邮件的 内容 的开始部分包含了邮件头信息，这些头信息只会作为邮件内容原样打印。以下是一个实例视图，从请求的 POST 数据中获取 subject， message 和 from_email，并将其发送至 admin@example.com ，成功后再重定向至 “/contact/thanks/“ 1234567891011121314151617from django.core.mail import BadHeaderError, send_mailfrom django.http import HttpResponse, HttpResponseRedirectdef send_email(request): subject = request.POST.get('subject', '') message = request.POST.get('message', '') from_email = request.POST.get('from_email', '') if subject and message and from_email: try: send_mail(subject, message, from_email, ['admin@example.com']) except BadHeaderError: return HttpResponse('Invalid header found.') return HttpResponseRedirect('/contact/thanks/') else: # In reality we'd use a form class # to get proper validation errors. return HttpResponse('Make sure all fields are entered and valid.') 关于发送邮件的单元测试资料，参见测试文档中 Email services 章节。 （21）分页 21.1 分页基本操作 123456789101112131415161718192021222324&gt;&gt;&gt; from django.core.paginator import Paginator&gt;&gt;&gt; objects = ['john', 'paul', 'george', 'ringo']&gt;&gt;&gt; p = Paginator(objects, 2)&gt;&gt;&gt; p.count4&gt;&gt;&gt; p.num_pages2&gt;&gt;&gt; page2 = p.page(2)&gt;&gt;&gt; page2.object_list['george', 'ringo']&gt;&gt;&gt; page2.has_next()False&gt;&gt;&gt; page2.has_previous()True&gt;&gt;&gt; page2.has_other_pages()True&gt;&gt;&gt; page2.previous_page_number()1&gt;&gt;&gt; page2.start_index() # The 1-based index of the first item on this page3&gt;&gt;&gt; page2.end_index() # The 1-based index of the last item on this page4 21.2 在视图中使用 Paginator The view function looks like this: 12345678910from django.core.paginator import Paginatorfrom django.shortcuts import renderdef listing(request): contact_list = Contacts.objects.all() paginator = Paginator(contact_list, 25) # Show 25 contacts per page page = request.GET.get('page') contacts = paginator.get_page(page) return render(request, 'list.html', &#123;'contacts': contacts&#125;) In the template list.html, you’ll want to include navigation between pages along with any interesting information from the objects themselves: 1234567891011121314151617181920212223&#123;% for contact in contacts %&#125; &#123;# Each "contact" is a Contact model object. #&#125; &#123;&#123; contact.full_name|upper &#125;&#125;&lt;br&gt; ...&#123;% endfor %&#125;&lt;div class="pagination"&gt; &lt;span class="step-links"&gt; &#123;% if contacts.has_previous %&#125; &lt;a href="?page=1"&gt;&amp;laquo; first&lt;/a&gt; &lt;a href="?page=&#123;&#123; contacts.previous_page_number &#125;&#125;"&gt;previous&lt;/a&gt; &#123;% endif %&#125; &lt;span class="current"&gt; Page &#123;&#123; contacts.number &#125;&#125; of &#123;&#123; contacts.paginator.num_pages &#125;&#125;. &lt;/span&gt; &#123;% if contacts.has_next %&#125; &lt;a href="?page=&#123;&#123; contacts.next_page_number &#125;&#125;"&gt;next&lt;/a&gt; &lt;a href="?page=&#123;&#123; contacts.paginator.num_pages &#125;&#125;"&gt;last &amp;raquo;&lt;/a&gt; &#123;% endif %&#125; &lt;/span&gt;&lt;/div&gt; 21.3 相关方法与属性 Page.has_next() Page.has_previous() Page.has_other_pages() Page.next_page_number() Page.previous_page_number() Page.start_index() Page.end_index() Page.object_list：此页上的对象列表。 Page.number：此页的基于 1 的页码。 Page.paginator：关联的 Paginator 对象。 （22）性能与优化 22.1 性能优化介绍 清楚地理解你所说的“绩效”是什么很重要，因为它不仅仅是一个指标。提高速度可能是程序最明显的目标，但有时可能会寻求其他性能改进，例如降低内存消耗或减少对数据库或网络的要求。一个领域的改进通常会提高另一个领域的性能，但并不总是如此；有时甚至会牺牲另一个领域的性能。例如，一个程序速度的提高可能会导致它使用更多的内存。更糟糕的是，如果速度提高太过内存不足，以致于系统开始耗尽内存，那么你所做的弊大于利。还有其他的权衡。你自己的时间是一个宝贵的资源，比CPU时间更宝贵。一些改进可能太难实现，或者可能影响代码的可移植性或可维护性。并非所有的性能改进都值得付出努力。所以，你需要知道你的目标是什么样的性能改进，你也需要知道你有一个很好的理由去瞄准那个方向——而且你需要： django-debug-toolbar https://github.com/jazzband/django-debug-toolbar/ 是一个非常方便的工具，它可以深入了解您的代码正在做什么以及花费了多少时间。特别是它可以显示您的页面生成的所有SQL查询，以及每个查询所用的时间。第三方面板也可用于工具栏，可以（例如）报告缓存性能和模板呈现时间。 22.2 性能优化的几个方面 内存与索引 数据库查询优化 中间件优化 访问时间 缓存 禁用 DEBUG = False （23）WSGI部署Django 23.1 使用 WSGI 进行部署 WSGI，PythonWeb服务器网关接口（Python Web Server Gateway Interface，缩写为WSGI)是Python应用程序或框架和Web服务器之间的一种接口，已经被广泛接受, 它已基本达成它的可移植性方面的目标。WSGI是作为Web服务器与Web应用程序或应用框架之间的一种低级别的接口，以提升可移植Web应用开发的共同点。WSGI是基于现存的[[CGI]]标准而设计的。 application对象 用 WSGI 部署的关键是 application callable，应用服务器用它与你的代码交互。 application callable 一般以一个位于 Python 模块中，名为 application 的对象的形式提供，且对服务器可见。startproject 命令创建了文件 &lt;project_name&gt;/wsgi.py，其中包含了 application callable。Django 开发服务器和生产环境的 WSGI 部署都用到了它。 WSGI 服务器从其配置中获取 application callable 的路径。Django 的默认服务器（ runserver 命令），从配置项 WSGI_APPLICATION 中获取。默认值是 &lt;project_name&gt;.wsgi.application，指向 &lt;project_name&gt;/wsgi.py 中的 application callable。 配置setting模块 wsgi.py 默认将其设置为 mysite.settings， mysite 即工程名字。这就是 runserver 默认的发现默认配置行为。 注解：由于环境变量是进程级的，所以如果在同一进程运行多个 Django 站点将出错。这在使用 mod_wsgi 时会出现。要避免此问题，为每个站点在后台进程使用 mod_wsgi 的后台模式，或者在 wsgi.py 中通过 os.environ[&quot;DJANGO_SETTINGS_MODULE&quot;]= &quot;mysite.settings&quot; 重写来自环境变量的值。 应用WSGI中间件 要应用 WSGI 中间层，你只需简单包裹应用对象。举个例子，你可以在 wsgi.py 末尾添加以下代码: 12from helloworld.wsgi import HelloWorldApplicationapplication = HelloWorldApplication(application) 如果你想将 Django 应用于一个 WSGI 应用或其它框架联合起来，可以用自定义 WSGI 应用替换 Django 的 WSGI 应用，前者会在稍晚时候将任务委托给 WSGI 应用。 23.2 使用 Apache 和 mod_wsgi 托管 Django mod_wsgi 是一个 Apache 模块，它可以管理任何 Python WSGI 应用，包括 Django。Django 支持所有支持 mod_wsgi 的 Apache 版本。官方 mod_wsgi 文档 介绍了如何使用 mod_wsgi 的全部细节。你可能更喜欢从 安装和配置文档 开始。 安装并激活 mod_wsgi 后，编辑 Apache 服务器的 httpd.conf 文件，并添加以下内容。若你正在使用的 Apache 版本号早于 2.4，用 Allow from all 替换 Require allgranted，并在其上添加一行 Order deny,allow。 123456789WSGIScriptAlias / /path/to/mysite.com/mysite/wsgi.pyWSGIPythonHome /path/to/venvWSGIPythonPath /path/to/mysite.com&lt;Directory /path/to/mysite.com/mysite&gt;&lt;Files wsgi.py&gt;Require all granted&lt;/Files&gt;&lt;/Directory&gt; WSGIScriptAlias 行的第一项是你所期望的应用所在的基础 URL 路径（ / 根 url），第二项是 “WSGI 文件” 的位置——一般位于项目包之内（本例中是 mysite）。这告诉 Apache 用该文件中定义的 WSGI 应用响应指定 URL 下的请求。 如果你在某个 virtualenv 内为应用安装项目的 Python 依赖，将该 virtualenv 的路径添加至 WSGIPythonHome 。参考 mod_wsgi virtualenv 指引 获取更多细节。 WSGIPythonPath 行确保你的项目包能从 Python path 导入；换句话说， importmysite 能正常工作。 &lt;Directory&gt; 片段仅确保 Apache 能访问 wsgi.py 文件。 下一步，我们需要确认 wsgi.py 文件包含一个 WSGI 应用对象。从 Django 1.4 起， startproject 会自动创建；换而言之，你无需手动创建。查阅 WSGI 概述文档 获取你需要配置的默认内容，以及其它可配置项。 注意1： 如果多个 Django 站点运行在同一 mod_wsgi 进程，它们会共用最先启动的站点配置。能通过以下修改改变行为: 123&gt; os.environ.setdefault("DJANGO_SETTINGS_MODULE", "&#123;&#123; project_name &#125;&#125;.settings")&gt; &gt; &gt; wsgi.py 中也这么改: 123&gt; os.environ["DJANGO_SETTINGS_MODULE"] = "&#123;&#123; project_name &#125;&#125;.settings"&gt; &gt; &gt; 或通过 使用 mod_wsgi 的后台模式 确保每个站点都运行于独立的后台进程。 注意2： 为文件上传修复 UnicodeEncodeError 上传名称包含非 ASCII 字符的文件时，若抛出 UnicodeEncodeError，确认 Apache 是否被正确配置，能接受非 ASCII 文件名: 1234&gt; export LANG='en_US.UTF-8'&gt; export LC_ALL='en_US.UTF-8'&gt; &gt; &gt; 常见的配置文件路径是 /etc/apache2/envvars。 参考 Unicode 参考指引的 Files 章节获取细节信息。 使用 mod_wsgi 后台模式 为了创建必要的后台进程组并在其中运行 Django 实例，你需要添加合适的 WSGIDaemonProcess和 WSGIProcessGroup 指令。上述配置在你使用后台模式时需要点小修改，即你不能使用 WSGIPythonPath；作为替换，你要在 WSGIDaemonProcess 中添加 python-path 选项，例如： 12WSGIDaemonProcess example.com python-home=/path/to/venv python-path=/path/to/mysite.comWSGIProcessGroup example.com 如果你想在子目录中开放你的项目（本例中 https://example.com/mysite），你可在上述配置中添加 WSGIScriptAlias： 1WSGIScriptAlias /mysite /path/to/mysite.com/mysite/wsgi.py process-group=example.com 参考官方 mod_wsgi 文档获取 配置后台模式的细节。 23.3 Apache 利用 Django 的用户数据库进行验证 使用 Apache 时，保持多个身份认证数据同步是一个常见的问题，你可以让 Apache 直接使用 Django 的 验证系统。这要求 Apache 版本 &gt;= 2.2，且 mod_wsgi &gt;= 2.0。例如这样： 仅为已授权的用户直接从 Apache 提供 static/media 文件。 仅为有特定权限的 Django 用户提供 Subversion 仓库访问。 允许某些用户连接到 mod_dav 创建的 WebDAV 共享。 确保你已按照 Apache 配合 mod_wsgi 文档正确安装并激活了 mod_wsgi。然后，编辑 Apache 配置，添加只允许授权用户查看的位置： 12345678910111213WSGIScriptAlias / /path/to/mysite.com/mysite/wsgi.pyWSGIPythonPath /path/to/mysite.comWSGIProcessGroup %&#123;GLOBAL&#125;WSGIApplicationGroup %&#123;GLOBAL&#125;&lt;Location "/secret"&gt; AuthType Basic AuthName "Top Secret" Require valid-user AuthBasicProvider wsgi WSGIAuthUserScript /path/to/mysite.com/mysite/wsgi.py&lt;/Location&gt; WSGIAuthUserScript 指令告诉 mod_wsgi 在指定 wsgi 脚本中执行 check_password 函数，并传递从提示符获取的用户名和密码。在本例中， WSGIAuthUserScript 与 WSGIScriptAlias 一样，后者 由 django-admin startproject 创建，定义了应用。 最后，编辑 WSGI 脚本 mysite.wsgi，通过导入 check_password 函数，将 Apache 的认证授权机制接续在你站点的授权机制之后: 12345678import osos.environ['DJANGO_SETTINGS_MODULE'] = 'mysite.settings'from django.contrib.auth.handlers.modwsgi import check_passwordfrom django.core.handlers.wsgi import WSGIHandlerapplication = WSGIHandler() 以 /secret/ 开头的请求现在会要求用户认证。 mod_wsgi 可达性控制机制文档 提供了其它授权机制和方法的更多细节和信息。 利用 mod_wsgi 和 Django 用户组(groups)进行授权¶mod_wsgi 也提供了将组成员限制至特定路径的功能。 在本例中，Apache 配置应该看起来像这样： 1234567891011121314WSGIScriptAlias / /path/to/mysite.com/mysite/wsgi.pyWSGIProcessGroup %&#123;GLOBAL&#125;WSGIApplicationGroup %&#123;GLOBAL&#125;&lt;Location "/secret"&gt; AuthType Basic AuthName "Top Secret" AuthBasicProvider wsgi WSGIAuthUserScript /path/to/mysite.com/mysite/wsgi.py WSGIAuthGroupScript /path/to/mysite.com/mysite/wsgi.py Require group secret-agents Require valid-user&lt;/Location&gt; 要支持 WSGIAuthGroupScript 指令，同样的 WSGI 脚本 mysite.wsgi 必须也导入 groups_for_user 函数，函数会返回用户所属用户组的列表。 1from django.contrib.auth.handlers.modwsgi import check_password, groups_for_user 对 /secret 的请求现在也会要求用户是 “secret-agents” 用户组的成员。 23.4 如何使用 Gunicorn 托管 Django Gunicorn (‘Green Unicorn’) 是一个 UNIX 下的纯 Python WSGI 服务器。它没有其它依赖，容易安装和使用。安装 gunicorn 非常简单，只要执行 pip install gunicorn 即可。 安装 Gunicorn 之后，可以使用 gunicorn 命令启动 Gunicorn 服务进程。最简模式下，只需要把包含了 WSGI 应用对象的 application 模块位置告诉 gunicorn，就可以启动了。因此对于典型的 Django 项目，像这样来调用 gunicorn: 1gunicorn myproject.wsgi 这样会创建一个进程，包含了一个监听在 127.0.0.1:8000 的线程。前提是你的项目在 Python path 中，要满足这个条件，最简单的方法是在 manage.py 文件所在的目录中运行这条命令。 23.5 如何用 uWSGI 托管 Django uWSGI 是一个快速的，自我驱动的，对开发者和系统管理员友好的应用容器服务器，完全由 C 编写。uWSGI 百科介绍了几种 安装流程。Pip （Python 包管理器）能让你仅用一行代码就安装任意版本的 uWSGI。例子： 12345# Install current stable version.$ pip install uwsgi# Or install LTS (long term support).$ pip install https://projects.unbit.it/downloads/uwsgi-lts.tar.gz 假设你有个叫做 mysite 的顶级项目包，期中包含一个模板 mysite/wsgi.py，模块包含一个 WSGI application 对象。如果你使用的是较新的 Django，这就是你运行 django-admin startproject mysite （使用你的项目名替换 mysite）后得到的目录结构。若该文件不存在，你需要创建它。参考文档 如何使用 WSGI 进行部署 看看你需要配置的默认内容，以及你还能添加什么。 Django 指定的参数如下： chdir：需要包含于 Python 的导入路径的目录的路径——例如，包含 mysite 包的目录。 module：要使用的 WSGI 模块——可能是 startproject 创建的 mysite.wsgi的模块。 env：至少要包括 DJANGO_SETTINGS_MODULE。 home: 可选的路径，指向你工程的 virtualenv。 示例 ini 配置文件: 12345678[uwsgi]chdir=/path/to/your/projectmodule=mysite.wsgi:applicationmaster=Truepidfile=/tmp/project-master.pidvacuum=Truemax-requests=5000daemonize=/var/log/uwsgi/yourproject.log 示例 ini 配置文件语法: 1uwsgi --ini uwsgi.ini （24）Django FAQ0 发音：Django 发音为 JANG，使用 FANG 来押韵，字母 “D “是不发声的. 我们也记录了一段 发音的音频片段. 稳定性：相当稳定。使用 Django 搭建的网站能承受每秒 50000 次点击的流量峰值。 扩展性：可以在任何级别添加硬件——数据库服务器，缓存服务器或 Web /应用程序服务器。 MVC 框架：在我们对 MVC 的解释中，“视图”描述了呈现给用户的数据。数据看起来怎么样并不重要，重要的是哪些数据被呈现。 Django “MTV “ 框架—即 “模型(Model) “、 “模板(Template)” 和 “视图(View). “视图(view)”是 Python 中针对一个特定 URL 的回调函数，此回调函数描述了需要展示的数据。展示效果就是模板。在 Django 里面，一个视图（view）描述了哪些数据会被展示。那控制器（Controller）在什么位置？在 Django 中，会根据 Django 的 URL 配置，将请求分发到适当的视图（view）。 数据库：官方文档推荐PostgreSQL 版本选择：Django2.1,2.2支持python3.5+。Django 的第三方插件可以自由设置他们的版本要求。生产中使用 Django，你应该使用稳定版本。 使用图片和文件字段。在模型中使用 FileField 或 ImageField ，你还需要完成如下步骤： 在你的 setting 文件中，你需要定义：setting: MEDIA_ROOT 作为 Django 存储上传文件目录的完整路径。（为了提高性能，这些文件不会储存在数据库中）定义： setting: MEDIA_URL 作为该目录的基本公共 URL， 确保该目录能够被 Web 服务器的账户写入。 在你的模型中添加 FileField 或者 ImageField ，可以通过定义:attr:~django.db.models.FileField.upload_to 在 MEDIA_ROOT 中明确一个子目录用来上传文件。 所有将被储存在数据库中的文件路径相同（相对于：setting: MEDIA_ROOT）。你很想用由 Django 提供的：attr:~django.db.models.fields.files.FieldFile.url，比如， 如果：class:~django.db.models.ImageField 被叫做mug_shot， 你就可以得到` 图片模板的绝对路径。 导入已有数据库：https://docs.djangoproject.com/zh-hans/2.2/howto/legacy-databases/ django只支持单列主键 Django 官方不支持 NoSQL 数据库 技术交流共享QQ群【机器学习和自然语言QQ群：436303759】： 机器学习和自然语言（QQ群号：436303759）是一个研究深度学习、机器学习、自然语言处理、数据挖掘、图像处理、目标检测、数据科学等AI相关领域的技术群。其宗旨是纯粹的AI技术圈子、绿色的交流环境。本群禁止有违背法律法规和道德的言谈举止。群成员备注格式：城市-自命名。微信订阅号：datathinks]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[笔记8:Django基础篇]]></title>
    <url>%2F2019%2F06%2F13%2F%E7%AC%94%E8%AE%B08-Django%E5%9F%BA%E7%A1%80%E7%AF%87%2F</url>
    <content type="text"><![CDATA[摘要：日常学习中对一些知识点进行总结得出该系列文章。学习笔记内容包括前端技术，Django web开发技术，数据库技术如MySQL，MongoDB，PGSQL等等。此外还有一些工具如Dock，ES等等。（本文原创，转载必须注明出处.） （1）查看python版本号： 1python -m django --version （2） 创建Django项目 1django-admin startproject mysite （3）测试开发服务器是否成功 1Desktop\bncDjango\mysite&gt;python manage.py runserver Django 自带一个用纯 Python 写的轻量级的 Web 服务器。为了让你能快速的开发出想要的东西，因为你不需要进行配置生产级别的服务器（比如 Apache）方面的工作，除非你已经准备好投入生产环境了。**千万不要** 将这个服务器用于和生产环境相关的任何地方。这个服务器只是为了开发而设计的。(我们在 Web 框架方面是专家，在 Web 服务器方面并不是。) （4）创建应用模块 1python manage.py startapp polls 1234567891011# Application definitionINSTALLED_APPS = [ 'django.contrib.admin', # 管理员站点 'django.contrib.auth', # 认证授权系统 'django.contrib.contenttypes', # 内容类型框架 'django.contrib.sessions', # 会话框架 'django.contrib.messages', # 消息框架 'django.contrib.staticfiles', #管理静态文件的框架 'polls', # 投票模块] （5）polls模型下编辑视图view 12345678from django.shortcuts import render# Create your views here.from django.http import HttpResponsedef index(request): return HttpResponse("Hello,this is my frist polls index.") （6）polls模块下映射url 123456from django.urls import pathfrom . import viewsurlpatterns = [ path('', views.index,name='index'),] （7）mysite主模块下配置url 1234567from django.contrib import adminfrom django.urls import path,include # 注意导入include模块urlpatterns = [ path('polls/', include('polls.urls')), # 配置polls子模板url，支持正则 path('admin/', admin.site.urls),] （8）网页查询http://localhost:8000/polls/ （9）数据库配置与迁移 12345678910111213141516171819DATABASES = &#123; 'default': &#123; # 'django.db.backends.sqlite3'， # 'django.db.backends.postgresql'， # 'django.db.backends.mysql'， # 'django.db.backends.oracle' 'ENGINE': 'django.db.backends.sqlite3', 'NAME': os.path.join(BASE_DIR, 'db.sqlite3'), &#125;, # MySQL数据库配置 # 'mysql': &#123; # 'ENGINE': 'django.db.backends.mysql', # 'NAME': 'all_news', # 数据库名 # 'USER': 'root', # 'PASSWORD': 'root', # 'HOST': '127.0.0.1', # 'PORT': '3306', # &#125;&#125; 1python manage.py migrate （10）编写模型M 123456789101112131415161718from django.db import models# Create your models here.class Question(models.Model): question_text = models.CharField(max_length=200) pub_data = models.DateField('date published') def __str__(self): return self.question_textclass Choice(models.Model): question = models.ForeignKey(Question,on_delete=models.CASCADE) choice_text = models.CharField(max_length=200) votes = models.IntegerField(default=0) def __str__(self): return self.choice_text （11）激活模型 为模型的改变生成迁移文件 1python manage.py makemigrations polls 另一种查看，选择执行接收一个迁移的名称，然后返回对应的 SQL 1python manage.py sqlmigrate polls 0001 应用数据库迁移 1python manage.py migrate （12）全自动后台管理页面 12.1 创建一个能登录管理页面的用户,均为admin 1python manage.py createsuperuser 12.2 启动开发服务器： 1python manage.py runserver http://127.0.0.1:8000/admin/login/?next=/admin/ 12.3 进入站点 12.4 管理页面中加入配置应用 1234567from django.contrib import admin# Register your models here.from .models import Question,Choiceadmin.site.register(Question)admin.site.register(Choice) （13）编写更多视图 13.1 polls下的views 123456789101112131415161718192021from django.shortcuts import renderfrom django.http import HttpResponse# 问题索引页def index(request): return HttpResponse("Hello,this is my frist polls index.")# 问题详情页def detail(request,question_id): return HttpResponse("You're looking at question %s." % question_id)# 问题结果页def results(request,question_id): return HttpResponse("You're looking at the results of question %s." % question_id)# 投票处理器def vote(request,question_id): return HttpResponse("You're voting on question %s." % question_id) 13.2 polls下的urls记得添加命名空间 12345678910111213from django.urls import pathfrom . import viewsapp_name = 'polls' #添加命名空间urlpatterns = [ # ex: /polls/ path('', views.index,name='index'), # ex: /polls/5/ path('&lt;int:question_id&gt;/', views.detail, name='detail'), # ex: /polls/5/results/ path('&lt;int:question_id&gt;/results/', views.results, name='results'), # ex: /polls/5/vote/ path('&lt;int:question_id&gt;/vote/', views.vote, name='vote'),] 13.3 查询数据库信息并页面显示 123456# 问题索引页def index(request): latest_question_list = Question.objects.order_by('pub_data')[:3] output = '&lt;br/&gt;'.join([q.question_text for q in latest_question_list])HttpResponse(template.render(context,request)) return HttpResponse(output) （14）编写模板T 14.1 在mysite下创建templates，并创建polls文件夹下创建index.html 1234567891011121314151617&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;投票页面&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&#123;% if latest_question_list %&#125; &lt;ul&gt; &#123;% for question in latest_question_list %&#125; &lt;li&gt;&lt;a href="/polls/&#123;&#123; question.id &#125;&#125;/"&gt;&#123;&#123; question.question_text &#125;&#125;&lt;/a&gt;&lt;/li&gt; &#123;% endfor %&#125; &lt;/ul&gt;&#123;% else %&#125; &lt;p&gt;No polls are available.&lt;/p&gt;&#123;% endif %&#125;&lt;/body&gt;&lt;/html&gt; 可以修改为（常用）.想改成 `polls/specifics/12/` ，你不用在模板里修改任何东西（包括其它模板），只要在 `polls/urls.py` 里稍微修改一下就行： 12345&lt;ul&gt;&#123;% for question in latest_question_list %&#125;&lt;li&gt;&lt;a href="&#123;% url 'polls:detail' question.id %&#125;"&gt;&#123;&#123; question.question_text &#125;&#125;&lt;/a&gt;&lt;/li&gt;&#123;% endfor %&#125;&lt;/ul&gt; 14.2 在mysite的settings修改DIRS 123456789101112131415TEMPLATES = [ &#123; 'BACKEND': 'django.template.backends.django.DjangoTemplates', 'DIRS': [os.path.join(BASE_DIR,'template/')], 'APP_DIRS': True, 'OPTIONS': &#123; 'context_processors': [ 'django.template.context_processors.debug', 'django.template.context_processors.request', 'django.contrib.auth.context_processors.auth', 'django.contrib.messages.context_processors.messages', ], &#125;, &#125;,] 14.3 polls/views.py 修改 123456789101112131415from django.shortcuts import renderfrom django.http import HttpResponsefrom django.template import loaderfrom polls.models import Question# 问题索引页def index(request): latest_question_list = Question.objects.order_by('pub_data')[:5] # output = '&lt;br/&gt;'.join([q.question_text for q in latest_question_list]) template = loader.get_template('polls/index.html') context = &#123; 'latest_question_list': latest_question_list, &#125; return HttpResponse(template.render(context,request)) 进一步可以修改为(常用)： 12345# 问题索引页def index(request): latest_question_list = Question.objects.order_by('pub_data')[:5] context = &#123;'latest_question_list': latest_question_list&#125; return render(request,'polls/index.html',context) 14.4 在浏览器访问 “/polls/“ 查看： （15）查看详细页面 15.1 polls下views.py 123456789from django.http import Http404# 问题详情页def detail(request,question_id): try: question = Question.objects.get(pk=question_id) except Question.DoesNotExist: raise Http404("Question does not exist") return render(request,'polls/detail.html', &#123;'question':question,'question_id':question_id&#125;) 优化后的算法（常用） 123456from django.shortcuts import render,get_object_or_404# 问题详情页def detail(request,question_id): question = get_object_or_404(Question, pk=question_id) return render(request, 'polls/detail.html', &#123;'question': question,'question_id':question_id&#125;) 15.2 template下detail.html 123456789101112131415&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;详细问题页面&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;&#123;&#123; question.question_text &#125;&#125;&lt;/h1&gt;&lt;ul&gt;&#123;% for choice in question.choice_set.all %&#125; &lt;li&gt;&#123;&#123; choice.choice_text &#125;&#125;&lt;/li&gt;&#123;% endfor %&#125;&lt;/ul&gt;&lt;/body&gt;&lt;/html&gt; 15.3 运行结果 （16）polls/detail.html详细页面添加一个表单form 1234567891011121314151617&lt;body&gt;&lt;h1&gt;`&#123;&#123; question.question_text &#125;&#125;`&lt;/h1&gt;`&#123;% if error_message %&#125;`&lt;p&gt;&lt;strong&gt; '&#123;&#123; error_message&#125;&#125;'&lt;/strong&gt;&lt;/p&gt;`&#123;% endif %&#125;`&lt;form action="&#123;% url 'polls:vote' question.id %&#125;" method="post"&gt;`&#123;% csrf_token %&#125;``&#123;% for choice in question.choice_set.all %&#125;` &lt;input type="radio" name="choice" id="choice&#123;&#123; forloop.counter&#125;&#125;" value="&#123;&#123; choice.id&#125;&#125;" /&gt; &lt;label for="choice&#123;&#123; forloop.counter&#125;&#125;"&gt;`&#123;&#123; choice.choice_text &#125;&#125;`&lt;/label&gt; &lt;br&gt;`&#123;% endfor %&#125;`&lt;input type="submit" value="Vote"/&gt;&lt;/form&gt;&lt;/body&gt; 每个单选按钮的 value 属性是对应的各个 Choice 的 ID。每个单选按钮的 name 是 &quot;choice&quot; 。这意味着，当有人选择一个单选按钮并提交表单提交时，它将发送一个 POST 数据 choice=# ，其中# 为选择的 Choice 的 ID。这是 HTML 表单的基本概念。 我们设置表单的 action 为 { % url “polls:vote” question.id %} ，并设置 method=&quot;post&quot; 。使用 method=”post”是非常重要的，因为这个提交表单的行为会改变服务器端的数据。当你需要创建一个改变服务器端数据的表单时，请使用`method=&quot;post&quot; 。这不是 Django 的特定技巧；这是优秀的网站开发技巧。 forloop.counter指示 [for`](https://docs.djangoproject.com/zh-hans/2.2/ref/templates/builtins/#std:templatetag-for) 标签已经循环多少次。 由于我们创建一个 POST 表单（它具有修改数据的作用），所以我们需要小心跨站点请求伪造。 谢天谢地，你不必太过担心，因为 Django 已经拥有一个用来防御它的非常容易使用的系统。 简而言之，所有针对内部 URL 的 POST 表单都应该使用{ % csrf_token %}模板标签。 （17） polls/views.py 视图编辑 12345678910111213# 投票处理器def vote(request,question_id): question = get_object_or_404(Question, pk=question_id) try: selected_choice = question.choice_set.get(pk=request.POST['choice']) except (KeyError, Choice.DoesNotExist): return render(request, 'polls/detail.html', &#123; 'question': question, 'error_message': "You didn't select a choice.",&#125;) else: selected_choice.votes +=1 selected_choice.save() return HttpResponseRedirect(reverse('polls:results', args=(question.id,))) request.POST 是一个类字典对象，让你可以通过关键字的名字获取提交的数据。 这个例子中， request.POST[&#39;choice&#39;] 以字符串形式返回选择的 Choice 的 ID。 request.POST 的值永远是字符串。 如果在 request.POST[&#39;choice&#39;] 数据中没有提供 choice ， POST 将引发一个 KeyError 。上面的代码检查 KeyError ，如果没有给出 choice 将重新显示 Question 表单和一个错误信息。 在增加 Choice 的得票数之后，代码返回一个 HttpResponseRedirect 而不是常用的 HttpResponse 、 HttpResponseRedirect 只接收一个参数：用户将要被重定向的 URL。构造函数中使用 reverse()函数。这个函数避免了我们在视图函数中硬编码 URL。重定向的 URL 将调用 &#39;results&#39; 视图来显示最终的页面。 (18) 重定向results.html 12345from django.shortcuts import get_object_or_404, renderdef results(request, question_id): question = get_object_or_404(Question, pk=question_id) return render(request, 'polls/results.html', &#123;'question': question&#125;) （19）通用视图，代码重构 19.1 detail() 视图几乎一模一样。唯一的不同是模板的名字。 12345678910111213141516# 问题索引页def index(request): latest_question_list = Question.objects.order_by('pub_data')[:5] return render(request,'polls/index.html',&#123;'latest_question_list': latest_question_list&#125;)# 问题详情页def detail(request,question_id): question = get_object_or_404(Question, pk=question_id) return render(request, 'polls/detail.html', &#123;'question': question,'question_id':question_id&#125;)# 问题结果页def results(request,question_id): question = get_object_or_404(Question, pk=question_id) return render(request, 'polls/results.html', &#123;'question': question&#125;) 19.2 创建一个 polls/results.html 模板 123456789&lt;h1&gt;&#123;&#123; question.question_text &#125;&#125;&lt;/h1&gt;&lt;ul&gt;`&#123;% for choice in question.choice_set.all %&#125;` &lt;li&gt;`&#123;&#123; choice.choice_text &#125;&#125; -- &#123;&#123; choice.votes &#125;&#125; vote&#123;&#123; choice.votes|pluralize &#125;&#125;`&lt;/li&gt;&#123;% endfor %&#125;&lt;/ul&gt;&lt;a href="&#123;% url 'polls:detail' question.id %&#125;"&gt;Vote again?&lt;/a&gt; 19.3 通用视图系统 通用视图将常见的模式抽象化，可以使你在编写应用时甚至不需要编写Python代码。将我们的投票应用转换成使用通用视图系统，这样我们可以删除许多我们的代码。我们仅仅需要做以下几步来完成转换， 转换 URLconf。 删除一些旧的、不再需要的视图。 基于 Django 的通用视图引入新的视图 1 改良URLconf 打开 polls/urls.py 这个 URLconf 并将它修改成：路径字符串中匹配模式的名称已经由 &lt;question_id&gt; 改为 &lt;pk&gt;。 12345678910from django.urls import pathfrom . import viewsapp_name = 'polls'urlpatterns = [ path('', views.IndexView.as_view(), name='index'), path('&lt;int:pk&gt;/', views.DetailView.as_view(), name='detail'), path('&lt;int:pk&gt;/results/', views.ResultsView.as_view(), name='results'), path('&lt;int:question_id&gt;/vote/', views.vote, name='vote'),] 2 改良视图 删除旧的 index, detail, 和 results 视图，并用 Django 的通用视图代替。打开 polls/views.py 文件，并将它修改成： 123456789101112131415161718192021222324252627282930313233343536373839from django.http import HttpResponseRedirectfrom django.shortcuts import get_object_or_404, renderfrom django.urls import reversefrom django.views import genericfrom .models import Choice, Questionclass IndexView(generic.ListView): template_name = 'polls/index.html' context_object_name = 'latest_question_list' def get_queryset(self): return Question.objects.order_by('pub_date')[:5]class DetailView(generic.DetailView): model = Question template_name = 'polls/detail.html'class ResultsView(generic.DetailView): model = Question template_name = 'polls/results.html'def vote(request,question_id): question = get_object_or_404(Question, pk=question_id) try: selected_choice = question.choice_set.get(pk=request.POST['choice']) except (KeyError, Choice.DoesNotExist): return render(request, 'polls/detail.html', &#123; 'question': question, 'error_message': "You didn't select a choice.", &#125;) else: selected_choice.votes +=1 selected_choice.save() return HttpResponseRedirect(reverse('polls:results', args=(question.id,))) 每个通用视图需要知道它将作用于哪个模型。 这由 model 属性提供。 DetailView 期望从 URL 中捕获名为 &quot;pk&quot; 的主键值，所以我们为通用视图把 question_id 改成 pk 。 19.4 运行程序 主页面 子页面 详情页面 （20）自动化测试 20.1 测试的策略 测试驱动：写代码之前先写测试。「测试驱动」的开发方法只是将问题的描述抽象为了 Python 的测试样例。 更普遍的情况是，一个刚接触自动化测试的新手更倾向于先写代码，然后再写测试。 如果你才写了几千行 Python 代码，选择从哪里开始写测试确实不怎么简单。如果是这种情况，那么在你下次修改代码（比如加新功能，或者修复 Bug）之前写个测试是比较合理且有效的。 20.2 第一个测试 需求：我们的要求是如果 Question 是在一天之内发布，was_published_recently()方法将会返回True，然而现在这个方法在Question的pub_date` 字段比当前时间还晚时也会返回 True 编写测试代码： 12345678910111213141516from django.test import TestCase# Create your tests here.from django.utils import timezonefrom .models import Questionclass QuestionModelTests(TestCase): def test_was_published_recently_with_future_question(self): """ was_published_recently() returns False for questions whose pub_date is in the future. """ time = timezone.now() + datetime.timedelta(days=30) future_question = Question(pub_date=time) self.assertIs(future_question.was_published_recently(), False) 运行代码：$ python manage.py test polls 测试结果： python manage.py test polls 将会寻找 polls 应用里的测试代码 它找到了 django.test.TestCase 的一个子类 它创建一个特殊的数据库供测试使用 它在类中寻找测试方法——以 test 开头的方法。 在 test_was_published_recently_with_future_question 方法中，它创建了一个 pub_date 值为 30 天后的 Question 实例。 接着使用 assertls() 方法，发现 was_published_recently() 返回了 True，而我们期望它返回 False。 测试系统通知我们哪些测试样例失败了，和造成测试失败的代码所在的行号。 （21）静态文件（图片/脚本/样式） 对于小项目来说，静态文件随便放在哪，只要服务程序能够找到它们就行。然而在大项目中，处理不同应用所需要的静态文件的工作就显得有点麻烦了。这就是 django.contrib.staticfiles 存在的意义 创建的 static 文件夹中创建 polls 的文件夹，再在 polls 文件夹中创建一个名为 style.css 的文件。样式表路径应是 polls/static/polls/style.css。因为 AppDirectoriesFinder 的存在，你可以在 Django 中简单地使用以 polls/style.css 的形式引用此文件，类似你引用模板路径的方式。 123li a &#123; color: green;&#125; polls的index.html引用 12345&lt;head&gt; &lt;title&gt;投票页面&lt;/title&gt; `&#123;% load static %&#125;` &lt;link rel="stylesheet" type="text/css" href="&#123;% static 'polls/style.css' %&#125;"&gt;&lt;/head&gt; 添加图片 我们会创建一个用于存在图像的目录。在 polls/static/polls 目录下创建一个名为 images 的子目录。在这个目录中，放一张名为 background.gif 的图片。换言之，在目录 polls/static/polls/images/background.jpg 中放一张图片。 123body &#123; background: white url("images/background.gif") no-repeat;&#125; 更多关于设置和框架的资料，参考 静态文件解惑 和 静态文件指南。部署静态文件 介绍了如何在真实服务器上使用静态文件。 (22) 编写第一个django应用 22.1 polls/admin定义后台表单,列表为字段显示顺序 123456789101112131415161718192021from django.contrib import admin# Register your models here.from .models import Question,Choiceclass QuestionAdmin(admin.ModelAdmin): # fieldsets = [ # ('问题内容', &#123;'fields': ['question_text']&#125;), # ('发布时间', &#123;'fields': ['pub_data']&#125;), # ] # fields = ['pub_data', 'question_text'] list_display = ('question_text', 'pub_data')admin.site.register(Question, QuestionAdmin)class ChoiceAdmin(admin.ModelAdmin): # fields = ['question','choice_text', 'votes'] list_display = ('question','choice_text', 'votes')admin.site.register(Choice, ChoiceAdmin) 22.2 字段过滤器 1234class QuestionAdmin(admin.ModelAdmin): list_display = ('question_text', 'pub_data') list_filter = ['pub_data'] # 过滤器admin.site.register(Question, QuestionAdmin) 22.3 自定义后台界面与风格 打开你的设置文件（mysite/settings.py，牢记），在 TEMPLATES 设置中添加 DIRS 选项： 在 templates 目录内创建名为 admin 的目录，随后，将存放 Django 默认模板的目录（django/contrib/admin/templates）内的模板文件 admin/base_site.html 复制到这个目录内。Django 的源文件在哪里？$ python -c “import django; print(django.path)” 完成后，你应该看到如下代码： 123`&#123;% block branding %&#125;`&lt;h1 id="site-name"&gt;&lt;a href="&#123;% url 'admin:index' %&#125;"&gt;Polls Administration&lt;/a&gt;&lt;/h1&gt;`&#123;% endblock %&#125;` 技术交流共享QQ群【机器学习和自然语言QQ群：436303759】： 机器学习和自然语言（QQ群号：436303759）是一个研究深度学习、机器学习、自然语言处理、数据挖掘、图像处理、目标检测、数据科学等AI相关领域的技术群。其宗旨是纯粹的AI技术圈子、绿色的交流环境。本群禁止有违背法律法规和道德的言谈举止。群成员备注格式：城市-自命名。微信订阅号：datathinks]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[笔记7:Python复习篇]]></title>
    <url>%2F2019%2F06%2F13%2F%E7%AC%94%E8%AE%B07-Python%E5%A4%8D%E4%B9%A0%E7%AF%87%2F</url>
    <content type="text"><![CDATA[摘要：日常学习中对一些知识点进行总结得出该系列文章。学习笔记内容包括前端技术，Django web开发技术，数据库技术如MySQL，MongoDB，PGSQL等等。此外还有一些工具如Dock，ES等等。（本文原创，转载必须注明出处.） （1）print 默认输出是换行的，如果要实现不换行需要在变量末尾加上 end=””： （2）isinstance 和 type 的区别在于：type()不会认为子类是一种父类类型。isinstance()会认为子类是一种父类类型。这么理解，父类：动物；子类：猫。isinstance()认为猫是动物，type()认为猫就是猫不是动物。 （3）数值的除法包含两个运算符：/ 返回一个浮点数，// 返回一个整数。 （4）Python 不支持单字符类型，单字符在 Python 中也是作为一个字符串使用。 （5）迭代器与生成器。 迭代是Python最强大的功能之一，是访问集合元素的一种方式。迭代器对象从集合的第一个元素开始访问，直到所有的元素被访问完结束。迭代器只能往前不会后退。迭代器有两个基本的方法：iter() 和 next()。把一个类作为一个迭代器使用需要在类中实现两个方法 iter() 与 next() 。 123456789101112131415161718class MyNumbers: def __iter__(self): self.a = 1 return self def __next__(self): x = self.a self.a += 1 return x myclass = MyNumbers()myiter = iter(myclass) print(next(myiter))print(next(myiter))print(next(myiter))print(next(myiter))print(next(myiter)) 在 Python 中，使用了 yield 的函数被称为生成器（generator）。生成器是一个返回迭代器的函数，只能用于迭代操作，更简单点理解生成器就是一个迭代器。每次遇到 yield 时函数会暂停并保存当前所有的运行信息，返回 yield 的值, 并在下一次执行 next() 方法时从当前位置继续运行。使用 yield 实现斐波那契数列： 12345678910111213141516171819#!/usr/bin/python3 import sys def fibonacci(n): # 生成器函数 - 斐波那契 a, b, counter = 0, 1, 0 while True: if (counter &gt; n): return yield a a, b = b, a + b counter += 1f = fibonacci(10) # f 是一个迭代器，由生成器返回生成 while True: try: print (next(f), end=" ") except StopIteration: sys.exit() （6）列表的栈与队列 （7）将输出的值转成字符串，可以使用 repr() 或 str() 函数来实现。 str()： 函数返回一个用户易读的表达形式。 repr()： 产生一个解释器易读的表达形式。 （8）类定义了 init() 方法，类的实例化操作会自动调用 init() 方法。类的方法与普通的函数只有一个特别的区别——它们必须有一个额外的第一个参数名称, 按照惯例它的名称是 self。与一般函数定义不同，类方法必须包含参数 self, 且为第一个参数，self 代表的是类的实例。self 的名字并不是规定死的，也可以使用 this，但是最好还是按照约定是用 self。 （9）__private_attrs：两个下划线开头，声明该属性为私有，不能在类的外部被使用或直接访问。在类内部的方法中使用时 self.__private_attrs。 （10）处理从 urls 接收数据的 urllib.request 以及用于发送电子邮件的 smtplib: 123456789101112131415&gt;&gt;&gt; from urllib.request import urlopen&gt;&gt;&gt; for line in urlopen('http://tycho.usno.navy.mil/cgi-bin/timer.pl'):... if 'EST' in line or 'EDT' in line.decode('utf-8'): ... print(line)&gt;&gt;&gt; import smtplib&gt;&gt;&gt; server = smtplib.SMTP('localhost')&gt;&gt;&gt; server.sendmail('soothsayer@example.org', 'jcaesar@example.org',... """To: jcaesar@example.org... From: soothsayer@example.org...... Beware the Ides of March.... """)&gt;&gt;&gt; server.quit() （11）doctest扫描模块并根据程序中内嵌的文档字符串执行测试。通过用户提供的例子，它强化了文档，允许 doctest 模块确认代码的结果是否与文档一致: 12345def average(values): return sum(values) / len(values)import doctestprint(doctest.testmod()) # 自动验证嵌入测试 （12）Python实例总结 https://www.runoob.com/python3/python3-examples.html （13）Python实现查找与排序：https://www.runoob.com/python3/python3-examples.html （14）re.match 尝试从字符串的起始位置匹配一个模式，如果不是起始位置匹配成功的话，match()就返回none。flags是否区分大小写。 1re.match(pattern, string, flags=0) （15）re.match与re.search的区别。re.match只匹配字符串的开始，如果字符串开始不符合正则表达式，则匹配失败，函数返回None；而re.search匹配整个字符串，直到找到一个匹配。 （16）Python连接MySQL 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475import MySQLdb# 显示所有数据库mydb = MySQLdb.Connect( host='localhost', user='root', passwd='root', database='all_news' )mycursor = mydb.cursor()mycursor.execute("SHOW DATABASES")for x in mycursor: print(x)print("*"*20)# 创建数据表# mycursor.execute("CREATE TABLE sites (name VARCHAR(255), url VARCHAR(255))")mycursor.execute("SHOW TABLES")for x in mycursor: print(x)# 修改表# mycursor.execute("ALTER TABLE sites ADD COLUMN id INT AUTO_INCREMENT PRIMARY KEY")# 插入数据# sql = "INSERT INTO sites (name, url) VALUES (%s, %s)"# val = ("RUNOOB", "https://www.runoob.com")# mycursor.execute(sql, val)# mydb.commit() # 数据表内容有更新，必须使用到该语句# print(mycursor.rowcount, "记录插入成功。")# 批量插入# sql = "INSERT INTO sites (name, url) VALUES (%s, %s)"# val = [# ('Google', 'https://www.google.com'),# ('Github', 'https://www.github.com'),# ('Taobao', 'https://www.taobao.com'),# ('stackoverflow', 'https://www.stackoverflow.com/')# ]# mycursor.executemany(sql, val)# mydb.commit() # 数据表内容有更新，必须使用到该语句# print(mycursor.rowcount, "记录插入成功。")# 查询数据print("="*20)mycursor.execute("SELECT * FROM sites")myresult = mycursor.fetchall() # fetchall() 获取所有记录# myresult = mycursor.fetchone() # 读一条数据for x in myresult: print(x)# 删除数据# sql = "DELETE FROM sites WHERE name = 'stackoverflow'"# mycursor.execute(sql)# mydb.commit()# print(mycursor.rowcount, " 条记录删除")# 更新数据sql = "UPDATE sites SET name = 'ZH' WHERE id = 4"mycursor.execute(sql)mydb.commit()print(mycursor.rowcount, " 条记录被修改")# 执行事务# SQL删除记录语句sql = "DELETE FROM EMPLOYEE WHERE AGE &gt; %s" % (20)try: # 执行SQL语句 cursor.execute(sql) # 向数据库提交 db.commit()except: # 发生错误时回滚 db.rollback() （17） 格式化日期： 1234567#!/usr/bin/python3import time# 格式化成2016-03-20 11:45:39形式print (time.strftime("%Y-%m-%d %H:%M:%S", time.localtime()))# 格式化成Sat Mar 28 22:24:24 2016形式print (time.strftime("%a %b %d %H:%M:%S %Y", time.localtime())) （18）Python操作MongoDB 启动服务 1D:\mongodb\bin&gt;mongod --dbpath D:\mongodb\data\db --logpath=D:\mongodb\log\mongo.log --logappend MongoDB 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374# Python 操作MongoDBprint("*"*50)import pymongomyclient = pymongo.MongoClient("mongodb://localhost:27017/")mydb = myclient["runoobdb"]dblist = myclient.list_database_names()if "runoobdb" in dblist: print("数据库已存在！")# 创建集合mycol = mydb["sites"]collist = mydb.list_collection_names()if "sites" in collist: # 判断 sites 集合是否存在 print("集合已存在！")# 添加数据mydict = [ &#123; "_id": 1, "name": "RUNOOB", "cn_name": "菜鸟教程"&#125;, &#123; "_id": 2, "name": "Google", "address": "Google 搜索"&#125;, &#123; "_id": 3, "name": "Facebook", "address": "脸书"&#125;, &#123; "_id": 4, "name": "Taobao", "address": "淘宝"&#125;, &#123; "_id": 5, "name": "Zhihu", "address": "知乎"&#125; ]# x = mycol.insert_one(mydict)# x = mycol.insert_many(mydict)# print(x.inserted_ids)# 修改数据myquery = &#123; "alexa": "10000" &#125;newvalues = &#123; "$set": &#123; "alexa": "12345" &#125; &#125;mycol.update_one(myquery, newvalues)# 输出修改后的 "sites" 集合# for x in mycol.find():# print(x)# 查询数据, find() 方法来查询指定字段的数据，将要返回的字段对应值设置为 1。# for x in mycol.find(): # print(x)# for x in mycol.find(&#123;&#125;,&#123;"_id":0&#125;):# print(x)# myquery = &#123; "name": "RUNOOB" &#125;# for x in mycol.find(myquery):# print(x)# myresult = mycol.find().limit(3)# 输出结果# for x in myresult:# print(x)# 删除数据# myquery = &#123; "name": "知乎" &#125;# myquery = &#123; "name": &#123;"$regex": "^F"&#125; &#125;# mycol.delete_one(myquery)# mycol.delete_many(myquery)# 删除后输出for x in mycol.find(): print(x)# 排序，升序sort("alexa")，降序sort("alexa", -1)# mydoc = mycol.find().sort("alexa")# for x in mydoc:# print(x) （19）WSGI 应用和常见的 Web 框架 部署Django 1234567[uwsgi]socket = 127.0.0.1:3031chdir = /home/foobar/myproject/wsgi-file = myproject/wsgi.pyprocesses = 4threads = 2stats = 127.0.0.1:9191 1uwsgi yourfile.ini 部署Flask 创建文件 myflaskapp.py ，代码如下： 12345from flask import Flaskapp = Flask(__name__)@app.route('/')def index(): return "&lt;span style='color:red'&gt;I am app 1&lt;/span&gt;" 执行以下命令： 1uwsgi --socket 127.0.0.1:3031 --wsgi-file myflaskapp.py --callable app --processes 4 --threads 2 --stats 127.0.0.1:9191 技术交流共享QQ群【机器学习和自然语言QQ群：436303759】： 机器学习和自然语言（QQ群号：436303759）是一个研究深度学习、机器学习、自然语言处理、数据挖掘、图像处理、目标检测、数据科学等AI相关领域的技术群。其宗旨是纯粹的AI技术圈子、绿色的交流环境。本群禁止有违背法律法规和道德的言谈举止。群成员备注格式：城市-自命名。微信订阅号：datathinks]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[笔记5:Elasticsearch复习篇]]></title>
    <url>%2F2019%2F06%2F13%2F%E7%AC%94%E8%AE%B05-Elasticsearch%E5%A4%8D%E4%B9%A0%E7%AF%87%2F</url>
    <content type="text"><![CDATA[摘要：日常学习中对一些知识点进行总结得出该系列文章。学习笔记内容包括前端技术，Django web开发技术，数据库技术如MySQL，MongoDB，PGSQL等等。此外还有一些工具如Dock，ES等等。（本文原创，转载必须注明出处.） 1 ES基本介绍概念介绍 Elasticsearch是一个基于Lucene库的搜索引擎。它提供了一个分布式、支持多租户的全文搜索引擎，它可以快速地储存、搜索和分析海量数据。Elasticsearch可以用于搜索各种文档。它提供可扩展的搜索，具有接近实时的搜索，并支持多租户。Elasticsearch至少需要Java 8。Elasticsearch是分布式的，这意味着索引可以被分成分片，每个分片可以有0个或多个副本。每个节点托管一个或多个分片，并充当协调器将操作委托给正确的分片。相关数据通常存储在同一个索引中，该索引由一个或多个主分片和零个或多个复制分片组成。一旦创建了索引，就不能更改主分片的数量。 集群(Cluster)：集群是一个或多个节点（服务器）的集合，它们共同保存您的整个数据，并提供跨所有节点的联合索引和搜索功能。本质上是一个分布式数据库，允许多台服务器协同工作，每台服务器可以运行多个 Elastic 实例。单个 Elastic 实例称为一个节点（node）。一组节点构成一个集群（cluster）。 节点（Node）：节点是作为集群一部分的单个服务器，存储数据并参与群集的索引和搜索功能。 索引（Index）：索引是具有某些类似特征的文档集合。索引由名称标识（必须全部小写），此名称用于在对其中的文档执行索引，搜索，更新和删除操作时引用索引。 数据管理的顶层单位就叫做 Index（索引）。它是单个数据库的同义词。每个 Index （即数据库）的名字必须是小写。 文档（Document）：文档是可以编制索引的基本信息单元。Index 里面单条的记录称为 Document（文档）。许多条 Document 构成了一个 Index。Document 使用 JSON 格式表示，同一个 Index 里面的 Document，不要求有相同的结构（scheme），但是最好保持相同，这样有利于提高搜索效率。 分片和副本（Shards &amp; Replicas）：索引可能存储大量可能超过单个节点的硬件限制的数据。为了解决这个问题，Elasticsearch提供了将索引细分为多个称为分片的功能。创建索引时，只需定义所需的分片数即可。每个分片本身都是一个功能齐全且独立的“索引”，可以托管在集群中的任何节点上。 副本集很重要：它在分片/节点发生故障时提供高可用性。它允许您扩展搜索量/吞吐量，因为可以在所有副本上并行执行搜索。默认情况下，Elasticsearch中的每个索引都分配了5个主分片和1个副本，这意味着如果群集中至少有两个节点，则索引将包含5个主分片和另外5个副本分片（1个完整副本），总计为每个索引10个分片。 应用场景 在线网上商店，允许客户搜索您销售的产品。在这种情况下，可以使用Elasticsearch存储整个产品目录和库存，并为它们提供搜索和自动填充建议。 收集日志或交易数据，并分析和挖掘此数据以查找趋势，统计信息，摘要或异常。在这种情况下，您可以使用Logstash（Elasticsearch / Logstash / Kibana堆栈的一部分）来收集，聚合和解析数据，然后让Logstash将此数据提供给Elasticsearch。一旦数据在Elasticsearch中，您就可以运行搜索和聚合来挖掘您感兴趣的任何信息。 价格警报平台，允许精通价格的客户指定一条规则，例如“我有兴趣购买特定的电子产品，如果小工具的价格在下个月内从任何供应商降至X美元以下，我希望收到通知” 。在这种情况下，您可以刮取供应商价格，将其推入Elasticsearch并使用其反向搜索功能来匹配价格变动与客户查询，并最终在发现匹配后将警报推送给客户。 核心模块 analysis：主要负责词法分析及语言处理，也就是我们常说的分词，通过该模块可最终形成存储或者搜索的最小单元 Term。 index 模块：主要负责索引的创建工作。 store 模块：主要负责索引的读写，主要是对文件的一些操作，其主要目的是抽象出和平台文件系统无关的存储。 queryParser 模块：主要负责语法分析，把我们的查询语句生成 Lucene 底层可以识别的条件。 search 模块：主要负责对索引的搜索工作。 similarity 模块：主要负责相关性打分和排序的实现。 检索方式 （1）单个词查询：指对一个 Term 进行查询。比如，若要查找包含字符串“Lucene”的文档，则只需在词典中找到 Term“Lucene”，再获得在倒排表中对应的文档链表即可。 （2）AND：指对多个集合求交集。比如，若要查找既包含字符串“Lucene”又包含字符串“Solr”的文档，则查找步骤如下：在词典中找到 Term “Lucene”，得到“Lucene”对应的文档链表。在词典中找到 Term “Solr”，得到“Solr”对应的文档链表。合并链表，对两个文档链表做交集运算，合并后的结果既包含“Lucene”也包含“Solr”。 （3） OR：指多个集合求并集。比如，若要查找包含字符串“Luence”或者包含字符串“Solr”的文档，则查找步骤如下：在词典中找到 Term “Lucene”，得到“Lucene”对应的文档链表。在词典中找到 Term “Solr”，得到“Solr”对应的文档链表。合并链表，对两个文档链表做并集运算，合并后的结果包含“Lucene”或者包含“Solr”。 （4）NOT：指对多个集合求差集。比如，若要查找包含字符串“Solr”但不包含字符串“Lucene”的文档，则查找步骤如下：在词典中找到 Term “Lucene”，得到“Lucene”对应的文档链表。在词典中找到 Term “Solr”，得到“Solr”对应的文档链表。合并链表，对两个文档链表做差集运算，用包含“Solr”的文档集减去包含“Lucene”的文档集，运算后的结果就是包含“Solr”但不包含“Lucene”。 通过上述四种查询方式，我们不难发现，由于 Lucene 是以倒排表的形式存储的。所以在 Lucene 的查找过程中只需在词典中找到这些 Term，根据 Term 获得文档链表，然后根据具体的查询条件对链表进行交、并、差等操作，就可以准确地查到我们想要的结果。相对于在关系型数据库中的“Like”查找要做全表扫描来说，这种思路是非常高效的。虽然在索引创建时要做很多工作，但这种一次生成、多次使用的思路也是很高明的。 ES特性 Elasticsearch可扩展高达PB级的结构化和非结构化数据。 Elasticsearch可以用来替代MongoDB和RavenDB等做文档存储。 Elasticsearch使用非标准化来提高搜索性能。 Elasticsearch是受欢迎的企业搜索引擎之一，目前被许多大型组织使用，如Wikipedia，The Guardian，StackOverflow，GitHub等。 Elasticsearch是开放源代码，可在Apache许可证版本2.0下提供。 ES优点 Elasticsearch是基于Java开发的，这使得它在几乎每个平台上都兼容。 Elasticsearch是实时的，换句话说，一秒钟后，添加的文档可以在这个引擎中搜索得到。 Elasticsearch是分布式的，这使得它易于在任何大型组织中扩展和集成。 通过使用Elasticsearch中的网关概念，创建完整备份很容易。 与Apache Solr相比，在Elasticsearch中处理多租户非常容易。 Elasticsearch使用JSON对象作为响应，这使得可以使用不同的编程语言调用Elasticsearch服务器。 Elasticsearch支持几乎大部分文档类型，但不支持文本呈现的文档类型。 ES缺点 Elasticsearch在处理请求和响应数据方面没有多语言和数据格式支持(仅在JSON中可用)，与Apache Solr不同，Elasticsearch不可以使用CSV，XML等格式。 Elasticsearch也有一些伤脑的问题发生，虽然在极少数情况下才会发生。 2 ES的安装部署本文主要采用Win10下的Elasticsearch安装，当然Linux安装操作起来更加简便了。完成之后对python安装elasticsearch包，并实现交互案例。 第一步：条件检查 Elasticsearch至少需要Java 8，首先需要java -version查看当前版本。 第二步：安装ES，这里采用elasticsearch-7.1.0-windows-x86_64下载地址链接: https://pan.baidu.com/s/1k5AOGpMy8uJEXtA6KoNb7g 提取码: qtmj 。 1234567bin ：运行Elasticsearch实例和插件管理所需的脚本confg: 配置文件所在的目录lib : Elasticsearch使用的库data : Elasticsearch使用的所有数据的存储位置logs : 关于事件和错误记录的文件plugins: 存储所安装插件的地方，比如中文分词工具work : Elasticsearch使用的临时文件，这个文件我这暂时好像没有，可以根据配置文件来 配置这些个文件的目录位置，比如上面的data，logs， 然后去运行 bin/elasticsearch（Mac 或 Linux）或者 bin\elasticsearch.bat (Windows) 即可启动 Elasticsearch 了。我们启动后发现网页并不现实信息，测试下本地网络是否联通： 发现是一般性故障，查询资料显示由于防火墙的问题，经过测试关闭”公用网络防火墙“即可： 之后我们再去ping下本地IP: 这时已经显示ping通状态，再次启动bin\elasticsearch.bat (Windows)，打开http://localhost:9200/显示如下表示成功安装ES。 第三步：Python安装ES， 下载地址是https://www.elastic.co/downloads/elasticsearch。如果在windows下安排部署参考文章http://www.cnblogs.com/viaiu/p/5715200.html。如果是Python开发可以使用pip install elasticsearch安装。 3 Python和ES构建搜索引擎插入数据：打开python运行环境，首先导入【from elasticsearch import Elasticsearch】，然后编写插入数据的方法： 12345678# 插入数据def InsertDatas(): # 默认host为localhost,port为9200.但也可以指定host与port es = Elasticsearch() es.create(index="my_index",doc_type="test_type",id=11,ignore=[400,409],body=&#123;"name":"python","addr":'四川省'&#125;) # 查询结果 result = es.get(index="my_index",doc_type="test_type",id=11) print('单条数据插入完成：\n',result) 实例化Elasticsearch，其中默认为空即host为localhost,port为9200。为空也可以指定网络IP与端口。通过创建索引index和文档类别doc_type，文档id，body为插入数据的内容,其中ES支持的数据仅为JSON类型，ignore=409忽略异常。运行结果如下： 批量插入数据：上面案例我们插入一条信息，查询显示一系列参数包括索引、文档类型、文档ID唯一标识，版本号等。其中资源中包含数据信息，如果我们想插入多条信息可以参考以下代码： 12345678910111213141516# 批量插入数据def AddDatas(): es = Elasticsearch() datas = [&#123; 'name': '美国留给伊拉克的是个烂摊子', 'addr': 'http://view.news.qq.com/zt2011/usa_iraq/index.htm' &#125;,&#123; "name":"python", "addr":'四川省' &#125;] for i,data in enumerate(datas): es.create(index="my_index",doc_type="test_type", id=i,ignore=[400,409],body=data) # 查询结果 result = es.get(index="my_index",doc_type="test_type",id=0) print('\n批量插入数据完成：\n',result['_source']) 我们将数据放在datas列表中，如果我们数据在一个json文件中存储，也可以通过读取文本信息并保存在datas中，之后对其进行插入即可。这里面文件ID我采用枚举的序号，也可以采用随机数或者指定格式。完成所有插入之后我们选择第一条id=0的信息查询，此处查询与上文不同，我们只看文章内容可以采用result[‘_source’]方法，结果如下： 更新数据：如果我们插入数据信息有问题，我们想去修正。可以采用update方法，这里面与我们接触的MySQL，MongoDB等SQL语句差不多。唯一注意的是我们更新数据时候采用{“doc”:{“name”:”python1”,”addr”:”深圳1”}}字典模式，尤其是doc标识不能忘记，代码实现如下： 12345678# 3 更新数据def UpdateDatas(): es = Elasticsearch() es.update(index="my_index",doc_type="test_type", id=11,ignore=[400,409],body=&#123;"doc":&#123;"name":"python1","addr":"深圳1"&#125;&#125;) # 更新结果 result = es.get(index="my_index",doc_type="test_type",id=11) print('\n数据id=11更新完成：\t',result['_source']['name']) 这里我们假如只想查询更新后信息的name字段，可以采用source后面加[‘name’]方法，为什么这么设置呢？请参看插入数据运行结果分析。 删除数据：这里面比较简单，我们指定文档的索引、文档类型和文档ID即可。 12345# 删除数据def DeleteDatas(): es = Elasticsearch() result = es.delete(index='my_index',doc_type='test_type',id=11) print('\n数据id=11删除完成：\t') 条件查询数据：我们通过插入数据构建一个简单我数据信息，如果我们想获取索引中的所有文档可以采用{“query”:{“match_all”:{}}}条件查询，这里面指定关注的是使用的search方法，上文查询数据采用get方法，其实两者都是可以作为查询使用的。代码如下： 1234567# 条件查询def ParaSearch(): es = Elasticsearch() query1 = es.search(index="my_index", body=&#123;"query":&#123;"match_all":&#123;&#125;&#125;&#125;) print('\n查询所有文档\n',query1) query2 = es.search(index="my_index", body=&#123;"query":&#123;"term":&#123;'name':'python'&#125;&#125;&#125;) print('\n查找名字Python的文档:\n',query2['hits']['hits'][0]) 我们获取索引所有文档的信息 获取文档中name为Python的信息 4 技术交流共享QQ群【机器学习和自然语言QQ群：436303759】： 机器学习和自然语言（QQ群号：436303759）是一个研究深度学习、机器学习、自然语言处理、数据挖掘、图像处理、目标检测、数据科学等AI相关领域的技术群。其宗旨是纯粹的AI技术圈子、绿色的交流环境。本群禁止有违背法律法规和道德的言谈举止。群成员备注格式：城市-自命名。微信订阅号：datathinks]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>elasticserch</tag>
        <tag>es</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[笔记6:MongoDB复习篇]]></title>
    <url>%2F2019%2F06%2F13%2F%E7%AC%94%E8%AE%B06-MongoDB%E5%A4%8D%E4%B9%A0%E7%AF%87%2F</url>
    <content type="text"><![CDATA[摘要：日常学习中对一些知识点进行总结得出该系列文章。学习笔记内容包括前端技术，Django web开发技术，数据库技术如MySQL，MongoDB，PGSQL等等。此外还有一些工具如Dock，ES等等。（本文原创，转载必须注明出处.） 1 基本介绍 基本概念 MongoDB**是一种面向文档的数据库管理系统，由C++语言编写的，是一个基于分布式文件存储的开源数据库系统。2007年10月，MongoDB由10gen团队所发展。2009年2月首度推出。在高负载的情况下，添加更多的节点，可以保证服务器性能。MongoDB 旨在为WEB应用提供可扩展的高性能数据存储解决方案。MongoDB 将数据存储为一个文档，数据结构由键值(key=&gt;value)对组成。MongoDB 文档类似于 JSON 对象。字段值可以包含其他文档，数组及文档数组。 优缺点 优点 文档结构的存储方式，能够更便捷的获取数据 内置GridFS，支持大容量的存储：GridFS是一个出色的分布式文件系统，可以支持海量的数据存储。 内置了GridFS了MongoDB，能够满足对大数据集的快速范围查询。 海量数据下，性能优越：在使用场合下，千万级别的文档对象，近10G的数据，对有索引的ID的查询不会比mysql慢，而对非索引字段的查询，则是全面胜出。 mysql实际无法胜任大数据量下任意字段的查询，而mongodb的查询性能实在让我惊讶。写入性能同样很令人满意。 动态查询 全索引支持,扩展到内部对象和内嵌数组：索引通常能够极大的提高查询的效率，如果没有索引，MongoDB在读取数据时必须扫描集合中的每个文件并选取那些符合查询条件的记录。这种扫描全集合的查询效率是非常低的，特别在处理大量的数据时，查询可以要花费几十秒甚至几分钟，这对网站的性能是非常致命的。索引是特殊的数据结构，索引存储在一个易于遍历读取的数据集合中，索引是对数据库表中一列或多列的值进行排序的一种结构。 查询记录分析 快速,就地更新 高效存储二进制大对象 (比如照片和视频) 复制（复制集）和支持自动故障恢复 内置 Auto- Sharding 自动分片支持云级扩展性，分片简单 MapReduce 支持复杂聚合：主要用于处理数据(诸如统计平均值,求和等)，并返回计算后的数据结果。有点类似sql语句中的 count(*)。 商业支持,培训和咨询 缺点 不支持事务操作：事务要求严格的系统（如果银行系统）肯定不能用它。 MongoDB没有如MySQL那样成熟的维护工具 无法进行关联表查询，不适用于关系多的数据 复杂聚合操作通过mapreduce创建，速度慢 模式自由,自由灵活的文件存储格式带来的数据错 MongoDB 在你删除记录后不会在文件系统回收空间。除非你删掉数据库。但是空间没有被浪费 关系型数据库遵循ACID 分布式计算优缺点 NoSQL：not only SQL优缺点 NoSQL数据库分类 MongoDB：C++语言编写，开源，高负载添加节点保证服务器性能。将数据存储为文档，于分布式文件存储的数据库。下载地址：http://www.mongodb.org/downloads MongoDB启动参数 配置MongoDB 解压下载https://www.mongodb.com/download-center/community的ZIP包，并更改文件名为mongodb，在其同目录下创建文件夹data和data\db，log和mongo.log。 打开cmd 进入cd mongodb\bin，执行下面命令启动网络： 1mongod --dbpath D:\mongodb\data\db --logpath=D:\mongodb\log\mongodb.log --logappend 最后，再次打开cmd 进入cd mongodb\bin，执行下面命令：mongo 在浏览器中打开地址： MongoDB概念解析 数据库命令 “show dbs” 命令显示所有数据的列表。 “db” 命令显示当前数据库对象或集合 “use”命令，可以连接到一个指定的数据库。 数据库命名规范 文档（行）：文档是一个键值(key-value)对(即BSON)。MongoDB 的文档不需要设置相同的字段，并且相同的字段不需要相同的数据类型，这与关系型数据库有很大的区别，也是 MongoDB 非常突出的特点。 集合（表）：集合就是 MongoDB 文档组，类似于 RDBMS （关系数据库管理系统：Relational Database Management System)中的表格。 数据类型 mongodb连接 用户名和密码连接到MongoDB，’username:password@hostname/dbname’ 2 MongoDB操作 创建数据库 use DATABASE_NAME 插入数据： 删除数据库： db.dropDatabase() 删除集合 db.collection.drop() 插入文档 ： db.COLLECTION_NAME.insert(document) 更新文档： db.collection.update( criteria, objNew, upsert, multi ) 查找文档 db.userdetails.find()，find() 方法以非结构化的方式来显示所有文档。还有一个 findOne() 方法，它只返回一个文档。 易懂的读取pretty()， 删除文档 remove() 方法， 1234db.collection.remove( &lt;query&gt;, &lt;justOne&gt;) MongoDB 与 RDBMS Where 语句比较 MongoDB AND条件：MongoDB 的 find() 方法可以传入多个键(key)，每个键(key)以逗号隔开，及常规 SQL 的 AND 条件。语法格式如下： 1db.col.find(&#123;key1:value1, key2:value2&#125;).pretty() OR查询条件：MongoDB OR 条件语句使用了关键字 $or,语法格式如下： 1234567&gt;db.col.find( &#123; $or: [ &#123;key1: value1&#125;, &#123;key2:value2&#125; ] &#125;).pretty() $type操作符：操作符是基于BSON类型来检索集合中匹配的数据类型，并返回结果。 limit与skip，limit()方法接受一个数字参数，该参数指定从MongoDB中读取的记录条数。 1db.COLLECTION_NAME.find().limit(NUMBER) skip()方法来跳过指定数量的数据，skip方法同样接受一个数字参数作为跳过的记录条数。 1db.COLLECTION_NAME.find().limit(NUMBER).skip(NUMBER) 排序：sort()方法可以通过参数指定排序的字段，并使用 1 和 -1 来指定排序的方式，其中 1 为升序排列，而-1是用于降序排列。 1db.COLLECTION_NAME.find().sort(&#123;KEY:1&#125;) 索引： ensureIndex() 方法来创建索引。语法中 Key 值为你要创建的索引字段，1为指定按升序创建索引，如果你想按降序来创建索引指定为-1即可。 1db.COLLECTION_NAME.ensureIndex(&#123;KEY:1&#125;) 聚合：处理数据(诸如统计平均值,求和等)，并返回计算后的数据结果。有点类似sql语句中的 count(*)。聚合的方法使用aggregate()。 1db.COLLECTION_NAME.aggregate(AGGREGATE_OPERATION) 管道 复制,复制是将数据同步在多个服务器的过程。复制提供了数据的冗余备份，并在多个服务器上存储数据副本，提高了数据的可用性， 并可以保证数据的安全性。复制还允许您从硬件故障和服务中断中恢复数据。 1mongod --port 27017 --dbpath D:\mongodb\data\db --logpath=D:\mongodb\log\mongodb.log --replSet rs0 以上实例会启动一个名为rs0的MongoDB实例，其端口号为27017。 启动后打开命令提示框并连接上mongoDB服务。 在Mongo客户端使用命令rs.initiate()来启动一个新的副本集。 我们可以使用rs.conf()来查看副本集的配置 查看副本集姿态使用 rs.status() 命令 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950D:\mongodb\bin&gt;mongoMongoDB shell version v4.1.11-262-gc237f4cconnecting to: mongodb://127.0.0.1:27017/?compressors=disabled&amp;gssapiServiceName=mongodbImplicit session: session &#123; "id" : UUID("7109348e-cade-4128-af30-7cf8776302a9") &#125;MongoDB server version: 4.1.11-262-gc237f4c# 添加副本集&gt; rs.add("mongod1.net:27017")2019-05-30T18:57:26.058-0700 E QUERY [js] uncaught exception: Error: assert failed : no config object retrievable from local.system.replset :doassert@src/mongo/shell/assert.js:20:14assert@src/mongo/shell/assert.js:152:9rs.add@src/mongo/shell/utils.js:1441:5@(shell):1:1# 查看当前运行是否是主节点&gt; db.isMaster()&#123; "ismaster" : false, "secondary" : false, "info" : "Does not have a valid replica set config", "isreplicaset" : true, "maxBsonObjectSize" : 16777216, "maxMessageSizeBytes" : 48000000, "maxWriteBatchSize" : 100000, "localTime" : ISODate("2019-05-31T01:57:40.632Z"), "logicalSessionTimeoutMinutes" : 30, "connectionId" : 1, "minWireVersion" : 0, "maxWireVersion" : 8, "readOnly" : false, "ok" : 1, "$clusterTime" : &#123; "clusterTime" : Timestamp(0, 0), "signature" : &#123; "hash" : BinData(0,"AAAAAAAAAAAAAAAAAAAAAAAAAAA="), "keyId" : NumberLong(0) &#125; &#125;, "operationTime" : Timestamp(0, 0)&#125;# 启动一个新的副本集&gt; rs.initiate()# 查看副本集的配置rs0:SECONDARY&gt; rs.conf()# 查看副本集姿态rs0:PRIMARY&gt; rs.status() MongoDB分片，另一种集群，就是分片技术,可以满足MongoDB数据量大量增长的需求。当MongoDB存储海量的数据时，一台机器可能不足以存储数据也足以提供可接受的读写吞吐量。就可以通过在多台机器上分割数据，使得数据库系统能存储和处理更多的数据。 分片实例 MongoDB数据备份：mongodbdump 使用mongodump命令来备份MongoDB数据。该命令可以导出所有数据到指定目录中。mongodump命令可以通过参数指定导出的数据量级转存的服务器。 、 MongoDB数据恢复，mongodbstore MongoDB监控 mongostat 和 mongotop 两个命令来监控MongoDB的运行情况。 3连接MongoDB数据库3.1 Java代码连接MongoDB数据库环境配置，在Java程序中如果要使用MongoDB，你需要确保已经安装了Java环境及MongoDB JDBC 驱动。 首先你必须下载mongo jar包，下载地址：https://github.com/mongodb/mongo-java-driver/downloads, 请确保下载最新版本。 你需要将mongo.jar包含在你的 classpath 中。。 连接数据库，你需要指定数据库名称，如果指定的数据库不存在，mongo会自动创建数据库。 1234567891011121314151617181920212223242526import com.mongodb.MongoClient;import com.mongodb.MongoException;import com.mongodb.WriteConcern;import com.mongodb.DB;import com.mongodb.DBCollection;import com.mongodb.BasicDBObject;import com.mongodb.DBObject;import com.mongodb.DBCursor;import com.mongodb.ServerAddress;import java.util.Arrays;public class MongoDBJDBC&#123; public static void main( String args[] )&#123; try&#123; // 连接到 mongodb 服务 MongoClient mongoClient = new MongoClient( "localhost" , 27017 ); // 连接到数据库 DB db = mongoClient.getDB( "test" ); System.out.println("Connect to database successfully"); boolean auth = db.authenticate(myUserName, myPassword); System.out.println("Authentication: "+auth); &#125;catch(Exception e)&#123; System.err.println( e.getClass().getName() + ": " + e.getMessage() ); &#125; &#125;&#125; 使用com.mongodb.DB类中的createCollection()来创建集合 1234567891011121314151617public class MongoDBJDBC&#123; public static void main( String args[] )&#123; try&#123; // 连接到 mongodb 服务 MongoClient mongoClient = new MongoClient( "localhost" , 27017 ); // 连接到数据库 DB db = mongoClient.getDB( "test" ); System.out.println("Connect to database successfully"); boolean auth = db.authenticate(myUserName, myPassword); System.out.println("Authentication: "+auth); DBCollection coll = db.createCollection("mycol"); System.out.println("Collection created successfully"); &#125;catch(Exception e)&#123; System.err.println( e.getClass().getName() + ": " + e.getMessage() ); &#125; &#125;&#125; 使用com.mongodb.DBCollection类的 getCollection() 方法来获取一个集合 12345678910111213141516171819public class MongoDBJDBC&#123; public static void main( String args[] )&#123; try&#123; // 连接到 mongodb 服务 MongoClient mongoClient = new MongoClient( "localhost" , 27017 ); // 连接到数据库 DB db = mongoClient.getDB( "test" ); System.out.println("Connect to database successfully"); boolean auth = db.authenticate(myUserName, myPassword); System.out.println("Authentication: "+auth); DBCollection coll = db.createCollection("mycol"); System.out.println("Collection created successfully"); DBCollection coll = db.getCollection("mycol"); System.out.println("Collection mycol selected successfully"); &#125;catch(Exception e)&#123; System.err.println( e.getClass().getName() + ": " + e.getMessage() ); &#125; &#125;&#125; 使用com.mongodb.DBCollection类的 insert() 方法来插入一个文档 123456789101112131415161718192021222324public class MongoDBJDBC&#123; public static void main( String args[] )&#123; try&#123; // 连接到 mongodb 服务 MongoClient mongoClient = new MongoClient( "localhost" , 27017 ); // 连接到数据库 DB db = mongoClient.getDB( "test" ); System.out.println("Connect to database successfully"); boolean auth = db.authenticate(myUserName, myPassword); System.out.println("Authentication: "+auth); DBCollection coll = db.getCollection("mycol"); System.out.println("Collection mycol selected successfully"); BasicDBObject doc = new BasicDBObject("title", "MongoDB"). append("description", "database"). append("likes", 100). append("url", "//www.w3cschool.cn/mongodb/"). append("by", "w3cschool.cn"); coll.insert(doc); System.out.println("Document inserted successfully"); &#125;catch(Exception e)&#123; System.err.println( e.getClass().getName() + ": " + e.getMessage() ); &#125; &#125;&#125; 使用com.mongodb.DBCollection类中的 find() 方法来获取集合中的所有文档。 123456789101112131415161718192021222324public class MongoDBJDBC&#123; public static void main( String args[] )&#123; try&#123; // 连接到 mongodb 服务 MongoClient mongoClient = new MongoClient( "localhost" , 27017 ); // 连接到数据库 DB db = mongoClient.getDB( "test" ); System.out.println("Connect to database successfully"); boolean auth = db.authenticate(myUserName, myPassword); System.out.println("Authentication: "+auth); DBCollection coll = db.getCollection("mycol"); System.out.println("Collection mycol selected successfully"); DBCursor cursor = coll.find(); int i=1; while (cursor.hasNext()) &#123; System.out.println("Inserted Document: "+i); System.out.println(cursor.next()); i++; &#125; &#125;catch(Exception e)&#123; System.err.println( e.getClass().getName() + ": " + e.getMessage() ); &#125; &#125;&#125; 使用 com.mongodb.DBCollection 类中的 update() 方法来更新集合中的文档 12345678910111213141516171819202122232425262728293031public class MongoDBJDBC&#123; public static void main( String args[] )&#123; try&#123; // 连接到Mongodb服务 MongoClient mongoClient = new MongoClient( "localhost" , 27017 ); // 连接到你的数据库 DB db = mongoClient.getDB( "test" ); System.out.println("Connect to database successfully"); boolean auth = db.authenticate(myUserName, myPassword); System.out.println("Authentication: "+auth); DBCollection coll = db.getCollection("mycol"); System.out.println("Collection mycol selected successfully"); DBCursor cursor = coll.find(); while (cursor.hasNext()) &#123; DBObject updateDocument = cursor.next(); updateDocument.put("likes","200") coll.update(updateDocument); &#125; System.out.println("Document updated successfully"); cursor = coll.find(); int i=1; while (cursor.hasNext()) &#123; System.out.println("Updated Document: "+i); System.out.println(cursor.next()); i++; &#125; &#125;catch(Exception e)&#123; System.err.println( e.getClass().getName() + ": " + e.getMessage() ); &#125; &#125;&#125; 使用com.mongodb.DBCollection类中的 findOne()方法来获取第一个文档，然后使用remove 方法删除 123456789101112131415161718192021222324252627public class MongoDBJDBC&#123; public static void main( String args[] )&#123; try&#123; // 连接到Mongodb服务 MongoClient mongoClient = new MongoClient( "localhost" , 27017 ); // 连接到你的数据库 DB db = mongoClient.getDB( "test" ); System.out.println("Connect to database successfully"); boolean auth = db.authenticate(myUserName, myPassword); System.out.println("Authentication: "+auth); DBCollection coll = db.getCollection("mycol"); System.out.println("Collection mycol selected successfully"); DBObject myDoc = coll.findOne(); coll.remove(myDoc); DBCursor cursor = coll.find(); int i=1; while (cursor.hasNext()) &#123; System.out.println("Inserted Document: "+i); System.out.println(cursor.next()); i++; &#125; System.out.println("Document deleted successfully"); &#125;catch(Exception e)&#123; System.err.println( e.getClass().getName() + ": " + e.getMessage() ); &#125; &#125;&#125; 4 MongoDB高级教程4.1 关系MongoDB 的关系表示多个文档之间在逻辑上的相互联系。文档间可以通过嵌入和引用来建立联系。MongoDB 中的关系可以是： 1:1 (1对1) 1: N (1对多) N: 1 (多对1) N: N (多对多) 一个用户可以有多个地址，所以是一对多的关系。 使用嵌入式方法，我们可以把用户地址嵌入到用户的文档中 123456789101112131415161718 "_id":ObjectId("52ffc33cd85242f436000001"), "contact": "987654321", "dob": "01-01-1991", "name": "Tom Benzamin", "address": [ &#123; "building": "22 A, Indiana Apt", "pincode": 123456, "city": "Los Angeles", "state": "California" &#125;, &#123; "building": "170 A, Acropolis Apt", "pincode": 456789, "city": "Chicago", "state": "Illinois" &#125;]&#125; 数据保存在单一文档中，比较容易的获取很维护数据。 你可以这样查询用户的地址： 1&gt;db.users.findOne(&#123;"name":"Tom Benzamin"&#125;,&#123;"address":1&#125;) 这种数据结构的缺点是，如果用户和用户地址在不断增加，数据量不断变大，会影响读写性能。 引用式关系，把用户数据文档和用户地址数据文档分开，通过引用文档的 id 字段来建立关系。 12345678910&#123; "_id":ObjectId("52ffc33cd85242f436000001"), "contact": "987654321", "dob": "01-01-1991", "name": "Tom Benzamin", "address_ids": [ ObjectId("52ffc4a5d85242602e000000"), ObjectId("52ffc4a5d85242602e000001") ]&#125; 需要两次查询，第一次查询用户地址的对象id（ObjectId），第二次通过查询的id获取用户的详细地址信息。 12&gt;var result = db.users.findOne(&#123;"name":"Tom Benzamin"&#125;,&#123;"address_ids":1&#125;)&gt;var addresses = db.address.find(&#123;"_id":&#123;"$in":result["address_ids"]&#125;&#125;) 4.2 MapReduce 使用MapReduce 全文检索，MongoDB 在 2.6 版本以后是默认开启全文检索的 创建全文索引 考虑以下 posts 集合的文档数据，包含了文章内容（post_text）及标签(tags)： 1234567&#123; "post_text": "enjoy the mongodb articles on w3cschool.cn", "tags": [ "mongodb", "w3cschool" ]&#125; 我们可以对 post_text 字段建立全文索引，这样我们可以搜索文章内的内容： 1&gt;db.posts.ensureIndex(&#123;post_text:"text"&#125;) 使用全文索引 现在我们已经对 post_text 建立了全文索引，我们可以搜索文章中的关键词w3cschool.cn： 1&gt;db.posts.find(&#123;$text:&#123;$search:"w3cschool.cn"&#125;&#125;) 以下命令返回了如下包含NoSQL关键词的文档数据： 123456789101112131415161718192021222324252627D:\mongodb\bin&gt;mongoMongoDB shell version v4.1.11-262-gc237f4c&gt; show dbsadmin 0.000GBconfig 0.000GBlocal 0.000GBmyinfo 0.000GBw3cschooldb 0.000GByouj 0.000GB&gt; use youjswitched to db youj&gt; db.col.find()&#123; "_id" : ObjectId("5cef550377781c00a00de949"), "title" : "MongoDB 教程", "description" : "MongoDB 是一个 Nosql 数据库", "by" : "w3cschool", "url" : "http://www.w3cschool.cn", "tags" : [ "mongodb", "database", "NoSQL" ], "likes" : 100 &#125;&#123; "_id" : ObjectId("5cef690677781c00a00de94a"), "title" : "MongoDB 教程", "description" : "MongoDB 是一个 Nosql 数据库", "by" : "w3cschool", "url" : "http://www.w3cschool.cn", "tags" : [ "mongodb", "database", "NoSQL" ], "likes" : 100 &#125;&gt; db.col.ensureIndex(&#123;description:"text"&#125;)&#123; "createdCollectionAutomatically" : false, "numIndexesBefore" : 1, "numIndexesAfter" : 2, "ok" : 1&#125;&gt; db.col.find(&#123;$text:&#123;$search:"NoSQL"&#125;&#125;)&#123; "_id" : ObjectId("5cef690677781c00a00de94a"), "title" : "MongoDB 教程", "description" : "MongoDB 是一个 Nosql 数据库", "by" : "w3cschool", "url" : "http://www.w3cschool.cn", "tags" : [ "mongodb", "database", "NoSQL" ], "likes" : 100 &#125;&#123; "_id" : ObjectId("5cef550377781c00a00de949"), "title" : "MongoDB 教程", "description" : "MongoDB 是一个 Nosql 数据库", "by" : "w3cschool", "url" : "http://www.w3cschool.cn", "tags" : [ "mongodb", "database", "NoSQL" ], "likes" : 100 &#125;&gt; 4.3 删除全文索引删除已存在的全文索引，可以使用 find 命令查找索引名： 1&gt;db.col.getIndexes() 通过以上命令获取索引名，本例的索引名为post_text_text，执行以下命令来删除索引： 1&gt; db.col.dropIndex('description_text') 4.4 正则使用 $regex 操作符来设置匹配字符串的正则表达式。 使用正则表达式，使用正则查找包含 w3cschool.cn 字符串的文章： 1&gt;db.posts.find(&#123;post_text:&#123;$regex:"w3cschool.cn"&#125;&#125;) 以上查询也可以写为： 1&gt;db.posts.find(&#123;post_text:/w3cschool.cn/&#125;) 不区分大小写的正则表达式 如果检索需要不区分大小写，我们可以设置 $options 为 $i。以下命令将查找不区分大小写的字符串 w3cschool.cn： 1&gt;db.posts.find(&#123;post_text:&#123;$regex:"w3cschool.cn",$options:"$i"&#125;&#125;) 集合中会返回所有包含字符串 w3cschool.cn 的数据，且不区分大小写： 12345&#123; "_id" : ObjectId("53493d37d852429c10000004"), "post_text" : "hey! this is my post on W3Cschool.cc", "tags" : [ "tutorialspoint" ]&#125; 数组元素使用正则表达式 这在标签的实现上非常有用，如果你需要查找包含以 tutorial 开头的标签数据(tutorial 或 tutorials 或 tutorialpoint 或 tutorialphp)， 你可以使用以下代码： 1&gt;db.posts.find(&#123;tags:&#123;$regex:"tutorial"&#125;&#125;) 优化正则表达式查询 如果你的文档中字段设置了索引，那么使用索引相比于正则表达式匹配查找所有的数据查询速度更快。 如果正则表达式是前缀表达式，所有匹配的数据将以指定的前缀字符串为开始。例如： 如果正则表达式为^tut ，查询语句将查找以 tut 为开头的字符串。 4.5 GridFSGridFS 用于存储和恢复那些超过16M（BSON文件限制）的文件(如：图片、音频、视频等)。也是文件存储的一种方式，但是它是存储在MonoDB的集合中。GridFS 会将大文件对象分割成多个小的chunk(文件片段),一般为256k/个,每个chunk将作为MongoDB的一个文档(document)被存储在chunks集合中。 GridFS 用两个集合来存储一个文件：fs.files与fs.chunks。每个文件的实际内容被存在chunks(二进制数据)中,和文件有关的meta数据(filename,content_type,还有用户自定义的属性)将会被存在files集合中。 以下是简单的 fs.files 集合文档： 1234567&#123; "filename": "test.txt", "chunkSize": NumberInt(261120), "uploadDate": ISODate("2014-04-13T11:32:33.557Z"), "md5": "7b762939321e146569b07f72c62cca4f", "length": NumberInt(646)&#125; 以下是简单的 fs.chunks 集合文档： 12345&#123; "files_id": ObjectId("534a75d19f54bfec8a2fe44b"), "n": NumberInt(0), "data": "Mongo Binary Data"&#125; GridFS 添加文件 现在我们使用 GridFS 的 put 命令来存储 mp3 文件。 调用 MongoDB 安装目录下bin的 mongofiles.exe工具。 打开命令提示符，进入到MongoDB的安装目录的bin目录中，找到mongofiles.exe，并输入下面的代码： 1&gt;mongofiles.exe -d gridfs put song.mp3 GridFS 是存储文件的数据名称。如果不存在该数据库，MongoDB会自动创建。Song.mp3 是音频文件名。使用以下命令来查看数据库中文件的文档： 1&gt;db.fs.files.find() 以上命令执行后返回以下文档数据： 1234567&#123; _id: ObjectId('534a811bf8b4aa4d33fdf94d'), filename: "song.mp3", chunkSize: 261120, uploadDate: new Date(1397391643474), md5: "e4f53379c909f7bed2e9d631e15c1c41", length: 10401959 &#125; 我们可以看到 fs.chunks 集合中所有的区块，以下我们得到了文件的 _id 值，我们可以根据这个 _id 获取区块(chunk)的数据： 1&gt;db.fs.chunks.find(&#123;files_id:ObjectId('534a811bf8b4aa4d33fdf94d')&#125;) 以上实例中，查询返回了 40 个文档的数据，意味着mp3文件被存储在40个区块中。 4.6 自动增长MongoDB 没有像 SQL 一样有自动增长的功能， MongoDB 的id是系统自动生成的12字节唯一标识。但在某些情况下，我们可能需要实现 ObjectId 自动增长功能。由于 MongoDB 没有实现这个功能，我们可以通过编程的方式来实现，以下我们将在 counters 集合中实现id字段自动增长。 使用集合 以下 products 文档。id 字段实现 从 1,2,3,4 到 n 的自动增长功能。 12345&#123; "_id":1, "product_name": "Apple iPhone", "category": "mobiles"&#125; 为此，创建 counters 集合，序列字段值可以实现自动长： 1&gt;db.createCollection("counters") 向 counters 集合中插入以下文档，使用 productid 作为 key: 1234&#123; "_id":"productid", "sequence_value": 0&#125; sequence_value 字段是序列通过自动增长后的一个值。使用以下命令插入 counters 集合的序列文档中： 1&gt;db.counters.insert(&#123;_id:"productid",sequence_value:0&#125;) 创建 Javascript 函数 创建函数 getNextSequenceValue 来作为序列名的输入， 指定的序列会自动增长 1 并返回最新序列值。在本文的实例中序列名为 productid 。 123456789&gt;function getNextSequenceValue(sequenceName)&#123; var sequenceDocument = db.counters.findAndModify( &#123; query:&#123;_id: sequenceName &#125;, update: &#123;$inc:&#123;sequence_value:1&#125;&#125;, new:true &#125;); return sequenceDocument.sequence_value;&#125; 使用 Javascript 函数 使用 getNextSequenceValue 函数创建一个新的文档， 并设置文档 _id 自动为返回的序列值： 123456789&gt;db.products.insert(&#123; "_id":getNextSequenceValue("productid"), "product_name":"Apple iPhone", "category":"mobiles"&#125;)&gt;db.products.insert(&#123; "_id":getNextSequenceValue("productid"), "product_name":"Samsung S3", "category":"mobiles"&#125;) 使用 getNextSequenceValue 函数来设置 _id 字段。为了验证函数是否有效，我们可以使用以下命令读取文档： 1&gt;db.prodcuts.find() 以上命令将返回以下结果，我们发现 _id 字段是自增长的： 123&#123; "_id" : 1, "product_name" : "Apple iPhone", "category" : "mobiles"&#125;&#123; "_id" : 2, "product_name" : "Samsung S3", "category" : "mobiles" &#125; 技术交流共享QQ群【机器学习和自然语言QQ群：436303759】： 机器学习和自然语言（QQ群号：436303759）是一个研究深度学习、机器学习、自然语言处理、数据挖掘、图像处理、目标检测、数据科学等AI相关领域的技术群。其宗旨是纯粹的AI技术圈子、绿色的交流环境。本群禁止有违背法律法规和道德的言谈举止。群成员备注格式：城市-自命名。微信订阅号：datathinks]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>mongodb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[笔记4:Docker复习篇]]></title>
    <url>%2F2019%2F06%2F13%2F%E7%AC%94%E8%AE%B04-Docker%E5%A4%8D%E4%B9%A0%E7%AF%87%2F</url>
    <content type="text"><![CDATA[摘要：日常学习中对一些知识点进行总结得出该系列文章。学习笔记内容包括前端技术，Django web开发技术，数据库技术如MySQL，MongoDB，PGSQL等等。此外还有一些工具如Dock，ES等等。（本文原创，转载必须注明出处.） 导读： 软件开发最大的麻烦事之一就是环境配置，操作系统设置，各种库和组件的安装。只有它们都正确，软件才能运行。如果从一种操作系统里面运行另一种操作系统，通常我们采取的策略就是引入虚拟机，比如在 Windows 系统里面运行 Linux 系统。这种方式有个很大的缺点就是资源占用多、冗余步骤多、启动慢。目前最流行的 Linux 容器解决方案之一就是Docker，它最大优点就是轻量、资源占用少、启动快。本文从什么是Docker？Docker解决什么问题？有哪些好处？如何去部署实现去全面介绍。 0 引言设想这样一个真实案例，假如我们要部署一个Python应用程序，要做哪些工作？ 首先需要python运行环境，比如部署的是python3，而机器上是python2。先装个python3，还要装各种依赖包，机器一些可能的冲突。 装完python之后，发现还要装mysql或者redis。继续下载安装配置。 啥？服务器不用了，需要换一台服务器？那重新来一遍吧。 啥？基础应用做的太好要进行推广，需要指导其他厂商部署？这怎么办？ 可以看出，在 Docker 之前软件行业的运维存在着以下这些痛点: 软件的发布和部署低效又繁琐，而且总是需要人工介入 环境的一致性难移保证 在不同环境之间迁移的成本较高 在完成Docker部署安装之前，我们还是先认识下Docker的优点: 软件构建容易，分发简单 应用得到隔离，依赖被解除 可以完美地用于 CI/CD 快速部署，测试完以后销毁也方便 1 什么是DockerDocker 是一个开源项目，诞生于 2013 年初，最初是 dotCloud 公司内部的一个业余项目。它基于 Google 公司推出的 Go 语言实现。 项目后来加入了 Linux 基金会，遵从了 Apache 2.0 协议，项目代码在 GitHub 上进行维护。2013年3月，dotCloud公司的创始人之一，Docker之父，28岁的Solomon Hykes正式决定，将Docker项目开源，Docker 自开源后受到广泛的关注和讨论。Redhat 已经在其 RHEL6.5 中集中支持 Docker；Google 也在其 PaaS 产品中广泛应用。WIn10下Docker下载地址：链接: https://pan.baidu.com/s/1GlneodbQduUdX9yLRF2hcA 提取码: mqf6 Docker 属于 Linux 容器的一种封装，提供简单易用的容器使用接口。它是目前最流行的 Linux 容器解决方案。有了 Docker，就不用担心环境问题。总体来说，Docker 的接口相当简单，用户可以方便地创建和使用容器，把自己的应用放入容器。容器还可以进行版本管理、复制、分享、修改，就像管理普通的代码一样。 通俗解释Docker Docker的思想来自于集装箱，集装箱解决了什么问题？在一艘大船上，把货物规整的摆放起来。并且各种各样的货物被集装箱标准化了，集装箱和集装箱之间不会互相影响。docker就是类似的理念。现在都流行云计算了，云计算就好比大货轮。docker就是集装箱。 不同的应用程序可能会有不同的应用环境，比如.net开发的网站和php开发的网站依赖的软件就不一样，如果把他们依赖的软件都安装在一个服务器上就要调试很久，而且很麻烦，还会造成一些冲突。这个时候你就要隔离.net开发的网站和php开发的网站。常规来讲，我们可以在服务器上创建不同的虚拟机在不同的虚拟机上放置不同的应用，但是虚拟机开销比较高。docker可以实现虚拟机隔离应用环境的功能，并且开销比虚拟机小，小就意味着省钱了。 开发软件的时候用的是Ubuntu，但是运维管理的都是centos，运维在把你的软件从开发环境转移到生产环境的时候就会遇到一些Ubuntu转centos的问题，比如：有个特殊版本的数据库，只有Ubuntu支持，centos不支持，在转移的过程当中运维就得想办法解决这样的问题。这时候要是有docker你就可以把开发环境直接封装转移给运维，运维直接部署你给他的docker就可以了。而且部署速度快。 在服务器负载方面，如果你单独开一个虚拟机，那么虚拟机会占用空闲内存的，docker部署的话，这些内存就会利用起来。总之docker就是集装箱原理。 2 Docker用途 Docker 的主要用途，目前有三大类。 （1）提供一次性的环境。本地测试的软件、持续集成的时候提供单元测试和构建的环境。 （2）提供弹性的云服务。因为 Docker 容器可以随开随关，很适合动态扩容和缩容。 （3）组建微服务架构。一台机器可以跑多个服务，在本机可以模拟出微服务架构。 应用场景 Web 应用的自动化打包和发布。 自动化测试和持续集成、发布。 在服务型环境中部署和调整数据库或其他的后台应用。 从头编译或者扩展现有的OpenShift或Cloud Foundry平台来搭建自己的PaaS环境。 Docker 能干什么？ 简化配置：这是Docker公司宣传的Docker的主要使用场景。虚拟机的最大好处是能在你的硬件设施上运行各种配置不一样的平台（软件、系统），Docker在降低额外开销的情况下提供了同样的功能。它能让你将运行环境和配置放在代码中然后部署，同一个Docker的配置可以在不同的环境中使用，这样就降低了硬件要求和应用环境之间耦合度。 代码流水线管理：前一个场景对于管理代码的流水线起到了很大的帮助。代码从开发者的机器到最终在生产环境上的部署，需要经过很多的中间环境。而每一个中间环境都有自己微小的差别，Docker给应用提供了一个从开发到上线均一致的环境，让代码的流水线变得简单不少。 提高开发效率：这就带来了一些额外的好处：Docker能提升开发者的开发效率。如果你想看一个详细一点的例子，可以参考Aater在DevOpsDays Austin 2014 大会或者是DockerCon上的演讲。 不同的开发环境中，我们都想把两件事做好。一是我们想让开发环境尽量贴近生产环境，二是我们想快速搭建开发环境。 理想状态中，要达到第一个目标，我们需要将每一个服务都跑在独立的虚拟机中以便监控生产环境中服务的运行状态。然而，我们却不想每次都需要网络连接，每次重新编译的时候远程连接上去特别麻烦。这就是Docker做的特别好的地方，开发环境的机器通常内存比较小，之前使用虚拟的时候，我们经常需要为开发环境的机器加内存，而现在Docker可以轻易的让几十个服务在Docker中跑起来。 隔离应用： 有很多种原因会让你选择在一个机器上运行不同的应用，比如之前提到的提高开发效率的场景等。我们经常需要考虑两点，一是因为要降低成本而进行服务器整合，二是将一个整体式的应用拆分成松耦合的单个服务（译者注：微服务架构）。如果你想了解为什么松耦合的应用这么重要，请参考Steve Yege的这篇论文，文中将Google和亚马逊做了比较。 整合服务器：正如通过虚拟机来整合多个应用，Docker隔离应用的能力使得Docker可以整合多个服务器以降低成本。由于没有多个操作系统的内存占用，以及能在多个实例之间共享没有使用的内存，Docker可以比虚拟机提供更好的服务器整合解决方案。 调试能力：Docker提供了很多的工具，这些工具不一定只是针对容器，但是却适用于容器。它们提供了很多的功能，包括可以为容器设置检查点、设置版本和查看两个容器之间的差别，这些特性可以帮助调试Bug。你可以在《Docker拯救世界》的文章中找到这一点的例证。 多租户： 另外一个Docker有意思的使用场景是在多租户的应用中，它可以避免关键应用的重写。我们一个特别的关于这个场景的例子是为IoT（译者注：物联网）的应用开发一个快速、易用的多租户环境。这种多租户的基本代码非常复杂，很难处理，重新规划这样一个应用不但消耗时间，也浪费金钱。使用Docker，可以为每一个租户的应用层的多个实例创建隔离的环境，这不仅简单而且成本低廉，当然这一切得益于Docker环境的启动速度和其高效的diff命令。 快速部署： 在虚拟机之前，引入新的硬件资源需要消耗几天的时间。虚拟化技术（Virtualization）将这个时间缩短到了分钟级别。而Docker通过为进程仅仅创建一个容器而无需启动一个操作系统，再次将这个过程缩短到了秒级。这正是Google和Facebook都看重的特性。你可以在数据中心创建销毁资源而无需担心重新启动带来的开销。通常数据中心的资源利用率只有30%，通过使用Docker并进行有效的资源分配可以提高资源的利用率。 3 Docker优点 更快速的交付和部署 Docker在整个开发周期都可以完美的辅助你实现快速交付。Docker允许开发者在装有应用和服务本地容器做开发。可以直接集成到可持续开发流程中。例如：开发者可以使用一个标准的镜像来构建一套开发容器，开发完成之后，运维人员可以直接使用这个容器来部署代码。 Docker 可以快速创建容器，快速迭代应用程序，并让整个过程全程可见，使团队中的其他成员更容易理解应用程序是如何创建和工作的。 Docker 容器很轻很快！容器的启动时间是秒级的，大量地节约开发、测试、部署的时间。 高效的部署和扩容 Docker 容器几乎可以在任意的平台上运行，包括物理机、虚拟机、公有云、私有云、个人电脑、服务器等。 这种兼容性可以让用户把一个应用程序从一个平台直接迁移到另外一个。 更高的资源利用率 Docker 对系统资源的利用率很高，一台主机上可以同时运行数千个 Docker 容器。容器除了运行其中应用外，基本不消耗额外的系统资源，使得应用的性能很高，同时系统的开销尽量小。传统虚拟机方式运行 10 个不同的应用就要起 10 个虚拟机，而Docker 只需要启动 10 个隔离的应用即可。 更简单的管理 使用 Docker，只需要小小的修改，就可以替代以往大量的更新工作。所有的修改都以增量的方式被分发和更新，从而实现自动化并且高效的管理。 4 Docker的三个概念 镜像（Image）：类似于虚拟机中的镜像。任何应用程序运行都需要环境，而镜像就是用来提供这种运行环境的。例如一个Ubuntu镜像就是一个包含Ubuntu操作系统环境的模板，同理在该镜像上装上Apache软件，就可以称为Apache镜像。 容器（Container）：类似于一个轻量级的沙盒，可以将其看作一个极简的Linux系统环境（包括root权限、进程空间、用户空间和网络空间等），以及运行在其中的应用程序。Docker引擎利用容器来运行、隔离各个应用。容器是镜像创建的应用实例，可以创建、启动、停止、删除容器，各个容器之间是是相互隔离的，互不影响。注意：镜像本身是只读的，容器从镜像启动时，Docker在镜像的上层创建一个可写层，镜像本身不变。 仓库（Repository）：类似于代码仓库，这里是镜像仓库，是Docker用来集中存放镜像文件的地方。注意与注册服务器（Registry）的区别：注册服务器是存放仓库的地方，一般会有多个仓库；而仓库是存放镜像的地方，一般每个仓库存放一类镜像，每个镜像利用tag进行区分，比如Ubuntu仓库存放有多个版本（12.04、14.04等）的Ubuntu镜像。 5 Docker的使用5.1 Win10下安装Docker 第一步：启动虚拟环境 Win10 系统下安装Docker，首先WIN+X，点击应用和功能；之后点击右侧的“程序和功能”，接着点击左侧栏“启用或关闭Windows功能”，并做以下Hyper-V（hyper-v可以理解为虚拟机平台）的配置： 第二步：安装Toolbox 最新版 Toolbox下载地址 链接: https://pan.baidu.com/s/1Nx3gVdbRrO32elJcRBfiOA 提取码: dsd4 。下载完成后，双击下载的 Docker for Windows Installer 安装文件，一路 Next，点击 Finish 完成安装。docker toolbox是一个工具集，它主要包含以下一些内容： Docker CLI 客户端，用来运行docker引擎创建镜像和容器 Docker Machine. 可以让你在windows的命令行中运行docker引擎命令 Docker Compose. 用来运行docker-compose命令 Kitematic. 这是Docker的GUI版本 Docker QuickStart shell. 这是一个已经配置好Docker的命令行环境 Oracle VM Virtualbox. 虚拟机 安装完成后，Docker 会自动启动。通知栏上会出现个小鲸鱼的图标，这表示 Docker 正在运行。桌边也会出现三个图标，我们可以在命令行执行 docker version 来查看版本号，docker run hello-world 来载入测试镜像测试。 点击WIN+R，输入CMD打开命令行窗口，输入命令docker version结果如下： 运行docker run hello-world 来载入测试镜像测试，效果如下： 第三步：镜像加速 鉴于国内网络问题，后续拉取 Docker 镜像十分缓慢，我们可以需要配置加速器来解决，我使用的是网易的镜像地址：http://hub-mirror.c.163.com。新版的 Docker 使用 /etc/docker/daemon.json（Linux） 或者 %programdata%\docker\config\daemon.json（Windows） 来配置 Daemon。请在该配置文件中加入（没有该文件的话，请先建一个）： 123&#123; "registry-mirrors": ["http://hub-mirror.c.163.com"]&#125; 也可以通过点击小鲸鱼右键settings来设置： 5.2 Docker 常用命令 确认容器有在运行，可以通过 docker ps 来查看 使用 docker stop 容器Name 命令来停止容器 查看docker信息 docker info 删除镜像：docker rmi imageID 停用镜像：docker stop ImageID 重启镜像： docker start imageID 删除容器: docker rm ID docker inspect 来查看 Docker 的底层信息 docker images 查看docker 镜像 从 Docker Hub 网站来搜索镜像，Docker Hub 网址为： https://hub.docker.com/ 使用 docker search 命令来搜索镜像。如搜索httpd的镜像。 使用命令 docker pull 来下载镜像。 命令 docker build ， 从零开始来创建一个新的镜像 容器连接：指定容器绑定的网络地址，比如绑定 127.0.0.1。 6 Docker的实例6.1 Docker 安装 MySQL 第一步：创建MySQL镜像：docker pull mysql 查找Docker Hub上的mysql镜像： docker search mysql 第二步：下载镜像： docker pull mysql:5.6 第三步：查看镜像。列表里查到REPOSITORY为mysql,标签为5.6的镜像。docker images mysql 第四步：使用最新的MySQL镜像。 1234567891011121314151617# docker 中下载 mysqldocker pull mysql#启动，设置初始密码docker run --name bnc-mysql -p 3307:3306 -e MYSQL_ROOT_PASSWORD=root -d mysql#进入容器docker exec -it bnc-mysql bash#登录mysqlmysql -u root -p ALTER user 'root'@'%' IDENTIFIED WITH mysql_native_password BY '123456'; FLUSH PRIVILEGES; #添加远程登录用户CREATE USER 'liaozesong'@'%' IDENTIFIED WITH mysql_native_password BY '123456';GRANT ALL PRIVILEGES ON *.* TO 'liaozesong'@'%'; 第五步：远程连接MySQL数据库。 第六步：新授权用户连接测试。 6.2 Docker 安装 Python项目场景描述：我们使用一个简单的python项目，本项目是中文分词的算法。如何实现Docker安装部署。 第一步： Win10下创建目录文本 选择在D盘下创建docker目录，分别新建三个文件：Dockerfile，app.py，equirements.txt Dockerfile（没有后缀）：一个文本文件，包含了一条条的指令(Instruction)，每一条指令构建一层，因此每一条指令的内容，就是描述该层应当如何构建。创建镜像必须文件。 1234567891011121314# 基于镜像基础FROM python:3.7 # 设置代码文件夹工作目录 /appWORKDIR /app # 复制当前代码文件到容器中 /appADD . /app # 安装所需的包RUN pip install -r requirements.txt # Run app.py when the container launchesCMD ["python", "app.py"] app.py：python项目的源代码，这里测试的单个python文件，如果是一个完整项目，可以将整个文件夹拷贝到这里。 123456789101112131415# coding:utf8"""DESC: Python数据预处理之第一个分词程序范例Author：伏草惟存Prompt: code in Python3 env"""import jiebastr = "道路千万条,安全第一条;行车不规范,亲人两行泪。"print("原句: \n" + str)seg_list = jieba.cut(str)print("分词: \n" + " / ".join(seg_list)) equirements.txt ：所需要的插件，以python为例，其获取方法是cmd命令，进入到【D:\docker】目录，执行命令：pip freeze &gt; requirements.txt 第二步：生成镜像。本文采用的windows环境。docker build -t friendlyhello .命令中最后的点不要忘记，这里表示当前目录 第三步：查看镜像是否生成 第四步：运行镜像程序，这里可以看到分词效果 6.3 Docker 安装 Django项目 第一步：载入镜像。一般采用自构建的方法，本文采用直接pull下载完成。docker pull training/webapp 第二步：运行镜像。docker run -d -P training/webapp python app.py # 多个PORTS端口 第三步：浏览器输入本地ip:端口号，访问网页信息 扩展：使用 Docker 和 Elasticsearch 构建一个全文搜索应用程序https://www.zcfy.cc/article/building-a-full-text-search-app-using-docker-and-elasticsearch 7 技术交流共享QQ群【机器学习和自然语言QQ群：436303759】： 机器学习和自然语言（QQ群号：436303759）是一个研究深度学习、机器学习、自然语言处理、数据挖掘、图像处理、目标检测、数据科学等AI相关领域的技术群。其宗旨是纯粹的AI技术圈子、绿色的交流环境。本群禁止有违背法律法规和道德的言谈举止。群成员备注格式：城市-自命名。微信订阅号：datathinks]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[笔记3:Linux复习篇]]></title>
    <url>%2F2019%2F06%2F13%2F%E7%AC%94%E8%AE%B03-Linux%E5%A4%8D%E4%B9%A0%E7%AF%87%2F</url>
    <content type="text"><![CDATA[摘要：日常学习中对一些知识点进行总结得出该系列文章。学习笔记内容包括前端技术，Django web开发技术，数据库技术如MySQL，MongoDB，PGSQL等等。此外还有一些工具如Dock，ES等等。（本文原创，转载必须注明出处.） 1 Linux介绍1.1 基本知识 应用领域：服务器使用 LAMP（Linux + Apache + MySQL + PHP）或 LNMP（Linux + Nginx+ MySQL + PHP）组合。 linux启动：Linux系统的启动过程可以分为5个阶段： 内核的引导。当计算机打开电源后，首先是BIOS开机自检，按照BIOS中设置的启动设备（通常是硬盘）来启动。操作系统接管硬件以后，首先读入 /boot 目录下的内核文件。 运行 init。init 进程是系统所有进程的起点，你可以把它比拟成系统所有进程的老祖宗，没有这个进程，系统中任何进程都不会启动。init 程序首先是需要读取配置文件 /etc/inittab。/boot /init 进程 系统初始化。在init的配置文件中有这么一行： si::sysinit:/etc/rc.d/rc.sysinit 它调用执行/etc/rc.d/rc.sysinit，而rc.sysinit是一个bash shell的脚本，它主要是完成一些系统初始化的工作，rc.sysinit是每一个运行级别都要首先运行的重要脚本。 建立终端 。rc执行完毕后，返回init。这时基本系统环境已经设置好了，各种守护进程也已经启动了。 用户登录系统。命令行登录、ssh登录、图形界面登录。 运行级别： 运行级别0：系统停机状态，系统默认运行级别不能设为0，否则不能正常启动 运行级别1：单用户工作状态，root权限，用于系统维护，禁止远程登陆 运行级别2：多用户状态(没有NFS) 运行级别3：完全的多用户状态(有NFS)，登陆后进入控制台命令行模式 运行级别4：系统未使用，保留 运行级别5：X11控制台，登陆后进入图形GUI模式 运行级别6：系统正常关闭并重启，默认运行级别不能设为6，否则不能正常启动 窗口切换：默认登录就是第一个窗口，也就是tty1，这个六个窗口分别为tty1,tty2 … tty6，你可以按下Ctrl + Alt + F1 ~ F6 来切换它们。当你进入命令窗口界面后再返回图形界面只要按下Ctrl + Alt + F7 就回来了。 Linux关机：正确的关机流程为：sync &gt; shutdown &gt; reboot &gt; halt 123456789sync 将数据由内存同步到硬盘中。shutdown –h now 立马关机shutdown –h 20:25 系统会在今天20:25关机shutdown –h +10 十分钟后关机shutdown –r now 系统立马重启shutdown –r +10 系统十分钟后重启reboot 就是重启，等同于 shutdown –r nowhalt 关闭系统，等同于shutdown –h now 和 poweroff Linux 忘记密码：进入单用户模式更改一下root密码即可。 远程登陆：Linux系统中是通过ssh服务实现的远程登录功能，默认ssh服务端口号为 22。Window系统上 Linux 远程登录客户端有SecureCRT, Putty, SSH Secure Shell等，本文以Putty为例来登录远程服务器。 putty下载地址：http://www.putty.org/ 1.2 Linux 系统目录结构登录系统后，在当前命令窗口下输入命令ls / /bin：目录存放着最经常使用的命令。 /boot：这里存放的是启动Linux时使用的一些核心文件，包括一些连接文件以及镜像文件。 /dev ：dev是Device(设备)的缩写, 该目录下存放的是Linux的外部设备，在Linux中访问设备的方式和访问文件的方式是相同的。 /etc：存放所有的系统管理所需要的配置文件和子目录。 /home：用户的主目录，在Linux中，每个用户都有一个自己的目录，一般该目录名是以用户的账号命名的。 /lib：这个目录里存放着系统最基本的动态连接共享库，其作用类似于Windows里的DLL文件。几乎所有的应用程序都需要用到这些共享库。 /media：linux系统会自动识别一些设备，例如U盘、光驱等等，当识别后，linux会把识别的设备挂载到这个目录下。 /mnt：系统提供该目录是为了让用户临时挂载别的文件系统的，我们可以将光驱挂载在/mnt/上，然后进入该目录就可以查看光驱里的内容了。 /opt： 这是给主机额外安装软件所摆放的目录。比如你安装一个ORACLE数据库则就可以放到这个目录下。默认是空的。 /proc：这个目录是一个虚拟的目录，它是系统内存的映射，我们可以通过直接访问这个目录来获取系统信息。 /root：该目录为系统管理员，也称作超级权限者的用户主目录。 /sbin：s就是Super User的意思，这里存放的是系统管理员使用的系统管理程序。 /selinux： 这个目录是Redhat/CentOS所特有的目录，Selinux是一个安全机制，类似于windows的防火墙，但是这套机制比较复杂，这个目录就是存放selinux相关的文件的。 /srv： 该目录存放一些服务启动之后需要提取的数据。 /sys：该目录下安装了2.6内核中新出现的一个文件系统 sysfs 。sysfs文件系统集成了下面3种文件系统的信息：针对进程信息的proc文件系统、针对设备的devfs文件系统以及针对伪终端的devpts文件系统。 /tmp：这个目录是用来存放一些临时文件的。 /usr： 这是一个非常重要的目录，用户的很多应用程序和文件都放在这个目录下，类似于windows下的program files目录。 /usr/bin：系统用户使用的应用程序。 /usr/sbin：超级用户使用的比较高级的管理程序和系统守护程序。 /usr/src：内核源代码默认的放置目录。 /var：这个目录中存放着在不断扩充着的东西，我们习惯将那些经常被修改的目录放在这个目录下。包括各种日志文件。 /run：是一个临时文件系统，存储系统启动以来的信息。当系统重启时，这个目录下的文件应该被删掉或清除。如果你的系统上有 /var/run 目录，应该让它指向 run。 1.3 文件操作 文件基本操作 在Linux中第一个字符代表这个文件是目录、文件或链接文件等等。 当为[ d ]则是目录 当为[ - ]则是文件； 若是[ l ]则表示为链接文档(link file)； 若是[ b ]则表示为装置文件里面的可供储存的接口设备(可随机存取装置)； 若是[ c ]则表示为装置文件里面的串行端口设备，例如键盘、鼠标(一次性读取装置)。 12345[root@www /]# ls -ltotal 64dr-xr-xr-x 2 root root 4096 Dec 14 2012 bindr-xr-xr-x 4 root root 4096 Apr 19 2012 boot…… 文件属主和属组 12345[root@www /]# ls -ltotal 64drwxr-xr-x 2 root root 4096 Feb 15 14:46 crondrwxr-xr-x 3 mysql mysql 4096 Apr 21 2014 mysql…… chgrp：更改文件属组，语法：-R：递归更改文件属组，该目录下的所有文件的属组都会更改。 1chgrp [-R] 属组名 文件名 chown：更改文件属主，也可以同时更改文件属组，语法： 12chown [–R] 属主名 文件名chown [-R] 属主名：属组名 文件名 进入 /root 目录（~）将install.log的拥有者改为bin这个账号： 1234[root@www ~] cd ~[root@www ~]# chown bin install.log[root@www ~]# ls -l-rw-r--r-- 1 bin users 68495 Jun 25 08:53 install.log 将install.log的拥有者与群组改回为root： 123[root@www ~]# chown root:root install.log[root@www ~]# ls -l-rw-r--r-- 1 root root 68495 Jun 25 08:53 install.log chmod：更改文件9个属性 Linux文件属性有两种设置方法，一种是数字，一种是符号。Linux文件的基本权限就有九个，分别是owner/group/others三种身份各有自己的read/write/execute权限。各权限的分数对照表如下：r:4，w:2，x:1 每种身份(owner/group/others)各自的三个权限(r/w/x)分数是需要累加的，例如当权限为： [-rwxrwx—-] 分数则是： owner = rwx = 4+2+1 = 7 group = rwx = 4+2+1 = 7 others= —- = 0+0+0 = 0 所以等一下我们设定权限的变更时，该文件的权限数字就是770啦！变更权限的指令chmod的语法是这样的：xyz : 就是刚刚提到的数字类型的权限属性，为 rwx 属性数值的相加。 1chmod [-R] xyz 文件或目录 举例来说，如果要将.bashrc这个文件所有的权限都设定启用，那么命令如下： 12345[root@www ~]# ls -al .bashrc-rw-r--r-- 1 root root 395 Jul 4 11:45 .bashrc[root@www ~]# chmod 777 .bashrc[root@www ~]# ls -al .bashrc-rwxrwxrwx 1 root root 395 Jul 4 11:45 .bashrc 那如果要将权限变成 -rwxr-xr— 呢？那么权限的分数就成为 [4+2+1][4+0+1][4+0+0]=754。 处理目录的常用命令 ls: 列出目录 cd：切换目录 pwd：显示目前的目录 mkdir：创建一个新的目录 rmdir：删除一个空的目录 cp: 复制文件或目录 rm: 移除文件或目录 mv: 移动文件与目录，或修改文件与目录的名称 文件内容查看 cat 由第一行开始显示文件内容 tac 从最后一行开始显示，可以看出 tac 是 cat 的倒著写！ nl 显示的时候，顺道输出行号！ more 一页一页的显示文件内容 less 与 more 类似，但是比 more 更好的是，他可以往前翻页！ head 只看头几行 tail 只看尾巴几行 1.4 用户和用户组实现用户账号的管理，要完成的工作主要有如下几个方面： 用户账号的添加、删除与修改。 1# useradd –d /usr/sam -m sam 用户口令的管理。用户管理的一项重要内容是用户口令的管理。用户账号刚创建时没有口令，但是被系统锁定，无法使用，必须为其指定口令后才可以使用，即使是指定空口令。 用户组的管理。用户组的管理涉及用户组的添加、删除和修改。组的增加、删除和修改实际上就是对/etc/group文件的更新。 1.5 磁盘管理Linux磁盘管理好坏直接关系到整个系统的性能问题。Linux磁盘管理常用三个命令为df、du和fdisk。 df：列出文件系统的整体磁盘使用量,可以利用该命令来获取硬盘被占用了多少空间，目前还剩下多少空间等信息。 du：检查磁盘空间使用量,Linux du命令是对文件和目录磁盘使用的空间的查看。 fdisk：用于磁盘分区 列出所有分区信息：fdisk -l 1.6 vim 编辑器Vim是从 vi 发展出来的一个文本编辑器。代码补完、编译及错误跳转等方便编程的功能特别丰富，在程序员中被广泛使用。 基本上 vi/vim 共分为三种模式，分别是命令模式（Command mode），输入模式（Insert mode）和底线命令模式（Last line mode）。 这三种模式的作用分别是： （1）命令模式 户刚刚启动 vi/vim，便进入了命令模式。此状态下敲击键盘动作会被Vim识别为命令，而非输入字符。比如我们此时按下i，并不会输入一个字符，i被当作了一个命令。以下是常用的几个命令： i 切换到输入模式，以输入字符。 x 删除当前光标所在处的字符。 : 切换到底线命令模式，以在最底一行输入命令。 若想要编辑文本：启动Vim，进入了命令模式，按下i，切换到输入模式。命令模式只有一些最基本的命令，因此仍要依靠底线命令模式输入更多命令。 （2）输入模式 在命令模式下按下i就进入了输入模式。在输入模式中，可以使用以下按键： 字符按键以及Shift组合，输入字符 ENTER，回车键，换行 BACK SPACE，退格键，删除光标前一个字符 DEL，删除键，删除光标后一个字符 方向键，在文本中移动光标 HOME/END，移动光标到行首/行尾 Page Up/Page Down，上/下翻页 Insert，切换光标为输入/替换模式，光标将变成竖线/下划线 ESC，退出输入模式，切换到命令模式 （3）底线命令模式 在命令模式下按下:（英文冒号）就进入了底线命令模式。底线命令模式可以输入单个或多个字符的命令，可用的命令非常多。在底线命令模式中，基本的命令有（已经省略了冒号）： q 退出程序 w 保存文件 按ESC键可随时退出底线命令模式。简单的说，我们可以将这三个模式想成底下的图标来表示： 使用案例 创建文件，按下i进入编辑模式，Esc回到一般模式，:wq保存且退出，:q退出 1$ vim runoob.txt 常见按键 1.7 yum 命令yum（ Yellow dog Updater, Modified）是一个Shell前端软件包管理器。可以自动处理依赖性关系，并且一次安装所有依赖的软体包，无须繁琐地一次次下载、安装。yum提供了查找、安装、删除某一个、一组甚至全部软件包的命令，而且命令简洁而又好记。 常见命令 1.列出所有可更新的软件清单命令：yum check-update 2.更新所有软件命令：yum update 3.仅安装指定的软件命令：yum install 4.仅更新指定的软件命令：yum update 5.列出所有可安裝的软件清单命令：yum list 6.删除软件包命令：yum remove 7.查找软件包 命令：yum search 8.清除缓存命令: yum clean packages: 清除缓存目录下的软件包 yum clean headers: 清除缓存目录下的 headers yum clean oldheaders: 清除缓存目录下旧的 headers yum clean, yum clean all (= yum clean packages; yum clean oldheaders) :清除缓存目录下的软件包及旧的headers 2 Shell编程2.1 基础知识Shell 是一个用 C 语言编写的程序，它是用户使用 Linux 的桥梁。Shell 既是一种命令语言，又是一种程序设计语言。Shell 脚本（shell script），是一种为 shell 编写的脚本程序。shell 和 shell script 是两个不同的概念。新建一个文件 test.sh，扩展名为 sh（sh代表shell）保存为 test.sh，并 cd 到相应目录： 12#!/bin/bashecho "这是我的第一行shell命令" （1）作为可执行程序运行 12chmod +x ./test.sh #使脚本具有执行权限./test.sh #执行脚本 (2) 作为解释器参数运行 12/bin/sh test.sh/bin/php test.php shell变量，变量名和等号之间不能有空格 1your_name="runoob.com" 123for file in `ls /etc`或for file in $(ls /etc) 以上语句将 /etc 下目录的文件名循环出来。 定义只读变量 123#!/bin/bashmyUrl="http://www.google.com"readonly myUrl 删除变量 1unset variable_name 变量类型，运行shell时，会同时存在三种变量： 1) 局部变量 局部变量在脚本或命令中定义，仅在当前shell实例中有效，其他shell启动的程序不能访问局部变量。 2) 环境变量 所有的程序，包括shell启动的程序，都能访问环境变量，有些程序需要环境变量来保证其正常运行。必要的时候shell脚本也可以定义环境变量。 3) shell变量 shell变量是由shell程序设置的特殊变量。shell变量中有一部分是环境变量，有一部分是局部变量，这些变量保证了shell的正常运行 拼接字符串 123456789your_name="runoob"# 使用双引号拼接greeting="hello, "$your_name" !"greeting_1="hello, $&#123;your_name&#125; !"echo $greeting $greeting_1# 使用单引号拼接greeting_2='hello, '$your_name' !'greeting_3='hello, $&#123;your_name&#125; !'echo $greeting_2 $greeting_3 获取字符串长度 12string="abcd"echo $&#123;#string&#125; #输出 4 提取子字符串，第 2 个字符开始截取 4 个字符 12string="runoob is a great site"echo $&#123;string:1:4&#125; # 输出 unoo 查找子字符串，查找字符 i 或 o 的位置(哪个字母先出现就计算哪个)： 12string="runoob is a great site"echo `expr index "$string" io` # 输出 4 shell数组，bash支持一维数组（不支持多维数组），没有限定数组大小。数组元素用&quot;空格&quot;符号分割开。定义数组的一般形式为： 12数组名=(值1 值2 ... 值n)array_name=(value0 value1 value2 value3) ​ 可以不使用连续的下标，而且下标的范围没有限制。 123array_name[0]=value0array_name[1]=value1array_name[n]=valuen 读取数组，使用 @ 符号可以获取数组中的所有元素 123$&#123;数组名[下标]&#125;valuen=$&#123;array_name[n]&#125;echo $&#123;array_name[@]&#125; 获取数组长度 123456# 取得数组元素的个数length=$&#123;#array_name[@]&#125;# 或者length=$&#123;#array_name[*]&#125;# 取得数组单个元素的长度lengthn=$&#123;#array_name[n]&#125; shell注释。每一行加一个 # 号设置多行注释， 2.2 Shell传参向脚本传递参数，脚本内获取参数的格式为：$n。n 代表一个数字，1 为执行脚本的第一个参数，2 为执行脚本的第二个参数，以此类推…… 1234567#!/bin/bashecho "Shell 传递参数实例！";echo "执行的文件名：$0";echo "第一个参数为：$1";echo "第二个参数为：$2";echo "第三个参数为：$3"; 为脚本设置可执行权限，并执行脚本，输出结果如下所示： 1234567$ chmod +x test.sh $ ./test.sh 1 2 3Shell 传递参数实例！执行的文件名：./test.sh第一个参数为：1第二个参数为：2第三个参数为：3 $* 与 $@ 区别： 相同点：都是引用所有参数。 不同点：只有在双引号中体现出来。假设在脚本运行时写了三个参数 1、2、3，，则 “ * “ 等价于 “1 2 3”（传递了一个参数），而 “@” 等价于 “1” “2” “3”（传递了三个参数）。 2.3 运算符原生bash不支持简单的数学运算，但是可以通过其他命令来实现，例如 awk 和 expr，expr 最常用。expr 是一款表达式计算工具，使用它能完成表达式的求值操作。 （1）算数运算符：乘号(*)前边必须加反斜杠()才能实现乘法运算；if…then…fi 是条件语句. （2）关系运算符 （3）布尔运算符 （4）字符串运算符 （5）文件测试运算符 2.4 echo 换行 结果定向到文件 1echo "It is a test" &gt; myfile 显示当前时间 1echo `date` 2.5 printf命令 %s %c %d %f都是格式替代符；%-10s 指一个宽度为10个字符（-表示左对齐，没有则表示右对齐），任何字符都会被显示在10个字符宽的字符内，如果不足则自动以空格填充，超过也会将内容全部显示出来。%-4.2f 指格式化为小数，其中.2指保留2位小数。 3 流程控制3.1 if-else-fiPython,Java、PHP等语言不一样，sh的流程控制不可为空. 3.2 for 3.3 while 使用中使用了 Bash let 命令，它用于执行一个或多个表达式，变量计算中不需要加上 $ 来表示变量 3.4 case 取值后面必须为单词in，每一模式必须以右括号结束。取值可以为变量或常数。匹配发现取值符合某一模式后，其间所有命令开始执行直至 ;;。取值将检测匹配的每一个模式。一旦模式匹配，则执行完匹配模式相应命令后不再继续其他模式。如果无一匹配模式，使用星号 * 捕获该值，再执行后面的命令。下面的脚本提示输入1到4，与每一种模式进行匹配： 123456789101112131415echo '输入 1 到 4 之间的数字:'echo '你输入的数字为:'read aNumcase $aNum in 1) echo '你选择了 1' ;; 2) echo '你选择了 2' ;; 3) echo '你选择了 3' ;; 4) echo '你选择了 4' ;; *) echo '你没有输入 1 到 4 之间的数字' ;;esac 3.5 break12345678910111213#!/bin/bashwhile :do echo -n "输入 1 到 5 之间的数字:" read aNum case $aNum in 1|2|3|4|5) echo "你输入的数字为 $aNum!" ;; *) echo "你输入的数字不是 1 到 5 之间的! 游戏结束" break ;; esacdone 3.6 continue1234567891011121314#!/bin/bashwhile :do echo -n "输入 1 到 5 之间的数字: " read aNum case $aNum in 1|2|3|4|5) echo "你输入的数字为 $aNum!" ;; *) echo "你输入的数字不是 1 到 5 之间的!" continue echo "游戏结束" ;; esacdone 4 函数4.1 函数的使用 123456789101112#!/bin/bashfunWithReturn()&#123; echo "这个函数会对输入的两个数字进行相加运算..." echo "输入第一个数字: " read aNum echo "输入第二个数字: " read anotherNum echo "两个数字分别为 $aNum 和 $anotherNum !" return $(($aNum+$anotherNum))&#125;funWithReturnecho "输入的两个数字之和为 $? !" 函数返回值在调用该函数后通过 $? 来获得。 4.2 函数参数在Shell中，调用函数时可以向其传递参数。在函数体内部，通过 n 的形式来获取参数的值，例如，​$1表示第一个参数，$2表示第二个参数… 12345678910funWithParam()&#123; echo "第一个参数为 $1 !" echo "第二个参数为 $2 !" echo "第十个参数为 $10 !" echo "第十个参数为 $&#123;10&#125; !" echo "第十一个参数为 $&#123;11&#125; !" echo "参数总数有 $# 个!" echo "作为一个字符串输出所有参数 $* !"&#125;funWithParam 1 2 3 4 5 6 7 8 9 34 73 4.3 重定向 输出重定向 12345$ echo "菜鸟教程：www.runoob.com" &gt;&gt; users$ cat users菜鸟教程：www.runoob.com菜鸟教程：www.runoob.com$ 输入重定向 一般情况下，每个 Unix/Linux 命令运行时都会打开三个文件： 标准输入文件(stdin)：stdin的文件描述符为0，Unix程序默认从stdin读取数据。 标准输出文件(stdout)：stdout 的文件描述符为1，Unix程序默认向stdout输出数据。 标准错误文件(stderr)：stderr的文件描述符为2，Unix程序会向stderr流中写入错误信息。 默认情况下，command &gt; file 将 stdout 重定向到 file，command &lt; file 将stdin 重定向到 file。注意：0 是标准输入（STDIN），1 是标准输出（STDOUT），2 是标准错误输出（STDERR）。 1$ command 2 &gt; file 4.4 文件包含 test1.sh 代码如下： 12#!/bin/bashurl="http://www.runoob.com" test2.sh 代码如下： 123456#!/bin/bash#使用 . 号来引用test1.sh 文件. ./test1.sh# 或者使用以下包含文件代码# source ./test1.shecho "菜鸟教程官网地址：$url" 接下来，我们为 test2.sh 添加可执行权限并执行： 123$ chmod +x test2.sh $ ./test2.sh 菜鸟教程官网地址：http://www.runoob.com 注：*被包含的文件 test1.sh 不需要可执行权限* 5 NginxNginx(“engine x”)是一款是由俄罗斯的程序设计师Igor Sysoev所开发高性能的 Web和 反向代理 服务器，也是一个 IMAP/POP3/SMTP 代理服务器。在高连接并发的情况下，Nginx是Apache服务器不错的替代品。 技术交流共享QQ群【机器学习和自然语言QQ群：436303759】： 机器学习和自然语言（QQ群号：436303759）是一个研究深度学习、机器学习、自然语言处理、数据挖掘、图像处理、目标检测、数据科学等AI相关领域的技术群。其宗旨是纯粹的AI技术圈子、绿色的交流环境。本群禁止有违背法律法规和道德的言谈举止。群成员备注格式：城市-自命名。微信订阅号：datathinks]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[笔记2:BootStrap复习篇]]></title>
    <url>%2F2019%2F06%2F13%2F%E7%AC%94%E8%AE%B02-BootStrap%E5%A4%8D%E4%B9%A0%E7%AF%87%2F</url>
    <content type="text"><![CDATA[摘要：日常学习中对一些知识点进行总结得出该系列文章。学习笔记内容包括前端技术，Django web开发技术，数据库技术如MySQL，MongoDB，PGSQL等等。此外还有一些工具如Dock，ES等等。（本文原创，转载必须注明出处.） 1 Bootstrap 基本结构Bootstrap，来自 Twitter，是目前最受欢迎的前端框架。Bootstrap 是基于 HTML、CSS、JAVASCRIPT 的，它简洁灵活，使得 Web 开发更加快捷。 Bootstrap 是一个用于快速开发 Web 应用程序和网站的前端框架。Bootstrap 是基于 HTML、CSS、JAVASCRIPT 的。 （1）创建html框架：在pycharm中输入!，之后tab即可 （2）Bootstrap 安装。您可以从 http://getbootstrap.com/ 上下载 Bootstrap 的最新版本。包含了 jquery.js、bootstrap.min.js 和 bootstrap.min.css 文件，用于让一个常规的 HTML 文件变为使用了 Bootstrap 的模板。国内推荐使用 Staticfile CDN 上的库： 12345678&lt;!-- 新 Bootstrap 核心 CSS 文件 --&gt;&lt;link href="https://cdn.staticfile.org/twitter-bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet"&gt;&lt;!-- jQuery文件。务必在bootstrap.min.js 之前引入 --&gt;&lt;script src="https://cdn.staticfile.org/jquery/2.1.1/jquery.min.js"&gt;&lt;/script&gt;&lt;!-- 最新的 Bootstrap 核心 JavaScript 文件 --&gt;&lt;script src="https://cdn.staticfile.org/twitter-bootstrap/3.3.7/js/bootstrap.min.js"&gt;&lt;/script&gt; 2 Bootstrap CSSBootstrap 3 默认的 CSS 本身就对移动设备友好支持。Bootstrap 3 的设计目标是移动设备优先，然后才是桌面设备。这实际上是一个非常及时的转变，因为现在越来越多的用户使用移动设备。为了让 Bootstrap 开发的网站对移动设备友好，确保适当的绘制和触屏缩放，需要在网页的 head 之中添加 viewport meta 标签，如下所示： 1&lt;meta name="viewport" content="width=device-width, initial-scale=1.0"&gt; width 属性控制设备的宽度。假设您的网站将被带有不同屏幕分辨率的设备浏览，那么将它设置为 device-width 可以确保它能正确呈现在不同设备上。 initial-scale=1.0 确保网页加载时，以 1:1 的比例呈现，不会有任何的缩放。 响应式图像 1&lt;img src="..." class="img-responsive" alt="响应式图像"&gt; img-responsive class 为图像赋予了 max-width: 100%; 和 height: auto; 属性，可以让图像按比例缩放，不超过其父元素的尺寸。 12345.img-responsive &#123; display: block; height: auto; max-width: 100%;&#125; 这表明相关的图像呈现为 block。当您把元素的 display 属性设置为 block，以块级元素显示。设置 height:auto，相关元素的高度取决于浏览器。 设置 max-width 为 100% 会重写任何通过 width 属性指定的宽度。这让图片对响应式布局的支持更友好。 如果需要让使用了 .img-responsive 类的图片水平居中，请使用 .center-block 类，不要用 .text-center。 body {margin: 0;} 来移除 body 的边距。请看下面有关 body 的设置： 1234567body &#123; font-family: "Helvetica Neue", Helvetica, Arial, sans-serif; font-size: 14px; line-height: 1.428571429; color: #333333; background-color: #ffffff;&#125; 第一条规则设置 body 的默认字体样式为 “Helvetica Neue”, Helvetica, Arial, sans-serif。最后一条规则设置默认的背景颜色为白色。 2.1 网格系统Bootstrap 提供了一套响应式、移动设备优先的流式网格系统，随着屏幕或视口（viewport）尺寸的增加，系统会自动分为最多12列。其实就是页面布局。 bootstrap网格系统（Grid System）：Bootstrap 包含了一个响应式的、移动设备优先的、不固定的网格系统，可以随着设备或视口大小的增加而适当地扩展到 12 列。它包含了用于简单的布局选项的预定义类，也包含了用于生成更多语义布局的功能强大的混合类。 网格系统工作原理 行必须放置在 .container class 内，以便获得适当的对齐（alignment）和内边距（padding）。 使用行来创建列的水平组。 内容应该放置在列内，且唯有列可以是行的直接子元素。 预定义的网格类，比如 .row 和 .col-xs-4，可用于快速创建网格布局。LESS 混合类可用于更多语义布局。 列通过内边距（padding）来创建列内容之间的间隙。该内边距是通过 .rows 上的外边距（margin）取负，表示第一列和最后一列的行偏移。 网格系统是通过指定您想要横跨的十二个可用的列来创建的。例如，要创建三个相等的列，则使用三个 .col-xs-4。 堆叠水平 12345678910111213141516171819202122232425262728293031&lt;div class="container"&gt; &lt;h1&gt;Hello, world!&lt;/h1&gt; &lt;div class="row"&gt; &lt;div class="col-md-6" style="background-color: #dedef8; box-shadow: inset 1px -1px 1px #444, inset -1px 1px 1px #444;"&gt; &lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. &lt;/p&gt; &lt;p&gt;Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. &lt;/p&gt; &lt;/div&gt; &lt;div class="col-md-6" style="background-color: #dedef8;box-shadow: inset 1px -1px 1px #444, inset -1px 1px 1px #444;"&gt; &lt;p&gt;Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium. &lt;/p&gt; &lt;p&gt; Neque porro quisquam est, qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt ut labore et dolore magnam aliquam quaerat voluptatem. &lt;/p&gt; &lt;/div&gt; &lt;/div&gt;&lt;/div&gt; ... 元素被添加，确保居中和最大宽度。 一旦添加了容器，接下来您需要考虑以行为单位。添加 ...，并在行内添加列 。 网格中的每一行是由 12 个单元组成的，您可以使用这些单元定义列的尺寸。在我们的实例中，有两个列，每个列由 6 个单元组成，即 6+6=12。 2.2 手机平板电脑通用尺寸https://www.runoob.com/bootstrap/bootstrap-grid-system-example3.html 2.3 排版123456789101112&lt;small&gt;本行内容是在标签内&lt;/small&gt;&lt;br&gt;&lt;strong&gt;本行内容是在标签内&lt;/strong&gt;&lt;br&gt;&lt;em&gt;本行内容是在标签内，并呈现为斜体&lt;/em&gt;&lt;br&gt;&lt;p class="text-left"&gt;向左对齐文本&lt;/p&gt;&lt;p class="text-center"&gt;居中对齐文本&lt;/p&gt;&lt;p class="text-right"&gt;向右对齐文本&lt;/p&gt;&lt;p class="text-muted"&gt;本行内容是减弱的&lt;/p&gt;&lt;p class="text-primary"&gt;本行内容带有一个 primary class&lt;/p&gt;&lt;p class="text-success"&gt;本行内容带有一个 success class&lt;/p&gt;&lt;p class="text-info"&gt;本行内容带有一个 info class&lt;/p&gt;&lt;p class="text-warning"&gt;本行内容带有一个 warning class&lt;/p&gt;&lt;p class="text-danger"&gt;本行内容带有一个 danger class&lt;/p&gt; 缩写 地址 12345678910&lt;address&gt; &lt;strong&gt;Some Company, Inc.&lt;/strong&gt;&lt;br&gt; 007 street&lt;br&gt; Some City, State XXXXX&lt;br&gt; &lt;abbr title="Phone"&gt;P:&lt;/abbr&gt; (123) 456-7890&lt;/address&gt;&lt;address&gt; &lt;strong&gt;Full Name&lt;/strong&gt;&lt;br&gt; &lt;a href="mailto:#"&gt;mailto@somedomain.com&lt;/a&gt;&lt;/address&gt; 引用 12345678910111213&lt;blockquote&gt; &lt;p&gt; 这是一个默认的引用实例。这是一个默认的引用实例。这是一个默认的引用实例。这是一个默认的引用实例。这是一个默认的引用实例。这是一个默认的引用实例。这是一个默认的引用实例。这是一个默认的引用实例。 &lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote&gt; 这是一个带有源标题的引用。 &lt;small&gt;Someone famous in &lt;cite title="Source Title"&gt;Source Title&lt;/cite&gt;&lt;/small&gt;&lt;/blockquote&gt;&lt;blockquote class="pull-right"&gt; 这是一个向右对齐的引用。 &lt;small&gt;Someone famous in &lt;cite title="Source Title"&gt;Source Title&lt;/cite&gt;&lt;/small&gt;&lt;/blockquote&gt; 列表 123456789101112131415161718192021222324252627282930313233343536373839404142&lt;h4&gt;有序列表&lt;/h4&gt;&lt;ol&gt; &lt;li&gt;Item 1&lt;/li&gt; &lt;li&gt;Item 2&lt;/li&gt; &lt;li&gt;Item 3&lt;/li&gt; &lt;li&gt;Item 4&lt;/li&gt;&lt;/ol&gt;&lt;h4&gt;无序列表&lt;/h4&gt;&lt;ul&gt; &lt;li&gt;Item 1&lt;/li&gt; &lt;li&gt;Item 2&lt;/li&gt; &lt;li&gt;Item 3&lt;/li&gt; &lt;li&gt;Item 4&lt;/li&gt;&lt;/ul&gt;&lt;h4&gt;未定义样式列表&lt;/h4&gt;&lt;ul class="list-unstyled"&gt; &lt;li&gt;Item 1&lt;/li&gt; &lt;li&gt;Item 2&lt;/li&gt; &lt;li&gt;Item 3&lt;/li&gt; &lt;li&gt;Item 4&lt;/li&gt;&lt;/ul&gt;&lt;h4&gt;内联列表&lt;/h4&gt;&lt;ul class="list-inline"&gt; &lt;li&gt;Item 1&lt;/li&gt; &lt;li&gt;Item 2&lt;/li&gt; &lt;li&gt;Item 3&lt;/li&gt; &lt;li&gt;Item 4&lt;/li&gt;&lt;/ul&gt;&lt;h4&gt;定义列表&lt;/h4&gt;&lt;dl&gt; &lt;dt&gt;Description 1&lt;/dt&gt; &lt;dd&gt;Item 1&lt;/dd&gt; &lt;dt&gt;Description 2&lt;/dt&gt; &lt;dd&gt;Item 2&lt;/dd&gt;&lt;/dl&gt;&lt;h4&gt;水平的定义列表&lt;/h4&gt;&lt;dl class="dl-horizontal"&gt; &lt;dt&gt;Description 1&lt;/dt&gt; &lt;dd&gt;Item 1&lt;/dd&gt; &lt;dt&gt;Description 2&lt;/dt&gt; &lt;dd&gt;Item 2&lt;/dd&gt;&lt;/dl&gt; 其他排版 2.4 代码Bootstrap 允许您以两种方式显示代码：开始和结束标签使用了 unicode 变体： &lt; 和 &gt;。 第一种是 标签。如果您想要内联显示代码，那么您应该使用 标签。 第二种是 标签。如果代码需要被显示为一个独立的块元素或者代码有多行，那么您应该使用 标签。 2.5 表格Bootstrap 提供了一个清晰的创建表格的布局。下表列出了 Bootstrap 支持的一些表格元素： 表格类，通过把任意的 .table 包在 .table-responsive class 内，您可以让表格水平滚动以适应小型设备（小于 768px）。当在大于 768px 宽的大型设备上查看时，您将看不到任何的差别。 , 和 类 基本表格 12345678910111213141516171819&lt;table class="table"&gt; &lt;caption&gt;基本的表格布局&lt;/caption&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;名称&lt;/th&gt; &lt;th&gt;城市&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Tanmay&lt;/td&gt; &lt;td&gt;Bangalore&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Sachin&lt;/td&gt; &lt;td&gt;Mumbai&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; 2.6 表单Bootstrap 提供了下列类型的表单布局：垂直表单（默认），内联表单，水平表单。创建基本表单的步骤： 向父 元素添加 role=”form”。 把标签和控件放在一个带有 class .form-group 的 中。这是获取最佳间距所必需的。 向所有的文本元素 、 和 添加 class =”form-control“ 。 1234567891011121314151617&lt;form role="form"&gt; &lt;div class="form-group"&gt; &lt;label for="name"&gt;名称&lt;/label&gt; &lt;input type="text" class="form-control" id="name" placeholder="请输入名称"&gt; &lt;/div&gt; &lt;div class="form-group"&gt; &lt;label for="inputfile"&gt;文件输入&lt;/label&gt; &lt;input type="file" id="inputfile"&gt; &lt;p class="help-block"&gt;这里是块级帮助文本的实例。&lt;/p&gt; &lt;/div&gt; &lt;div class="checkbox"&gt; &lt;label&gt; &lt;input type="checkbox"&gt;请打勾 &lt;/label&gt; &lt;/div&gt; &lt;button type="submit" class="btn btn-default"&gt;提交&lt;/button&gt;&lt;/form&gt; 内联表单，请向 标签添加 class .form-inline。使用 class .sr-only，您可以隐藏内联表单的标签。 12345678910111213141516&lt;form class="form-inline" role="form"&gt; &lt;div class="form-group"&gt; &lt;label class="sr-only" for="name"&gt;名称&lt;/label&gt; &lt;input type="text" class="form-control" id="name" placeholder="请输入名称"&gt; &lt;/div&gt; &lt;div class="form-group"&gt; &lt;label class="sr-only" for="inputfile"&gt;文件输入&lt;/label&gt; &lt;input type="file" id="inputfile"&gt; &lt;/div&gt; &lt;div class="checkbox"&gt; &lt;label&gt; &lt;input type="checkbox"&gt;请打勾 &lt;/label&gt; &lt;/div&gt; &lt;button type="submit" class="btn btn-default"&gt;提交&lt;/button&gt;&lt;/form&gt; 水平表单 水平表单与其他表单不仅标记的数量上不同，而且表单的呈现形式也不同。如需创建一个水平布局的表单，请按下面的几个步骤进行： 向父 元素添加 class .form-horizontal。 把标签和控件放在一个带有 class .form-group 的 中。 向标签添加 class .control-label。 12345678910111213141516171819202122232425262728&lt;form class="form-horizontal" role="form"&gt; &lt;div class="form-group"&gt; &lt;label for="firstname" class="col-sm-2 control-label"&gt;名字&lt;/label&gt; &lt;div class="col-sm-10"&gt; &lt;input type="text" class="form-control" id="firstname" placeholder="请输入名字"&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="form-group"&gt; &lt;label for="lastname" class="col-sm-2 control-label"&gt;姓&lt;/label&gt; &lt;div class="col-sm-10"&gt; &lt;input type="text" class="form-control" id="lastname" placeholder="请输入姓"&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="form-group"&gt; &lt;div class="col-sm-offset-2 col-sm-10"&gt; &lt;div class="checkbox"&gt; &lt;label&gt; &lt;input type="checkbox"&gt;请记住我 &lt;/label&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="form-group"&gt; &lt;div class="col-sm-offset-2 col-sm-10"&gt; &lt;button type="submit" class="btn btn-default"&gt;登录&lt;/button&gt; &lt;/div&gt; &lt;/div&gt;&lt;/form&gt; Bootstrap 支持最常见的表单控件，主要是 input、textarea、checkbox、radio 和 select。 输入框 123456&lt;form role="form"&gt; &lt;div class="form-group"&gt; &lt;label for="name"&gt;标签&lt;/label&gt; &lt;input type="text" class="form-control" placeholder="文本输入"&gt; &lt;/div&gt; &lt;/form&gt; 文本框，多行输入使用文本框 textarea。必要时可以改变 rows 属性（较少的行 = 较小的盒子，较多的行 = 较大的盒子）。 123456&lt;form role="form"&gt; &lt;div class="form-group"&gt; &lt;label for="name"&gt;文本框&lt;/label&gt; &lt;textarea class="form-control" rows="3"&gt;&lt;/textarea&gt; &lt;/div&gt;&lt;/form&gt; 复选框和单选框 复选框和单选按钮用于让用户从一系列预设置的选项中进行选择。 当创建表单时，如果您想让用户从列表中选择若干个选项时，请使用 checkbox。如果您限制用户只能选择一个选项，请使用 radio。 对一系列复选框和单选框使用 .checkbox-inline 或 .radio-inline class，控制它们显示在同一行上。 1234567891011121314151617181920212223242526272829303132333435&lt;label for="name"&gt;默认的复选框和单选按钮的实例&lt;/label&gt;&lt;div class="checkbox"&gt; &lt;label&gt;&lt;input type="checkbox" value=""&gt;选项 1&lt;/label&gt;&lt;/div&gt;&lt;div class="checkbox"&gt; &lt;label&gt;&lt;input type="checkbox" value=""&gt;选项 2&lt;/label&gt;&lt;/div&gt;&lt;div class="radio"&gt; &lt;label&gt; &lt;input type="radio" name="optionsRadios" id="optionsRadios1" value="option1" checked&gt; 选项 1 &lt;/label&gt;&lt;/div&gt;&lt;div class="radio"&gt; &lt;label&gt; &lt;input type="radio" name="optionsRadios" id="optionsRadios2" value="option2"&gt;选项 2 - 选择它将会取消选择选项 1 &lt;/label&gt;&lt;/div&gt;&lt;label for="name"&gt;内联的复选框和单选按钮的实例&lt;/label&gt;&lt;div&gt; &lt;label class="checkbox-inline"&gt; &lt;input type="checkbox" id="inlineCheckbox1" value="option1"&gt; 选项 1 &lt;/label&gt; &lt;label class="checkbox-inline"&gt; &lt;input type="checkbox" id="inlineCheckbox2" value="option2"&gt; 选项 2 &lt;/label&gt; &lt;label class="checkbox-inline"&gt; &lt;input type="checkbox" id="inlineCheckbox3" value="option3"&gt; 选项 3 &lt;/label&gt; &lt;label class="radio-inline"&gt; &lt;input type="radio" name="optionsRadiosinline" id="optionsRadios3" value="option1" checked&gt; 选项 1 &lt;/label&gt; &lt;label class="radio-inline"&gt; &lt;input type="radio" name="optionsRadiosinline" id="optionsRadios4" value="option2"&gt; 选项 2 &lt;/label&gt;&lt;/div&gt; 选择框，当您想让用户从多个选项中进行选择，但是默认情况下只能选择一个选项时，则使用选择框。 使用 展示列表选项，通常是那些用户很熟悉的选择列表，比如州或者数字。 使用 multiple=”multiple” 允许用户选择多个选项。 1234567891011121314151617181920&lt;form role="form"&gt; &lt;div class="form-group"&gt; &lt;label for="name"&gt;选择列表&lt;/label&gt; &lt;select class="form-control"&gt; &lt;option&gt;1&lt;/option&gt; &lt;option&gt;2&lt;/option&gt; &lt;option&gt;3&lt;/option&gt; &lt;option&gt;4&lt;/option&gt; &lt;option&gt;5&lt;/option&gt; &lt;/select&gt; &lt;label for="name"&gt;可多选的选择列表&lt;/label&gt; &lt;select multiple class="form-control"&gt; &lt;option&gt;1&lt;/option&gt; &lt;option&gt;2&lt;/option&gt; &lt;option&gt;3&lt;/option&gt; &lt;option&gt;4&lt;/option&gt; &lt;option&gt;5&lt;/option&gt; &lt;/select&gt; &lt;/div&gt;&lt;/form&gt; 静态控件，在一个水平表单内的表单标签后放置纯文本时，请在 上使用 class .form-control-static。 1234567891011121314&lt;form class="form-horizontal" role="form"&gt; &lt;div class="form-group"&gt; &lt;label class="col-sm-2 control-label"&gt;Email&lt;/label&gt; &lt;div class="col-sm-10"&gt; &lt;p class="form-control-static"&gt;email@example.com&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="form-group"&gt; &lt;label for="inputPassword" class="col-sm-2 control-label"&gt;密码&lt;/label&gt; &lt;div class="col-sm-10"&gt; &lt;input type="password" class="form-control" id="inputPassword" placeholder="请输入密码"&gt; &lt;/div&gt; &lt;/div&gt;&lt;/form&gt; 2.7 按钮以下样式可用于, , 或 元素上： 按钮实例 1234567891011121314&lt;!-- 标准的按钮 --&gt;&lt;button type="button" class="btn btn-default"&gt;默认按钮&lt;/button&gt;&lt;!-- 提供额外的视觉效果，标识一组按钮中的原始动作 --&gt;&lt;button type="button" class="btn btn-primary"&gt;原始按钮&lt;/button&gt;&lt;!-- 表示一个成功的或积极的动作 --&gt;&lt;button type="button" class="btn btn-success"&gt;成功按钮&lt;/button&gt;&lt;!-- 信息警告消息的上下文按钮 --&gt;&lt;button type="button" class="btn btn-info"&gt;信息按钮&lt;/button&gt;&lt;!-- 表示应谨慎采取的动作 --&gt;&lt;button type="button" class="btn btn-warning"&gt;警告按钮&lt;/button&gt;&lt;!-- 表示一个危险的或潜在的负面动作 --&gt;&lt;button type="button" class="btn btn-danger"&gt;危险按钮&lt;/button&gt;&lt;!-- 并不强调是一个按钮，看起来像一个链接，但同时保持按钮的行为 --&gt;&lt;button type="button" class="btn btn-link"&gt;链接按钮&lt;/button&gt; 按钮大小 1234567891011121314151617181920&lt;p&gt; &lt;button type="button" class="btn btn-primary btn-lg"&gt;大的原始按钮&lt;/button&gt; &lt;button type="button" class="btn btn-default btn-lg"&gt;大的按钮&lt;/button&gt;&lt;/p&gt;&lt;p&gt; &lt;button type="button" class="btn btn-primary"&gt;默认大小的原始按钮&lt;/button&gt; &lt;button type="button" class="btn btn-default"&gt;默认大小的按钮&lt;/button&gt;&lt;/p&gt;&lt;p&gt; &lt;button type="button" class="btn btn-primary btn-sm"&gt;小的原始按钮&lt;/button&gt; &lt;button type="button" class="btn btn-default btn-sm"&gt;小的按钮&lt;/button&gt;&lt;/p&gt;&lt;p&gt; &lt;button type="button" class="btn btn-primary btn-xs"&gt;特别小的原始按钮&lt;/button&gt; &lt;button type="button" class="btn btn-default btn-xs"&gt;特别小的按钮&lt;/button&gt;&lt;/p&gt;&lt;p&gt; &lt;button type="button" class="btn btn-primary btn-lg btn-block"&gt;块级的原始按钮&lt;/button&gt; &lt;button type="button" class="btn btn-default btn-lg btn-block"&gt;块级的按钮&lt;/button&gt;&lt;/p&gt; 按钮状态 12345678&lt;p&gt; &lt;button type="button" class="btn btn-default btn-lg "&gt;默认按钮&lt;/button&gt; &lt;button type="button" class="btn btn-default btn-lg active"&gt;激活按钮&lt;/button&gt;&lt;/p&gt;&lt;p&gt; &lt;button type="button" class="btn btn-primary btn-lg "&gt;原始按钮&lt;/button&gt; &lt;button type="button" class="btn btn-primary btn-lg active"&gt;激活的原始按钮&lt;/button&gt;&lt;/p&gt; 按钮禁用 12345678910111213141516&lt;p&gt; &lt;button type="button" class="btn btn-default btn-lg"&gt;默认按钮&lt;/button&gt; &lt;button type="button" class="btn btn-default btn-lg" disabled="disabled"&gt;禁用按钮&lt;/button&gt;&lt;/p&gt;&lt;p&gt; &lt;button type="button" class="btn btn-primary btn-lg "&gt;原始按钮&lt;/button&gt; &lt;button type="button" class="btn btn-primary btn-lg" disabled="disabled"&gt;禁用的原始按钮&lt;/button&gt;&lt;/p&gt;&lt;p&gt; &lt;a href="#" class="btn btn-default btn-lg" role="button"&gt;链接&lt;/a&gt; &lt;a href="#" class="btn btn-default btn-lg disabled" role="button"&gt;禁用链接&lt;/a&gt;&lt;/p&gt;&lt;p&gt; &lt;a href="#" class="btn btn-primary btn-lg" role="button"&gt;原始链接&lt;/a&gt; &lt;a href="#" class="btn btn-primary btn-lg disabled" role="button"&gt;禁用的原始链接&lt;/a&gt;&lt;/p&gt; 按钮标签，可以在 、 或 元素上使用按钮 class。但是建议您在 元素上使用按钮 class，避免跨浏览器的不一致性问题。 1234&lt;a class="btn btn-default" href="#" role="button"&gt;链接&lt;/a&gt;&lt;button class="btn btn-default" type="submit"&gt;按钮&lt;/button&gt;&lt;input class="btn btn-default" type="button" value="输入"&gt;&lt;input class="btn btn-default" type="submit" value="提交"&gt; 按钮组，使用 .btn-group-lg|sm|xs 来控制按钮组的大小，如果要设置垂直方向的按钮可以通过 .btn-group-vertical 类来设置。通过 .btn-group-justified 类来设置自适应大小的按钮组。 12345&lt;div class="btn-group btn-group-lg"&gt; &lt;button type="button" class="btn btn-primary"&gt;Apple&lt;/button&gt; &lt;button type="button" class="btn btn-primary"&gt;Samsung&lt;/button&gt; &lt;button type="button" class="btn btn-primary"&gt;Sony&lt;/button&gt;&lt;/div&gt; 内嵌下拉菜单按钮组 12345678910111213141516&lt;div class="container"&gt; &lt;h2&gt;内嵌按钮组&lt;/h2&gt; &lt;p&gt;内嵌按钮组创建下拉菜单：&lt;/p&gt; &lt;div class="btn-group"&gt; &lt;button type="button" class="btn btn-primary"&gt;Apple&lt;/button&gt; &lt;button type="button" class="btn btn-primary"&gt;Samsung&lt;/button&gt; &lt;div class="btn-group"&gt; &lt;button type="button" class="btn btn-primary dropdown-toggle" data-toggle="dropdown"&gt; Sony &lt;span class="caret"&gt;&lt;/span&gt;&lt;/button&gt; &lt;ul class="dropdown-menu" role="menu"&gt; &lt;li&gt;&lt;a href="#"&gt;Tablet&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;Smartphone&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt;&lt;/div&gt; 分隔按钮 12345678910111213&lt;div class="container"&gt; &lt;h2&gt;分隔按钮&lt;/h2&gt; &lt;div class="btn-group"&gt; &lt;button type="button" class="btn btn-primary"&gt;Sony&lt;/button&gt; &lt;button type="button" class="btn btn-primary dropdown-toggle" data-toggle="dropdown"&gt; &lt;span class="caret"&gt;&lt;/span&gt; &lt;/button&gt; &lt;ul class="dropdown-menu" role="menu"&gt; &lt;li&gt;&lt;a href="#"&gt;Tablet&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;Smartphone&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt; 2.8 图片Bootstrap 提供了三个可对图片应用简单样式的 class： .img-rounded：添加 border-radius:6px 来获得图片圆角。 .img-circle：添加 border-radius:50% 来让整个图片变成圆形。 .img-thumbnail：添加一些内边距（padding）和一个灰色的边框。 123&lt;img src="/wp-content/uploads/2014/06/download.png" class="img-rounded"&gt;&lt;img src="/wp-content/uploads/2014/06/download.png" class="img-circle"&gt;&lt;img src="/wp-content/uploads/2014/06/download.png" class="img-thumbnail"&gt; img类 响应式图片 通过在 标签添加 .img-responsive 类来让图片支持响应式设计。 图片将很好地扩展到父元素。.img-responsive 类将 max-width: 100%; 和 height: auto; 样式应用在图片上： 3 Bootstrap 布局组件3.1 基本布局 使用图标，在图标和文本之间保留适当的空间 123&lt;button type="button" class="btn btn-default btn-lg"&gt; &lt;span class="glyphicon glyphicon-user"&gt;&lt;/span&gt; User&lt;/button&gt; 带有字体图标的导航栏 12345678910111213141516171819202122&lt;body&gt;&lt;div class="navbar navbar-fixed-top navbar-inverse" role="navigation"&gt; &lt;div class="container"&gt; &lt;div class="navbar-header"&gt; &lt;button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse"&gt; &lt;span class="sr-only"&gt;Toggle navigation&lt;/span&gt; &lt;span class="icon-bar"&gt;&lt;/span&gt; &lt;span class="icon-bar"&gt;&lt;/span&gt; &lt;span class="icon-bar"&gt;&lt;/span&gt; &lt;/button&gt; &lt;a class="navbar-brand" href="#"&gt;Project name&lt;/a&gt; &lt;/div&gt; &lt;div class="collapse navbar-collapse"&gt; &lt;ul class="nav navbar-nav"&gt; &lt;li class="active"&gt;&lt;a href="#"&gt;&lt;span class="glyphicon glyphicon-home"&gt;&lt;/span&gt; Home&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#shop"&gt;&lt;span class="glyphicon glyphicon-shopping-cart"&gt;&lt;/span&gt; Shop&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#support"&gt;&lt;span class="glyphicon glyphicon-headphones"&gt;&lt;/span&gt; Support&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- /.nav-collapse --&gt; &lt;/div&gt;&lt;!-- /.container --&gt;&lt;/div&gt;&lt;/body&gt; 查看更多小图标：https://www.runoob.com/bootstrap/bootstrap-glyphicons.html 下拉菜单。只需要在 class .dropdown 内加上下拉菜单即可。 1234567891011121314151617181920&lt;div class="dropdown"&gt; &lt;button type="button" class="btn dropdown-toggle" id="dropdownMenu1" data-toggle="dropdown"&gt;主题 &lt;span class="caret"&gt;&lt;/span&gt; &lt;/button&gt; &lt;ul class="dropdown-menu" role="menu" aria-labelledby="dropdownMenu1"&gt; &lt;li role="presentation"&gt; &lt;a role="menuitem" tabindex="-1" href="#"&gt;Java&lt;/a&gt; &lt;/li&gt; &lt;li role="presentation"&gt; &lt;a role="menuitem" tabindex="-1" href="#"&gt;数据挖掘&lt;/a&gt; &lt;/li&gt; &lt;li role="presentation"&gt; &lt;a role="menuitem" tabindex="-1" href="#"&gt;数据通信/网络&lt;/a&gt; &lt;/li&gt; &lt;li role="presentation" class="divider"&gt;&lt;/li&gt; &lt;li role="presentation"&gt; &lt;a role="menuitem" tabindex="-1" href="#"&gt;分离的链接&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt;&lt;/div&gt; 按钮组 按钮大小 导航元素 导航栏 创建一个默认的导航栏的步骤如下： 向 标签添加 class .navbar、.navbar-default。 向上面的元素添加 role=”navigation”，有助于增加可访问性。 向 元素添加一个标题 class .navbar-header，内部包含了带有 class navbar-brand 的 元素。这会让文本看起来更大一号。 为了向导航栏添加链接，只需要简单地添加带有 class .nav、.navbar-nav 的无序列表即可。 12345678910111213141516171819202122232425262728&lt;nav class="navbar navbar-default" role="navigation"&gt; &lt;div class="container-fluid"&gt; &lt;div class="navbar-header"&gt; &lt;a class="navbar-brand" href="#"&gt;菜鸟教程&lt;/a&gt; &lt;/div&gt; &lt;div&gt; &lt;ul class="nav navbar-nav"&gt; &lt;li class="active"&gt;&lt;a href="#"&gt;iOS&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;SVN&lt;/a&gt;&lt;/li&gt; &lt;li class="dropdown"&gt; &lt;a href="#" class="dropdown-toggle" data-toggle="dropdown"&gt; Java &lt;b class="caret"&gt;&lt;/b&gt; &lt;/a&gt; &lt;ul class="dropdown-menu"&gt; &lt;li&gt;&lt;a href="#"&gt;jmeter&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;EJB&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;Jasper Report&lt;/a&gt;&lt;/li&gt; &lt;li class="divider"&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;分离的链接&lt;/a&gt;&lt;/li&gt; &lt;li class="divider"&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;另一个分离的链接&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt;&lt;/nav&gt; 分页 123456789&lt;ul class="pagination"&gt; &lt;li&gt;&lt;a href="#"&gt;&amp;laquo;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;1&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;3&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;4&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;5&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;&amp;raquo;&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt; 123456789&lt;ul class="pagination"&gt; &lt;li&gt;&lt;a href="#"&gt;&amp;laquo;&lt;/a&gt;&lt;/li&gt; &lt;li class="active"&gt;&lt;a href="#"&gt;1&lt;/a&gt;&lt;/li&gt; &lt;li class="disabled"&gt;&lt;a href="#"&gt;2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;3&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;4&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;5&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;&amp;raquo;&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt; 标签 徽章 缩略图 警告 进度条 3.2 创建一个网站的基本架构123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960&lt;div class="jumbotron text-center" style="margin-bottom:0"&gt; &lt;h1&gt;我的第一个 Bootstrap 页面&lt;/h1&gt; &lt;p&gt;重置浏览器窗口大小查看效果！&lt;/p&gt; &lt;/div&gt; &lt;nav class="navbar navbar-inverse"&gt; &lt;div class="container-fluid"&gt; &lt;div class="navbar-header"&gt; &lt;button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#myNavbar"&gt; &lt;span class="icon-bar"&gt;&lt;/span&gt; &lt;span class="icon-bar"&gt;&lt;/span&gt; &lt;span class="icon-bar"&gt;&lt;/span&gt; &lt;/button&gt; &lt;a class="navbar-brand" href="#"&gt;网站名&lt;/a&gt; &lt;/div&gt; &lt;div class="collapse navbar-collapse" id="myNavbar"&gt; &lt;ul class="nav navbar-nav"&gt; &lt;li class="active"&gt;&lt;a href="#"&gt;主页&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;页面 2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;页面 3&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt;&lt;/nav&gt; &lt;div class="container"&gt; &lt;div class="row"&gt; &lt;div class="col-sm-4"&gt; &lt;h2&gt;关于我&lt;/h2&gt; &lt;h5&gt;我的照片:&lt;/h5&gt; &lt;div class="fakeimg"&gt;这边插入图像&lt;/div&gt; &lt;p&gt;关于我的介绍..&lt;/p&gt; &lt;h3&gt;链接&lt;/h3&gt; &lt;p&gt;描述文本。&lt;/p&gt; &lt;ul class="nav nav-pills nav-stacked"&gt; &lt;li class="active"&gt;&lt;a href="#"&gt;链接 1&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;链接 2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;链接 3&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;hr class="hidden-sm hidden-md hidden-lg"&gt; &lt;/div&gt; &lt;div class="col-sm-8"&gt; &lt;h2&gt;标题&lt;/h2&gt; &lt;h5&gt;副标题&lt;/h5&gt; &lt;div class="fakeimg"&gt;图像&lt;/div&gt; &lt;p&gt;一些文本..&lt;/p&gt; &lt;p&gt;菜鸟教程，学的不仅是技术，更是梦想！！！菜鸟教程，学的不仅是技术，更是梦想！！！菜鸟教程，学的不仅是技术，更是梦想！！！&lt;/p&gt; &lt;br&gt; &lt;h2&gt;标题&lt;/h2&gt; &lt;h5&gt;副标题&lt;/h5&gt; &lt;div class="fakeimg"&gt;图像&lt;/div&gt; &lt;p&gt;一些文本..&lt;/p&gt; &lt;p&gt;菜鸟教程，学的不仅是技术，更是梦想！！！菜鸟教程，学的不仅是技术，更是梦想！！！菜鸟教程，学的不仅是技术，更是梦想！！！&lt;/p&gt; &lt;/div&gt; &lt;/div&gt;&lt;/div&gt; &lt;div class="jumbotron text-center" style="margin-bottom:0"&gt; &lt;p&gt;底部内容&lt;/p&gt;&lt;/div&gt; 4 Bootstrap 面试总结https://www.cnblogs.com/zhangjinghe/p/7918906.html 技术交流共享QQ群【机器学习和自然语言QQ群：436303759】： 机器学习和自然语言（QQ群号：436303759）是一个研究深度学习、机器学习、自然语言处理、数据挖掘、图像处理、目标检测、数据科学等AI相关领域的技术群。其宗旨是纯粹的AI技术圈子、绿色的交流环境。本群禁止有违背法律法规和道德的言谈举止。群成员备注格式：城市-自命名。微信订阅号：datathinks]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>bootstrap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[笔记1：jQuery复习篇]]></title>
    <url>%2F2019%2F06%2F13%2F%E7%AC%94%E8%AE%B01%EF%BC%9AjQuery%E5%A4%8D%E4%B9%A0%E7%AF%87%2F</url>
    <content type="text"><![CDATA[摘要：日常学习中对一些知识点进行总结得出该系列文章。学习笔记内容包括前端技术，Django web开发技术，数据库技术如MySQL，MongoDB，PGSQL等等。此外还有一些工具如Dock，ES等等。（本文原创，转载必须注明出处.） 1 基本知识jQuery 是一个 JavaScript 库。jQuery 极大地简化了 JavaScript 编程。其下载地址：http://jquery.com/download/ 1234567891011121314151617181920&lt;html&gt;&lt;head&gt;&lt;meta charset="utf-8"&gt; &lt;title&gt;菜鸟教程(runoob.com)&lt;/title&gt; &lt;script src="https://cdn.staticfile.org/jquery/1.10.2/jquery.min.js"&gt;&lt;/script&gt;&lt;script&gt;$(document).ready(function()&#123; $("p").click(function()&#123; $(this).hide(); &#125;);&#125;);&lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;p&gt;如果你点我，我就会消失。&lt;/p&gt;&lt;p&gt;继续点我!&lt;/p&gt;&lt;p&gt;接着点我!&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; jquery语法，jQuery 使用的语法是 XPath 与 CSS 选择器语法的组合。 文档就绪事件 123456789$(document).ready(function()&#123; // 开始写 jQuery 代码... &#125;);等价于$(function()&#123; // 开始写 jQuery 代码...&#125;); jquery选择器 jQuery 选择器基于元素的 id、类、类型、属性、属性值等”查找”（或选择）HTML 元素。 它基于已经存在的 CSS 选择器，除此之外，它还有一些自定义的选择器。 12345$(function()&#123; $("button").click(function()&#123; $("p").hide(); &#125;);&#125;); id选择器 页面中元素的 id 应该是唯一的，所以您要在页面中选取唯一的元素需要通过 #id 选择器。 12345$(document).ready(function()&#123; $("button").click(function()&#123; $("#test").hide(); &#125;);&#125;); class选择器 Query 类选择器可以通过指定的 class 查找元素。 12345$(function()&#123; $("button").click(function()&#123; $("p").hide(); &#125;);&#125;); 如果您的网站包含许多页面，并且您希望您的 jQuery 函数易于维护，那么请把您的 jQuery 函数放到独立的 .js 文件中。当我们在教程中演示 jQuery 时，会将函数直接添加到 部分中。不过，把它们放到一个单独的文件中会更好，就像这样（通过 src 属性来引用文件）： jquery事件 页面对不同访问者的响应叫做事件。事件处理程序指的是当 HTML 中发生某些事件时所调用的方法。 在元素上移动鼠标。 选取单选按钮 点击元素 2 效果 12345678910111213141516171819202122232425262728293031323334&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset="utf-8"&gt;&lt;script src="https://cdn.staticfile.org/jquery/1.10.2/jquery.min.js"&gt;&lt;/script&gt;&lt;script&gt; $(document).ready(function()&#123; $("#flip").click(function()&#123; $("#panel").slideToggle("slow"); &#125;);&#125;);&lt;/script&gt; &lt;style type="text/css"&gt; #panel,#flip&#123; padding:5px; text-align:center; background-color:#e5eecc; border:solid 1px #c3c3c3;&#125;#panel&#123; padding:50px; display:none;&#125;&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div id="flip"&gt;点我，显示或隐藏面板。&lt;/div&gt;&lt;div id="panel"&gt;Hello world!&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 3 获取内容与属性jQuery 中非常重要的部分，就是操作 DOM 的能力。jQuery 提供一系列与 DOM 相关的方法，这使访问和操作元素和属性变得很容易。DOM = Document Object Model（文档对象模型） 获取值 123$("#btn1").click(function()&#123; alert("值为: " + $("#test").val());&#125;); 改变文本中的值 1234567891011121314151617181920212223&lt;script&gt;$(document).ready(function()&#123; $("#btn1").click(function()&#123; $("#test1").text("Hello world!"); &#125;); $("#btn2").click(function()&#123; $("#test2").html("&lt;b&gt;Hello world!&lt;/b&gt;"); &#125;); $("#btn3").click(function()&#123; $("#test3").val("RUNOOB"); &#125;);&#125;);&lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;p id="test1"&gt;这是一个段落。&lt;/p&gt;&lt;p id="test2"&gt;这是另外一个段落。&lt;/p&gt;&lt;p&gt;输入框: &lt;input type="text" id="test3" value="菜鸟教程"&gt;&lt;/p&gt;&lt;button id="btn1"&gt;设置文本&lt;/button&gt;&lt;button id="btn2"&gt;设置 HTML&lt;/button&gt;&lt;button id="btn3"&gt;设置值&lt;/button&gt;&lt;/body&gt; 改变标签属性 1234567891011121314&lt;script&gt;$(document).ready(function()&#123; $("button").click(function()&#123; $("#runoob").attr("href","http://www.runoob.com/jquery"); &#125;);&#125;);&lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;p&gt;&lt;a href="//www.runoob.com" id="runoob"&gt;菜鸟教程&lt;/a&gt;&lt;/p&gt;&lt;button&gt;修改 href 值&lt;/button&gt;&lt;p&gt;点击按钮修改后，可以点击链接查看链接地址是否变化。&lt;/p&gt;&lt;/body&gt; 添加元素 操作CSS 向元素添加css样式 12345678910111213141516171819202122232425262728293031&lt;script&gt;$(document).ready(function()&#123; $("button").click(function()&#123; $("h1,h2,p").addClass("blue"); $("div").addClass("important"); &#125;);&#125;);&lt;/script&gt;&lt;style type="text/css"&gt;.important&#123; font-weight:bold; font-size:xx-large;&#125;.blue&#123; color:blue;&#125;&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;标题 1&lt;/h1&gt;&lt;h2&gt;标题 2&lt;/h2&gt;&lt;p&gt;这是一个段落。&lt;/p&gt;&lt;p&gt;这是另外一个段落。&lt;/p&gt;&lt;div&gt;这是一些重要的文本!&lt;/div&gt;&lt;br&gt;&lt;button&gt;为元素添加 class&lt;/button&gt;&lt;/body&gt; 返回css元素属性 1234567891011121314151617181920212223&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset="utf-8"&gt;&lt;script src="https://cdn.staticfile.org/jquery/1.10.2/jquery.min.js"&gt;&lt;/script&gt;&lt;script&gt;$(document).ready(function()&#123; $("button").click(function()&#123; alert("背景颜色 = " + $("p").css("background-color")); &#125;);&#125;);&lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;h2&gt;这是一个标题&lt;/h2&gt;&lt;p style="background-color:#ff0000"&gt;这是一个段落。&lt;/p&gt;&lt;p style="background-color:#00ff00"&gt;这是一个段落。&lt;/p&gt;&lt;p style="background-color:#0000ff"&gt;这是一个段落。&lt;/p&gt;&lt;button&gt;返回第一个 p 元素的 background-color &lt;/button&gt;&lt;/body&gt;&lt;/html&gt; jQuery遍历 ajax。AJAX 是与服务器交换数据的技术，它在不重载全部页面的情况下，实现了对部分网页的更新。 AJAX = 异步 JavaScript 和 XML（Asynchronous JavaScript and XML）。简短地说，在不重载整个网页的情况下，AJAX 通过后台加载数据，并在网页上进行显示。jQuery load() 方法是简单但强大的 AJAX 方法。load() 方法从服务器加载数据，并把返回的数据放入被选元素中。语法： 1$(selector).load(URL,data,callback); 回调函数 可选的 callback 参数规定当 load() 方法完成后所要允许的回调函数。回调函数可以设置不同的参数： responseTxt - 包含调用成功时的结果内容 statusTXT - 包含调用的状态 xhr - 包含 XMLHttpRequest 对象 下面的例子会在 load() 方法完成后显示一个提示框。如果 load() 方法已成功，则显示”外部内容加载成功！”，而如果失败，则显示错误消息： 1234567891011121314151617181920212223242526&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset="utf-8"&gt;&lt;script src="https://cdn.staticfile.org/jquery/1.10.2/jquery.min.js"&gt;&lt;/script&gt;&lt;script&gt;$(document).ready(function()&#123; $("button").click(function()&#123; $("#div1").load("/try/ajax/demo_test.txt",function(responseTxt,statusTxt,xhr)&#123; if(statusTxt=="success") alert("外部内容加载成功!"); if(statusTxt=="error") alert("Error: "+xhr.status+": "+xhr.statusText); &#125;); &#125;);&#125;);&lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;div id="div1"&gt;&lt;h2&gt;使用 jQuery AJAX 修改该文本&lt;/h2&gt;&lt;/div&gt;&lt;button&gt;获取外部内容&lt;/button&gt;&lt;/body&gt;&lt;/html&gt; get和post jQuery get() 和 post() 方法用于通过 HTTP GET 或 POST 请求从服务器请求数据。HTTP 请求：GET vs. POST，两种在客户端和服务器端进行请求-响应的常用方法是：GET 和 POST。 GET - 从指定的资源请求数据 POST - 向指定的资源提交要处理的数据 GET 基本上用于从服务器获得（取回）数据。注释：GET 方法可能返回缓存数据。POST 也可用于从服务器获取数据。不过，POST 方法不会缓存数据，并且常用于连同请求一起发送数据。 123456$.get(URL,callback);demo_test.php 文件代码:&lt;?phpecho '这是个从PHP文件中读取的数据。';?&gt; 1234567891011121314151617181920212223&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset="utf-8"&gt;&lt;title&gt;菜鸟教程(runoob.com)&lt;/title&gt;&lt;script src="https://cdn.staticfile.org/jquery/1.10.2/jquery.min.js"&gt;&lt;/script&gt;&lt;script&gt;$(document).ready(function()&#123; $("button").click(function()&#123; $.get("/try/ajax/demo_test.php",function(data,status)&#123; alert("数据: " + data + "\n状态: " + status); &#125;); &#125;);&#125;);&lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;button&gt;发送一个 HTTP GET 请求并获取返回结果&lt;/button&gt;&lt;/body&gt;&lt;/html&gt; $.post() 方法通过 HTTP POST 请求向服务器提交数据。必需的 URL 参数规定您希望请求的 URL。可选的 data 参数规定连同请求发送的数据。可选的 callback 参数是请求成功后所执行的函数名。 12345678910$.post(URL,data,callback);demo_test_post.php 文件代码:&lt;?php$name = isset($_POST['name']) ? htmlspecialchars($_POST['name']) : '';$url = isset($_POST['url']) ? htmlspecialchars($_POST['url']) : '';echo '网站名: ' . $name;echo "\n";echo 'URL 地址: ' .$url;?&gt; 12345678910$("button").click(function()&#123; $.post("/try/ajax/demo_test_post.php", &#123; name:"菜鸟教程", url:"http://www.runoob.com" &#125;, function(data,status)&#123; alert("数据: \n" + data + "\n状态: " + status); &#125;);&#125;); JSONP Jsonp(JSON with Padding) 是 json 的一种”使用模式”，可以让网页从别的域名（网站）那获取资料，即跨域读取数据。为什么我们从不同的域（网站）访问数据需要一个特殊的技术(JSONP )呢？这是因为同源策略。同源策略，它是由Netscape提出的一个著名的安全策略，现在所有支持JavaScript 的浏览器都会使用这个策略。 服务器JSON数据，服务端文件jsonp.php代码为： 123456789&lt;?phpheader('Content-type: application/json');//获取回调函数名$jsoncallback = htmlspecialchars($_REQUEST ['jsoncallback']);//json数据$json_data = '["customername1","customername2"]';//输出jsonp格式的数据echo $jsoncallback . "(" . $json_data . ")";?&gt; 客户端实现 callbackFunction 函数 123456789101112&lt;script type="text/javascript"&gt;function callbackFunction(result, methodName)&#123; var html = '&lt;ul&gt;'; for(var i = 0; i &lt; result.length; i++) &#123; html += '&lt;li&gt;' + result[i] + '&lt;/li&gt;'; &#125; html += '&lt;/ul&gt;'; document.getElementById('divCustomers').innerHTML = html;&#125;&lt;/script&gt; 技术交流共享QQ群【机器学习和自然语言QQ群：436303759】： 机器学习和自然语言（QQ群号：436303759）是一个研究深度学习、机器学习、自然语言处理、数据挖掘、图像处理、目标检测、数据科学等AI相关领域的技术群。其宗旨是纯粹的AI技术圈子、绿色的交流环境。本群禁止有违背法律法规和道德的言谈举止。群成员备注格式：城市-自命名。微信订阅号：datathinks]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>jquery</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python和Elasticsearch 构建简易搜索]]></title>
    <url>%2F2019%2F05%2F24%2FPython-%E5%92%8C-Elasticsearch-%E6%9E%84%E5%BB%BA%E7%AE%80%E6%98%93%E6%90%9C%E7%B4%A2%2F</url>
    <content type="text"><![CDATA[1 ES基本介绍概念介绍 Elasticsearch是一个基于Lucene库的搜索引擎。它提供了一个分布式、支持多租户的全文搜索引擎，它可以快速地储存、搜索和分析海量数据。Elasticsearch可以用于搜索各种文档。它提供可扩展的搜索，具有接近实时的搜索，并支持多租户。Elasticsearch至少需要Java 8。Elasticsearch是分布式的，这意味着索引可以被分成分片，每个分片可以有0个或多个副本。每个节点托管一个或多个分片，并充当协调器将操作委托给正确的分片。相关数据通常存储在同一个索引中，该索引由一个或多个主分片和零个或多个复制分片组成。一旦创建了索引，就不能更改主分片的数量。 集群(Cluster)：集群是一个或多个节点（服务器）的集合，它们共同保存您的整个数据，并提供跨所有节点的联合索引和搜索功能。本质上是一个分布式数据库，允许多台服务器协同工作，每台服务器可以运行多个 Elastic 实例。单个 Elastic 实例称为一个节点（node）。一组节点构成一个集群（cluster）。 节点（Node）：节点是作为集群一部分的单个服务器，存储数据并参与群集的索引和搜索功能。 索引（Index）：索引是具有某些类似特征的文档集合。索引由名称标识（必须全部小写），此名称用于在对其中的文档执行索引，搜索，更新和删除操作时引用索引。 数据管理的顶层单位就叫做 Index（索引）。它是单个数据库的同义词。每个 Index （即数据库）的名字必须是小写。 文档（Document）：文档是可以编制索引的基本信息单元。Index 里面单条的记录称为 Document（文档）。许多条 Document 构成了一个 Index。Document 使用 JSON 格式表示，同一个 Index 里面的 Document，不要求有相同的结构（scheme），但是最好保持相同，这样有利于提高搜索效率。 分片和副本（Shards &amp; Replicas）：索引可能存储大量可能超过单个节点的硬件限制的数据。为了解决这个问题，Elasticsearch提供了将索引细分为多个称为分片的功能。创建索引时，只需定义所需的分片数即可。每个分片本身都是一个功能齐全且独立的“索引”，可以托管在集群中的任何节点上。 副本集很重要：它在分片/节点发生故障时提供高可用性。它允许您扩展搜索量/吞吐量，因为可以在所有副本上并行执行搜索。默认情况下，Elasticsearch中的每个索引都分配了5个主分片和1个副本，这意味着如果群集中至少有两个节点，则索引将包含5个主分片和另外5个副本分片（1个完整副本），总计为每个索引10个分片。 应用场景 在线网上商店，允许客户搜索您销售的产品。在这种情况下，可以使用Elasticsearch存储整个产品目录和库存，并为它们提供搜索和自动填充建议。 收集日志或交易数据，并分析和挖掘此数据以查找趋势，统计信息，摘要或异常。在这种情况下，您可以使用Logstash（Elasticsearch / Logstash / Kibana堆栈的一部分）来收集，聚合和解析数据，然后让Logstash将此数据提供给Elasticsearch。一旦数据在Elasticsearch中，您就可以运行搜索和聚合来挖掘您感兴趣的任何信息。 价格警报平台，允许精通价格的客户指定一条规则，例如“我有兴趣购买特定的电子产品，如果小工具的价格在下个月内从任何供应商降至X美元以下，我希望收到通知” 。在这种情况下，您可以刮取供应商价格，将其推入Elasticsearch并使用其反向搜索功能来匹配价格变动与客户查询，并最终在发现匹配后将警报推送给客户。 核心模块 analysis：主要负责词法分析及语言处理，也就是我们常说的分词，通过该模块可最终形成存储或者搜索的最小单元 Term。 index 模块：主要负责索引的创建工作。 store 模块：主要负责索引的读写，主要是对文件的一些操作，其主要目的是抽象出和平台文件系统无关的存储。 queryParser 模块：主要负责语法分析，把我们的查询语句生成 Lucene 底层可以识别的条件。 search 模块：主要负责对索引的搜索工作。 similarity 模块：主要负责相关性打分和排序的实现。 检索方式 （1）单个词查询：指对一个 Term 进行查询。比如，若要查找包含字符串“Lucene”的文档，则只需在词典中找到 Term“Lucene”，再获得在倒排表中对应的文档链表即可。 （2）AND：指对多个集合求交集。比如，若要查找既包含字符串“Lucene”又包含字符串“Solr”的文档，则查找步骤如下：在词典中找到 Term “Lucene”，得到“Lucene”对应的文档链表。在词典中找到 Term “Solr”，得到“Solr”对应的文档链表。合并链表，对两个文档链表做交集运算，合并后的结果既包含“Lucene”也包含“Solr”。 （3） OR：指多个集合求并集。比如，若要查找包含字符串“Luence”或者包含字符串“Solr”的文档，则查找步骤如下：在词典中找到 Term “Lucene”，得到“Lucene”对应的文档链表。在词典中找到 Term “Solr”，得到“Solr”对应的文档链表。合并链表，对两个文档链表做并集运算，合并后的结果包含“Lucene”或者包含“Solr”。 （4）NOT：指对多个集合求差集。比如，若要查找包含字符串“Solr”但不包含字符串“Lucene”的文档，则查找步骤如下：在词典中找到 Term “Lucene”，得到“Lucene”对应的文档链表。在词典中找到 Term “Solr”，得到“Solr”对应的文档链表。合并链表，对两个文档链表做差集运算，用包含“Solr”的文档集减去包含“Lucene”的文档集，运算后的结果就是包含“Solr”但不包含“Lucene”。 通过上述四种查询方式，我们不难发现，由于 Lucene 是以倒排表的形式存储的。所以在 Lucene 的查找过程中只需在词典中找到这些 Term，根据 Term 获得文档链表，然后根据具体的查询条件对链表进行交、并、差等操作，就可以准确地查到我们想要的结果。相对于在关系型数据库中的“Like”查找要做全表扫描来说，这种思路是非常高效的。虽然在索引创建时要做很多工作，但这种一次生成、多次使用的思路也是很高明的。 ES特性 Elasticsearch可扩展高达PB级的结构化和非结构化数据。 Elasticsearch可以用来替代MongoDB和RavenDB等做文档存储。 Elasticsearch使用非标准化来提高搜索性能。 Elasticsearch是受欢迎的企业搜索引擎之一，目前被许多大型组织使用，如Wikipedia，The Guardian，StackOverflow，GitHub等。 Elasticsearch是开放源代码，可在Apache许可证版本2.0下提供。 ES优点 Elasticsearch是基于Java开发的，这使得它在几乎每个平台上都兼容。 Elasticsearch是实时的，换句话说，一秒钟后，添加的文档可以在这个引擎中搜索得到。 Elasticsearch是分布式的，这使得它易于在任何大型组织中扩展和集成。 通过使用Elasticsearch中的网关概念，创建完整备份很容易。 与Apache Solr相比，在Elasticsearch中处理多租户非常容易。 Elasticsearch使用JSON对象作为响应，这使得可以使用不同的编程语言调用Elasticsearch服务器。 Elasticsearch支持几乎大部分文档类型，但不支持文本呈现的文档类型。 ES缺点 Elasticsearch在处理请求和响应数据方面没有多语言和数据格式支持(仅在JSON中可用)，与Apache Solr不同，Elasticsearch不可以使用CSV，XML等格式。 Elasticsearch也有一些伤脑的问题发生，虽然在极少数情况下才会发生。 2 ES的安装部署本文主要采用Win10下的Elasticsearch安装，当然Linux安装操作起来更加简便了。完成之后对python安装elasticsearch包，并实现交互案例。 第一步：条件检查 Elasticsearch至少需要Java 8，首先需要java -version查看当前版本。 第二步：安装ES，这里采用elasticsearch-7.1.0-windows-x86_64下载地址链接: https://pan.baidu.com/s/1k5AOGpMy8uJEXtA6KoNb7g 提取码: qtmj 。 1234567bin ：运行Elasticsearch实例和插件管理所需的脚本confg: 配置文件所在的目录lib : Elasticsearch使用的库data : Elasticsearch使用的所有数据的存储位置logs : 关于事件和错误记录的文件plugins: 存储所安装插件的地方，比如中文分词工具work : Elasticsearch使用的临时文件，这个文件我这暂时好像没有，可以根据配置文件来 配置这些个文件的目录位置，比如上面的data，logs， 然后去运行 bin/elasticsearch（Mac 或 Linux）或者 bin\elasticsearch.bat (Windows) 即可启动 Elasticsearch 了。我们启动后发现网页并不现实信息，测试下本地网络是否联通： 发现是一般性故障，查询资料显示由于防火墙的问题，经过测试关闭”公用网络防火墙“即可： 之后我们再去ping下本地IP: 这时已经显示ping通状态，再次启动bin\elasticsearch.bat (Windows)，打开http://localhost:9200/显示如下表示成功安装ES。 第三步：Python安装ES， 下载地址是https://www.elastic.co/downloads/elasticsearch。如果在windows下安排部署参考文章http://www.cnblogs.com/viaiu/p/5715200.html。如果是Python开发可以使用pip install elasticsearch安装。 3 Python和ES构建搜索引擎插入数据：打开python运行环境，首先导入【from elasticsearch import Elasticsearch】，然后编写插入数据的方法： 12345678# 插入数据def InsertDatas(): # 默认host为localhost,port为9200.但也可以指定host与port es = Elasticsearch() es.create(index="my_index",doc_type="test_type",id=11,ignore=[400,409],body=&#123;"name":"python","addr":'四川省'&#125;) # 查询结果 result = es.get(index="my_index",doc_type="test_type",id=11) print('单条数据插入完成：\n',result) 实例化Elasticsearch，其中默认为空即host为localhost,port为9200。为空也可以指定网络IP与端口。通过创建索引index和文档类别doc_type，文档id，body为插入数据的内容,其中ES支持的数据仅为JSON类型，ignore=409忽略异常。运行结果如下： 批量插入数据：上面案例我们插入一条信息，查询显示一系列参数包括索引、文档类型、文档ID唯一标识，版本号等。其中资源中包含数据信息，如果我们想插入多条信息可以参考以下代码： 12345678910111213141516# 批量插入数据def AddDatas(): es = Elasticsearch() datas = [&#123; 'name': '美国留给伊拉克的是个烂摊子', 'addr': 'http://view.news.qq.com/zt2011/usa_iraq/index.htm' &#125;,&#123; "name":"python", "addr":'四川省' &#125;] for i,data in enumerate(datas): es.create(index="my_index",doc_type="test_type", id=i,ignore=[400,409],body=data) # 查询结果 result = es.get(index="my_index",doc_type="test_type",id=0) print('\n批量插入数据完成：\n',result['_source']) 我们将数据放在datas列表中，如果我们数据在一个json文件中存储，也可以通过读取文本信息并保存在datas中，之后对其进行插入即可。这里面文件ID我采用枚举的序号，也可以采用随机数或者指定格式。完成所有插入之后我们选择第一条id=0的信息查询，此处查询与上文不同，我们只看文章内容可以采用result[‘_source’]方法，结果如下： 更新数据：如果我们插入数据信息有问题，我们想去修正。可以采用update方法，这里面与我们接触的MySQL，MongoDB等SQL语句差不多。唯一注意的是我们更新数据时候采用{“doc”:{“name”:”python1”,”addr”:”深圳1”}}字典模式，尤其是doc标识不能忘记，代码实现如下： 12345678# 3 更新数据def UpdateDatas(): es = Elasticsearch() es.update(index="my_index",doc_type="test_type", id=11,ignore=[400,409],body=&#123;"doc":&#123;"name":"python1","addr":"深圳1"&#125;&#125;) # 更新结果 result = es.get(index="my_index",doc_type="test_type",id=11) print('\n数据id=11更新完成：\t',result['_source']['name']) 这里我们假如只想查询更新后信息的name字段，可以采用source后面加[‘name’]方法，为什么这么设置呢？请参看插入数据运行结果分析。 删除数据：这里面比较简单，我们指定文档的索引、文档类型和文档ID即可。 12345# 删除数据def DeleteDatas(): es = Elasticsearch() result = es.delete(index='my_index',doc_type='test_type',id=11) print('\n数据id=11删除完成：\t') 条件查询数据：我们通过插入数据构建一个简单我数据信息，如果我们想获取索引中的所有文档可以采用{“query”:{“match_all”:{}}}条件查询，这里面指定关注的是使用的search方法，上文查询数据采用get方法，其实两者都是可以作为查询使用的。代码如下： 1234567# 条件查询def ParaSearch(): es = Elasticsearch() query1 = es.search(index="my_index", body=&#123;"query":&#123;"match_all":&#123;&#125;&#125;&#125;) print('\n查询所有文档\n',query1) query2 = es.search(index="my_index", body=&#123;"query":&#123;"term":&#123;'name':'python'&#125;&#125;&#125;) print('\n查找名字Python的文档:\n',query2['hits']['hits'][0]) 我们获取索引所有文档的信息 获取文档中name为Python的信息 4 技术交流共享QQ群【机器学习和自然语言QQ群：436303759】： 机器学习和自然语言（QQ群号：436303759）是一个研究深度学习、机器学习、自然语言处理、数据挖掘、图像处理、目标检测、数据科学等AI相关领域的技术群。其宗旨是纯粹的AI技术圈子、绿色的交流环境。本群禁止有违背法律法规和道德的言谈举止。群成员备注格式：城市-自命名。微信订阅号：datathinks]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Elasticsearech</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker 部署Python项目]]></title>
    <url>%2F2019%2F05%2F22%2FDocker-%E9%83%A8%E7%BD%B2Python%E9%A1%B9%E7%9B%AE%2F</url>
    <content type="text"><![CDATA[导读： 软件开发最大的麻烦事之一就是环境配置，操作系统设置，各种库和组件的安装。只有它们都正确，软件才能运行。如果从一种操作系统里面运行另一种操作系统，通常我们采取的策略就是引入虚拟机，比如在 Windows 系统里面运行 Linux 系统。这种方式有个很大的缺点就是资源占用多、冗余步骤多、启动慢。目前最流行的 Linux 容器解决方案之一就是Docker，它最大优点就是轻量、资源占用少、启动快。本文从什么是Docker？Docker解决什么问题？有哪些好处？如何去部署实现去全面介绍。 0 引言设想这样一个真实案例，假如我们要部署一个Python应用程序，要做哪些工作？ 首先需要python运行环境，比如部署的是python3，而机器上是python2。先装个python3，还要装各种依赖包，机器一些可能的冲突。 装完python之后，发现还要装mysql或者redis。继续下载安装配置。 啥？服务器不用了，需要换一台服务器？那重新来一遍吧。 啥？基础应用做的太好要进行推广，需要指导其他厂商部署？这怎么办？ 可以看出，在 Docker 之前软件行业的运维存在着以下这些痛点: 软件的发布和部署低效又繁琐，而且总是需要人工介入 环境的一致性难移保证 在不同环境之间迁移的成本较高 在完成Docker部署安装之前，我们还是先认识下Docker的优点: 软件构建容易，分发简单 应用得到隔离，依赖被解除 可以完美地用于 CI/CD 快速部署，测试完以后销毁也方便 1 什么是DockerDocker 是一个开源项目，诞生于 2013 年初，最初是 dotCloud 公司内部的一个业余项目。它基于 Google 公司推出的 Go 语言实现。 项目后来加入了 Linux 基金会，遵从了 Apache 2.0 协议，项目代码在 GitHub 上进行维护。2013年3月，dotCloud公司的创始人之一，Docker之父，28岁的Solomon Hykes正式决定，将Docker项目开源，Docker 自开源后受到广泛的关注和讨论。Redhat 已经在其 RHEL6.5 中集中支持 Docker；Google 也在其 PaaS 产品中广泛应用。 Docker 属于 Linux 容器的一种封装，提供简单易用的容器使用接口。它是目前最流行的 Linux 容器解决方案。有了 Docker，就不用担心环境问题。总体来说，Docker 的接口相当简单，用户可以方便地创建和使用容器，把自己的应用放入容器。容器还可以进行版本管理、复制、分享、修改，就像管理普通的代码一样。 通俗解释Docker Docker的思想来自于集装箱，集装箱解决了什么问题？在一艘大船上，把货物规整的摆放起来。并且各种各样的货物被集装箱标准化了，集装箱和集装箱之间不会互相影响。docker就是类似的理念。现在都流行云计算了，云计算就好比大货轮。docker就是集装箱。 不同的应用程序可能会有不同的应用环境，比如.net开发的网站和php开发的网站依赖的软件就不一样，如果把他们依赖的软件都安装在一个服务器上就要调试很久，而且很麻烦，还会造成一些冲突。这个时候你就要隔离.net开发的网站和php开发的网站。常规来讲，我们可以在服务器上创建不同的虚拟机在不同的虚拟机上放置不同的应用，但是虚拟机开销比较高。docker可以实现虚拟机隔离应用环境的功能，并且开销比虚拟机小，小就意味着省钱了。 开发软件的时候用的是Ubuntu，但是运维管理的都是centos，运维在把你的软件从开发环境转移到生产环境的时候就会遇到一些Ubuntu转centos的问题，比如：有个特殊版本的数据库，只有Ubuntu支持，centos不支持，在转移的过程当中运维就得想办法解决这样的问题。这时候要是有docker你就可以把开发环境直接封装转移给运维，运维直接部署你给他的docker就可以了。而且部署速度快。 在服务器负载方面，如果你单独开一个虚拟机，那么虚拟机会占用空闲内存的，docker部署的话，这些内存就会利用起来。总之docker就是集装箱原理。 2 Docker用途 Docker 的主要用途，目前有三大类。 （1）提供一次性的环境。本地测试的软件、持续集成的时候提供单元测试和构建的环境。 （2）提供弹性的云服务。因为 Docker 容器可以随开随关，很适合动态扩容和缩容。 （3）组建微服务架构。一台机器可以跑多个服务，在本机可以模拟出微服务架构。 应用场景 Web 应用的自动化打包和发布。 自动化测试和持续集成、发布。 在服务型环境中部署和调整数据库或其他的后台应用。 从头编译或者扩展现有的OpenShift或Cloud Foundry平台来搭建自己的PaaS环境。 Docker 能干什么？ 简化配置：这是Docker公司宣传的Docker的主要使用场景。虚拟机的最大好处是能在你的硬件设施上运行各种配置不一样的平台（软件、系统），Docker在降低额外开销的情况下提供了同样的功能。它能让你将运行环境和配置放在代码中然后部署，同一个Docker的配置可以在不同的环境中使用，这样就降低了硬件要求和应用环境之间耦合度。 代码流水线管理：前一个场景对于管理代码的流水线起到了很大的帮助。代码从开发者的机器到最终在生产环境上的部署，需要经过很多的中间环境。而每一个中间环境都有自己微小的差别，Docker给应用提供了一个从开发到上线均一致的环境，让代码的流水线变得简单不少。 提高开发效率：这就带来了一些额外的好处：Docker能提升开发者的开发效率。如果你想看一个详细一点的例子，可以参考Aater在DevOpsDays Austin 2014 大会或者是DockerCon上的演讲。 不同的开发环境中，我们都想把两件事做好。一是我们想让开发环境尽量贴近生产环境，二是我们想快速搭建开发环境。 理想状态中，要达到第一个目标，我们需要将每一个服务都跑在独立的虚拟机中以便监控生产环境中服务的运行状态。然而，我们却不想每次都需要网络连接，每次重新编译的时候远程连接上去特别麻烦。这就是Docker做的特别好的地方，开发环境的机器通常内存比较小，之前使用虚拟的时候，我们经常需要为开发环境的机器加内存，而现在Docker可以轻易的让几十个服务在Docker中跑起来。 隔离应用： 有很多种原因会让你选择在一个机器上运行不同的应用，比如之前提到的提高开发效率的场景等。我们经常需要考虑两点，一是因为要降低成本而进行服务器整合，二是将一个整体式的应用拆分成松耦合的单个服务（译者注：微服务架构）。如果你想了解为什么松耦合的应用这么重要，请参考Steve Yege的这篇论文，文中将Google和亚马逊做了比较。 整合服务器：正如通过虚拟机来整合多个应用，Docker隔离应用的能力使得Docker可以整合多个服务器以降低成本。由于没有多个操作系统的内存占用，以及能在多个实例之间共享没有使用的内存，Docker可以比虚拟机提供更好的服务器整合解决方案。 调试能力：Docker提供了很多的工具，这些工具不一定只是针对容器，但是却适用于容器。它们提供了很多的功能，包括可以为容器设置检查点、设置版本和查看两个容器之间的差别，这些特性可以帮助调试Bug。你可以在《Docker拯救世界》的文章中找到这一点的例证。 多租户： 另外一个Docker有意思的使用场景是在多租户的应用中，它可以避免关键应用的重写。我们一个特别的关于这个场景的例子是为IoT（译者注：物联网）的应用开发一个快速、易用的多租户环境。这种多租户的基本代码非常复杂，很难处理，重新规划这样一个应用不但消耗时间，也浪费金钱。使用Docker，可以为每一个租户的应用层的多个实例创建隔离的环境，这不仅简单而且成本低廉，当然这一切得益于Docker环境的启动速度和其高效的diff命令。 快速部署： 在虚拟机之前，引入新的硬件资源需要消耗几天的时间。虚拟化技术（Virtualization）将这个时间缩短到了分钟级别。而Docker通过为进程仅仅创建一个容器而无需启动一个操作系统，再次将这个过程缩短到了秒级。这正是Google和Facebook都看重的特性。你可以在数据中心创建销毁资源而无需担心重新启动带来的开销。通常数据中心的资源利用率只有30%，通过使用Docker并进行有效的资源分配可以提高资源的利用率。 3 Docker优点 更快速的交付和部署 Docker在整个开发周期都可以完美的辅助你实现快速交付。Docker允许开发者在装有应用和服务本地容器做开发。可以直接集成到可持续开发流程中。例如：开发者可以使用一个标准的镜像来构建一套开发容器，开发完成之后，运维人员可以直接使用这个容器来部署代码。 Docker 可以快速创建容器，快速迭代应用程序，并让整个过程全程可见，使团队中的其他成员更容易理解应用程序是如何创建和工作的。 Docker 容器很轻很快！容器的启动时间是秒级的，大量地节约开发、测试、部署的时间。 高效的部署和扩容 Docker 容器几乎可以在任意的平台上运行，包括物理机、虚拟机、公有云、私有云、个人电脑、服务器等。 这种兼容性可以让用户把一个应用程序从一个平台直接迁移到另外一个。 更高的资源利用率 Docker 对系统资源的利用率很高，一台主机上可以同时运行数千个 Docker 容器。容器除了运行其中应用外，基本不消耗额外的系统资源，使得应用的性能很高，同时系统的开销尽量小。传统虚拟机方式运行 10 个不同的应用就要起 10 个虚拟机，而Docker 只需要启动 10 个隔离的应用即可。 更简单的管理 使用 Docker，只需要小小的修改，就可以替代以往大量的更新工作。所有的修改都以增量的方式被分发和更新，从而实现自动化并且高效的管理。 4 Docker的三个概念 镜像（Image）：类似于虚拟机中的镜像。任何应用程序运行都需要环境，而镜像就是用来提供这种运行环境的。例如一个Ubuntu镜像就是一个包含Ubuntu操作系统环境的模板，同理在该镜像上装上Apache软件，就可以称为Apache镜像。 容器（Container）：类似于一个轻量级的沙盒，可以将其看作一个极简的Linux系统环境（包括root权限、进程空间、用户空间和网络空间等），以及运行在其中的应用程序。Docker引擎利用容器来运行、隔离各个应用。容器是镜像创建的应用实例，可以创建、启动、停止、删除容器，各个容器之间是是相互隔离的，互不影响。注意：镜像本身是只读的，容器从镜像启动时，Docker在镜像的上层创建一个可写层，镜像本身不变。 仓库（Repository）：类似于代码仓库，这里是镜像仓库，是Docker用来集中存放镜像文件的地方。注意与注册服务器（Registry）的区别：注册服务器是存放仓库的地方，一般会有多个仓库；而仓库是存放镜像的地方，一般每个仓库存放一类镜像，每个镜像利用tag进行区分，比如Ubuntu仓库存放有多个版本（12.04、14.04等）的Ubuntu镜像。 5 Docker的使用5.1 Win10下安装Docker 第一步：启动虚拟环境 Win10 系统下安装Docker，首先WIN+X，点击应用和功能；之后点击右侧的“程序和功能”，接着点击左侧栏“启用或关闭Windows功能”，并做以下Hyper-V（hyper-v可以理解为虚拟机平台）的配置： 第二步：安装Toolbox 最新版 Toolbox下载地址 链接: https://pan.baidu.com/s/1Nx3gVdbRrO32elJcRBfiOA 提取码: dsd4 。下载完成后，双击下载的 Docker for Windows Installer 安装文件，一路 Next，点击 Finish 完成安装。docker toolbox是一个工具集，它主要包含以下一些内容： Docker CLI 客户端，用来运行docker引擎创建镜像和容器 Docker Machine. 可以让你在windows的命令行中运行docker引擎命令 Docker Compose. 用来运行docker-compose命令 Kitematic. 这是Docker的GUI版本 Docker QuickStart shell. 这是一个已经配置好Docker的命令行环境 Oracle VM Virtualbox. 虚拟机 安装完成后，Docker 会自动启动。通知栏上会出现个小鲸鱼的图标，这表示 Docker 正在运行。桌边也会出现三个图标，我们可以在命令行执行 docker version 来查看版本号，docker run hello-world 来载入测试镜像测试。 点击WIN+R，输入CMD打开命令行窗口，输入命令docker version结果如下： 运行docker run hello-world 来载入测试镜像测试，效果如下： 第三步：镜像加速 鉴于国内网络问题，后续拉取 Docker 镜像十分缓慢，我们可以需要配置加速器来解决，我使用的是网易的镜像地址：http://hub-mirror.c.163.com。新版的 Docker 使用 /etc/docker/daemon.json（Linux） 或者 %programdata%\docker\config\daemon.json（Windows） 来配置 Daemon。请在该配置文件中加入（没有该文件的话，请先建一个）： 123&#123; "registry-mirrors": ["http://hub-mirror.c.163.com"]&#125; 也可以通过点击小鲸鱼右键settings来设置： 5.2 Docker 常用命令 确认容器有在运行，可以通过 docker ps 来查看 使用 docker stop 容器Name 命令来停止容器 查看docker信息 docker info 删除镜像：docker rmi imageID 停用镜像：docker stop ImageID 重启镜像： docker start imageID 删除容器: docker rm ID docker inspect 来查看 Docker 的底层信息 docker images 查看docker 镜像 从 Docker Hub 网站来搜索镜像，Docker Hub 网址为： https://hub.docker.com/ 使用 docker search 命令来搜索镜像。如搜索httpd的镜像。 使用命令 docker pull 来下载镜像。 命令 docker build ， 从零开始来创建一个新的镜像 容器连接：指定容器绑定的网络地址，比如绑定 127.0.0.1。 6 Docker的实例6.1 Docker 安装 MySQL 第一步：创建MySQL镜像：docker pull mysql 查找Docker Hub上的mysql镜像： docker search mysql 第二步：下载镜像： docker pull mysql:5.6 第三步：查看镜像。列表里查到REPOSITORY为mysql,标签为5.6的镜像。docker images mysql 第四步：使用最新的MySQL镜像。 1234567891011121314151617# docker 中下载 mysqldocker pull mysql#启动，设置初始密码docker run --name bnc-mysql -p 3307:3306 -e MYSQL_ROOT_PASSWORD=root -d mysql#进入容器docker exec -it bnc-mysql bash#登录mysqlmysql -u root -p ALTER user 'root'@'%' IDENTIFIED WITH mysql_native_password BY '123456'; FLUSH PRIVILEGES; #添加远程登录用户CREATE USER 'liaozesong'@'%' IDENTIFIED WITH mysql_native_password BY '123456';GRANT ALL PRIVILEGES ON *.* TO 'liaozesong'@'%'; 第五步：远程连接MySQL数据库。 第六步：新授权用户连接测试。 6.2 Docker 安装 Python项目场景描述：我们使用一个简单的python项目，本项目是中文分词的算法。如何实现Docker安装部署。 第一步： Win10下创建目录文本 选择在D盘下创建docker目录，分别新建三个文件：Dockerfile，app.py，equirements.txt Dockerfile（没有后缀）：一个文本文件，包含了一条条的指令(Instruction)，每一条指令构建一层，因此每一条指令的内容，就是描述该层应当如何构建。创建镜像必须文件。 1234567891011121314# 基于镜像基础FROM python:3.7 # 设置代码文件夹工作目录 /appWORKDIR /app # 复制当前代码文件到容器中 /appADD . /app # 安装所需的包RUN pip install -r requirements.txt # Run app.py when the container launchesCMD ["python", "app.py"] app.py：python项目的源代码，这里测试的单个python文件，如果是一个完整项目，可以将整个文件夹拷贝到这里。 123456789101112131415# coding:utf8"""DESC: Python数据预处理之第一个分词程序范例Author：伏草惟存Prompt: code in Python3 env"""import jiebastr = "道路千万条,安全第一条;行车不规范,亲人两行泪。"print("原句: \n" + str)seg_list = jieba.cut(str)print("分词: \n" + " / ".join(seg_list)) equirements.txt ：所需要的插件，以python为例，其获取方法是cmd命令，进入到【D:\docker】目录，执行命令：pip freeze &gt; requirements.txt 第二步：生成镜像。本文采用的windows环境。docker build -t friendlyhello .命令中最后的点不要忘记，这里表示当前目录 第三步：查看镜像是否生成 第四步：运行镜像程序，这里可以看到分词效果 6.3 Docker 安装 Django项目 第一步：载入镜像。一般采用自构建的方法，本文采用直接pull下载完成。docker pull training/webapp 第二步：运行镜像。docker run -d -P training/webapp python app.py # 多个PORTS端口 第三步：浏览器输入本地ip:端口号，访问网页信息 7 技术交流共享QQ群【机器学习和自然语言QQ群：436303759】： 机器学习和自然语言（QQ群号：436303759）是一个研究深度学习、机器学习、自然语言处理、数据挖掘、图像处理、目标检测、数据科学等AI相关领域的技术群。其宗旨是纯粹的AI技术圈子、绿色的交流环境。本群禁止有违背法律法规和道德的言谈举止。群成员备注格式：城市-自命名。微信订阅号：datathinks]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 科学计算工具]]></title>
    <url>%2F2019%2F03%2F25%2FPython-%E7%A7%91%E5%AD%A6%E8%AE%A1%E7%AE%97%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[导读：Python语言及其应用，可谓如火如荼，遍地开花，读者对其并不陌生。本章主要针对科学计算工具包Numpy、SciPy、Pandaa和Matplotlib，分别从包的简介、安装、特点和常见方法几个方面介绍，本章知识也是后续章节的基础。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Numpy</tag>
        <tag>Pandas</tag>
        <tag>Scipy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python简明入门教程与网站制作]]></title>
    <url>%2F2019%2F02%2F13%2F%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8BPython%2F</url>
    <content type="text"><![CDATA[摘要：Python作为一门简洁优美且功能强大的语言，越来越受编程人员的喜欢，在工业界和学术界也非常受欢迎。本书的全部代码都是通过python实现的，之所以选择python语言，是因为其可以跨平台跨应用开发，因此本章旨在帮助读者快速领略python的概貌。如果读者已经具备python基础，可略过此章。本章首先介绍Python语言及其可以用来做什么事情。哪些人群适合学习python和python的语法特点。其次我们将介绍Python进阶，以实际案例演示常用的语句和控制流、表达式、函数、数据结构、标准库等知识点。然后扩展到python第三方库，使读者对python有个全面的理解和认识。最后一节，采用实际案例帮助读者综合运用python知识。（本文原创，转载必须注明出处.） 初识Python编程语言Python概述 Python介绍 Python是一种面向对象、直译式的计算机程序语言。它包含了一组功能完备的标准库，能够轻松完成很多常见的任务。它的语法简单，与其它大多数程序设计语言使用大括号不一样，它使用缩进来定义语句块。Python同样是一种动态语言，具备垃圾回收功能，能够自动管理内存使用。它经常被当作脚本语言用于处理系统管理任务和网络程序编写，然而它也非常适合完成各种高级任务。Python支持命令式程序设计、面向对象程序设计、函数式编程、面向侧面的程序设计、泛型编程多种编程范式。Python是完全面向对象的语言。函数、模块、数字、字符串都是对象。并且完全支持继承、重载、派生、多重继承，有益于增强源代码的复用性。Python支持重载运算符，因此Python也支持泛型设计。 Python发展历史 Python的创始人是Guido van Rossum。1989年的圣诞节期间，Guido van Rossum为了在阿姆斯特丹打发时间，决心开发一个新的脚本解释程序，作为ABC语言的一种继承。之所以选中Python作为程序的名字，是因为他是BBC电视剧——蒙提·派森的飞行马戏团（Monty Python’s Flying Circus）的爱好者。ABC是由Guido参加设计的一种教学语言。就Guido本人看来，ABC这种语言非常优美和强大，是专门为非专业程序员设计的。但是ABC语言并没有成功，究其原因，Guido认为是非开放造成的。吉多决心在Python中避免这一错误，并获取了非常好的效果，完美结合了C和其他一些语言。就这样，Python在吉多手中诞生了。目前Guido仍然是Python的主要开发者，决定整个Python语言的发展方向。Python社区经常称呼他是仁慈的独裁者。 Python标准库的主要功能 文本处理，包含文本格式化、正则表达式匹配、文本差异计算与合并、Unicode支持，二进制数据处理等功能 文件处理，包含文件操作、创建临时文件、文件压缩与归档、操作配置文件等功能 操作系统功能，包含线程与进程支持、IO复用、日期与时间处理、调用系统函数、日志（logging）等功能 网络通信，包含网络套接字，SSL加密通信、异步网络通信等功能 网络协议，支持HTTP，FTP，SMTP，POP，IMAP，NNTP，XMLRPC等多种网络协议，并提供了编写网络服务器的框架 W3C格式支持，包含HTML，SGML，XML的处理 其它功能，包括国际化支持、数学运算、HASH、Tkinter等 Python优缺点 (1) 优点 简单、易学、免费、开源、高层语言（无需考虑如何管理程序使用内存等细节问题） 可移植性（这些平台包括Linux、Windows、FreeBSD、Macintosh、Solaris、OS/2、Amiga、AROS、AS/400、BeOS、OS/390、z/OS、Palm OS、QNX、VMS、Psion、Acom RISC OS、VxWorks、PlayStation、Sharp Zaurus、Windows CE、PocketPC、Symbian以及Google基于linux开发的android平台） 解释性、面向对象、可扩展性（可以部分程序用C或C++编写，然后在Python程序中使用它们） 可嵌入性（可以把Python嵌入C/C++程序，从而向程序用户提供脚本功能）、丰富的库、规范的代码 (2) 缺点 单行语句和命令行输出问题、独特的语法、运行速度慢（与C和C++相比） Python开发环境 通用IDE / 文本编辑器，很多并非集成开发环境软件的文本编辑器，也对Python有不同程度的支持。本文默认开发环境均集成在Anaconda中，第1章已经详细介绍过。此外，还有如下开发环境： Eclipse + pydev插件，目前对Python 3.X只支持到3.0 emacs +插件 NetBeans +插件 SlickEdit TextMate Python Tools for Visual Studio Vim +插件 Sublime Text +插件 EditPlus UltraEdit PSPad Editra由Python开发的程序编辑器。 Notepad++ Python能做什么？ Python能做什么 系统编程：提供API，能方便进行系统维护和管理，很多系统管理员理想的编程工具 。 图形处理：有PIL、Tkinter等图形库支持，能方便进行图形处理。 数学处理：NumPy扩展提供大量与许多标准数学库的接口。 文本处理：python提供的re模块能支持正则表达式，还提供SGML，XML分析模块，许多程序员利用python进行XML程序的开发。 数据库编程：程序员可通过遵循Python DB-API（数据库应用程序编程接口）规范的模块与Microsoft SQL Server，Oracle，Sybase，DB2，MySQL、SQLite等数据库通信。python自带有一个Gadfly模块，提供了一个完整的SQL环境。 网络编程：提供丰富的模块支持sockets编程，能方便快速地开发分布式应用程序。 Web编程：应用的开发语言，支持最新的XML技术。 多媒体应用：能进行二维和三维图像处理。PyGame模块可用于编写游戏软件。 黑客编程：python有一个hack的库,内置你熟悉的或不熟悉的函数，但是缺少成就感。 Python开发的应用案例 Reddit - 社交分享网站 Dropbox - 文件分享服务 豆瓣网 - 图书、唱片、电影等文化产品的资料数据库网站 Django - 鼓励快速开发的Web应用框架 Pylons - Web应用框架 Zope - 应用服务器 Plone - 内容管理系统 TurboGears - 另一个Web应用快速开发框架 Twisted - Python的网络应用程序框架 Fabric - 用于管理成百上千台Linux主机的程序库 Python Wikipedia Robot Framework - MediaWiki的机器人程序 MoinMoinWiki - Python写成的Wiki程序 Trac - 使用Python编写的BUG管理系统 Mailman - 使用Python编写的邮件列表软件 Mezzanine - 基于Django编写的内容管理系统系统 flask - Python微Web框架 Webpy - Python微Web框架 Bottle - Python微Web框架 EVE - 网络游戏EVE大量使用Python进行开发 Blender - 使用Python作为建模工具与GUI语言的开源3D绘图软件 Inkscape - 一个开源的SVG矢量图形编辑器。 知乎 - 一个问答网站 果壳 - 一个泛科技主题网站 Python适合谁去学？ 知乎精选： 于这个问题，我先带着大家去知乎看看，大家若感兴趣也可以去知乎搜索下，基本上语调是一致的。就笔者本人而言，本科主要是net技术研究，在C#学习上花费很多了精力和时间。后来读研初期又开始java方面学习。俗话说“技多不压身”，但也总是因为研究方向的客观变化而转战于不同语言之间，外加语言环境平台也会浪费不少时间，最终均没有深入下去。反之，python的跨平台性就优势凸显了，你习惯Linux命令行，完全可以适应。接近伪代码的操作为你节省不少时间，特别在文本处理，自然语言分析方面，笔者之前用java编写，耗费一番功夫。总而言之，园子里面多数同学为本科在读生，在拥有一门入门语言的情况下，研究下python我觉得是值得的，这也是大的趋势。无论你做运维、web开发、亦或算法研究以及大数据分析，你都会用到它。前天与一家大数据公司技术负责人聊天，他们产品全是python，从文本处理到数据清洗分析，直到模型构建结果评价。读者也可以看看“我爱自然语言处理”社区，里面的求职信息无论数据挖掘、自然语言处理还是机器学习等均要求python经验。 Python语法和特点 Python基本语法 编码：默认情况下，Python 3 源码文件以 UTF-8 编码，所有字符串都是 unicode 字符串。 当然你也可以为源码文件指定不同的编码： # -*- coding: cp-1252 -*- 标识符： 在python里，标识符由字母、数字、下划线组成。 在python中，所有标识符可以包括英文、数字以及下划线（_），但不能以数字开头。 python中的标识符是区分大小写的。 以下划线开头的标识符是有特殊意义的： 以单下划线开头（_foo）的代表不能直接访问的类属性，需通过类提供的接口进行访问，不能用"from xxx import *"而导入； 以双下划线开头的（__foo）代表类的私有成员；以双下划线开头和结尾的（__foo__）代表python里特殊方法专用的标识。 python保留字：保留字即关键字，我们不能把它们用作任何标识符名称。 >>> import keyword >>> keyword.kwlist ['False', 'None', 'True', 'and', 'as', 'assert', 'break', 'class', 'continue', 'def', 'del', 'elif', 'else', 'except', 'finally', 'for', 'from', 'global', 'if', 'import', 'in', 'is', 'lambda', 'nonlocal', 'not', 'or', 'pass', 'raise', 'return', 'try', 'while', 'with', 'yield'] 注释：Python中单行注释以 # 开头，实例如下： # 第一个注释 print ("Hello, Python!") # 第二个注释 行与缩进：python最具特色的就是使用缩进来表示代码块，不需要使用大括号({})。四个空格或者Tab进行缩进。 if True: print ("True") else: print ("False") 多行语句：Python 通常是一行写完一条语句，但如果语句很长，我们可以使用反斜杠()来实现多行语句，例如： total = item_one + \ item_two + \ item_three 在 [], {}, 或 () 中的多行语句，不需要使用反斜杠()，例如： total = ['item_one', 'item_two', 'item_three', 'item_four', 'item_five'] 引号:Python 接收单引号(‘ )，双引号(“ )，三引号(‘’’ “””) 来表示字符串，引号的开始与结束必须是相同类型的。 word = 'word' sentence = "这是一个句子。" paragraph = """这是一个段落。 包含了多个语句""" 字符串 python中单引号和双引号的使用完全相同。 使用三引号('''或""")可以指定一个多行字符串。 转义符 '\'。 自然字符串， 通过在字符串前加r或R。 如 r"this is a line with \n" 则\n会显示，而不是换行。 python允许处理unicode字符串，加前缀u或U， 如 u"this is an unicode string"。 字符串是不可变的。 按字面意义级联字符串，如"this " "is " "string"会被自动转换为this is string。 空行 函数之间或类的方法之间用空行分隔，表示一段新的代码的开始。类和函数入口之间也用一行空行分隔，以突出函数入口的开始。 Print 输出：print 默认输出是换行的，如果要实现不换行需要在变量末尾加上 end=””： x="a" y="b" # 换行输出 print( x ) print( y ) print('---------') # 不换行输出 print( x, end=" " ) print( y, end=" " ) print() 运行结果： a b --------- a b import 与 from…import：在 python 用 import 或者 from…import 来导入相应的模块。 (1) 将整个模块(somemodule)导入，格式为： import somemodule (2) 从某个模块中导入某个函数，格式为： from somemodule import somefunction (3) 从某个模块中导入多个函数，格式为： from somemodule import firstfunc, secondfunc, thirdfunc (4) 将某个模块中的全部函数导入，格式为： from somemodule import * # 导入 sys 模块 import sys print('================Python import mode=========================='); print ('命令行参数为:') for i in sys.argv: print (i) print ('\n python 路径为',sys.path) # 导入 sys 模块的 argv,path 成员 from sys import argv,path # 导入特定的成员 print('================python from import===================================') print('path:',path) # 因为已经导入path成员，所以此处引用时不需要加sys.path 数据类型 表1-1 python数据类型 类型 描述 例子 str 一个由字符组成的不可更改的有序列。在Python 3.x里，字符串由Unicode字符组成。 Wikipedia、Wikipedia、Spanning、multiple、lines bytes 一个由字节组成的不可更改的有序列 b'Some ASCII' b"Some ASCII" list 可以包含多种类型的可改变的有序列 [4.0, 'string', True] tuple 可以包含多种类型的不可改变的有序列 (4.0, 'string', True) set/frozenset 与数学中集合的概念类似。无序的、每个元素唯一。 {4.0, 'string', True}/frozenset([4.0, 'string', True]) dict或map 一个可改变的由键值对组成的无序列 {'key1': 1.0, 3: False} int 精度不限的整数 42 float 浮点数。精度与系统相关。 3.1415927 complex 复数 3+2.7j bool 逻辑值。只有两个值：真、假。 True False Python进阶Hello World编程第一步，打印“Hello World”的输入结果： def callHello(): print("hello world！") callHello() 运行结果： hello world！ 语句和控制流 if 语句::根据成绩评判等级 # if 语句:根据成绩评判等级 def CallLevel(score): if score >= 90 : print("优秀") elif score >= 60: print("及格") else: print("不及格") CallLevel(score=60) 运行结果： 及格 - for 语句：循环输出所有评分指标 # for 语句:循环输出所有评分指标 def GetLevel(score): for lev in score: print(lev,end=" ") # 设置不换行 GetLevel(score=["优秀","及格","不及格"]) # 列表参数 运行结果： 优秀 及格 不及格 - while 语句：循环输出所有评分 # while 语句:循环输出所有评分 def GetLevel2(score): countlen=len(score) while countlen > 0: print(score[countlen-1],end=" ") # 设置不换行 countlen -= 1 GetLevel2(score=[90,30,100,98,60]) 运行结果： 60 98 100 30 90 - range() 函数：循环输出所有评分指标的下标 # range() 函数:循环输出所有评分指标的下标 def GetValue(score): for lev in range(len(score)): print(lev,end=" ") GetValue(score=["优秀","及格","不及格"]) 运行结果： 0 1 2 - break 语句：统计成绩优秀学生的个数 # break 语句:不及格的跳过 def GetHighLev(score): result=0 for lev in score: if lev < 90: break else: result += 1 print("成绩优秀的学生有："+str(result)+"位。") GetHighLev(score=[90,30,100,98,60]) 运行结果： 成绩优秀的学生有：1位。 分析：实际优秀者个数是90,100,98共计3位，输出结果却是1位。造成这种结果的原因是，当循环输入列表90时候，满足条件自动加1.继续输入30，不满足条件，直接跳出整个程序，输出最后一条语句。 - continue 语句：统计优秀成绩的个数 # continue 语句:统计优秀成绩的个数 def GetHighLev2(score): result=0 for lev in score: if lev < 90: continue else: result += 1 print("成绩优秀的学生有："+str(result)+"位。") GetHighLev2(score=[90,30,100,98,60]) 运行结果： 成绩优秀的学生有：3位。 - pass 语句：什么也不做，占位符 # pass 语句：什么也不做，占位符 def callPass(): pass print(callPass()) 运行结果： None ## 函数 - 定义函数：斐波那契数列 # 输出指定数的斐波那契数列 def fib(n): a, b = 0, 1 while a < n: print(a, end=' ') a, b = b, a+b print() fib(100) 运行结果： 0 1 1 2 3 5 8 13 21 34 55 89 结果分析： 关键字 def 引入了一个函数定义。在其后必须跟有函数名和包括形式参数的圆括号。函数体语句从下一行开始，必须是缩进的。一个函数定义会在当前符号表内引入函数名。函数名指代的值（即函数体）有一个被 Python 解释器认定为用户自定义函数的类型。 - 函数默认参数 # 函数默认参数 def sayhello(name="Tom"): print("Hello,"+name) sayhello() 运行结果： Hello,Tom - 关键字参数 # 关键字参数 def person(name, age, **kw): #前两个是必须参数，最后一个可选可变参数 print('name:', name, 'age:', age, 'other:', kw) person("Tome",30) # 只调用必须参数 person("Tom",30,city="ChengDu",sex="man") # 自定义关键字参数 运行结果： name: Tome age: 30 other: {} name: Tom age: 30 other: {'sex': 'man', 'city': 'ChengDu'} - 可变参数 # 可变参数 def concat(*args, sep="/"): print(sep.join(args)) concat('我','是','可变','参数') 运行结果： 我/是/可变/参数 - Lambda 形式 # Lambda 形式 def Lambda(nums): nums.sort(key=lambda num: num[0]) print(nums) Lambda(nums = [(1, 'one'), (2, 'two'), (3, 'three'), (4, 'four')]) 运行结果： [(1, 'one'), (2, 'two'), (3, 'three'), (4, 'four')] ## List 列表 - list常用方法 list.append(x) 添加元素到列表尾部 list.extend(L) 列表合并 list.insert(i, x) 在指定位置插入一个元素 list.remove(x) 删除列表中值为 x 的第一个元素 list.pop([i]) 从列表的指定位置删除元素，并将其返回 list.clear() 从列表中删除所有元素，相当于 del a[:] list.index(x) 返回列表中第一个值为 x 的元素的索引 list.count(x) 返回 x 在列表中出现的次数 list.sort() 对列表中的元素就地进行排序 list.reverse() 就地倒排列表中的元素 list.copy() 返回列表的一个浅拷贝，等同于 a[:] 列表的切分 # 列表的切分 def calllist(names): print(names[-1:]) # 输出列表最后一个值 print(names[:3]) # 输出列表前3个值 calllist(names=['this','is','a','list']) 运行结果： [&#39;list&#39;] [&#39;this&#39;, &#39;is&#39;, &#39;a&#39;] List ：列表作堆栈(先进先出)把列表当作堆栈使用def SomeList(stack): print(“原始列表（栈）：”,end=’ ‘) print(stack) stack.append(‘贺知章’) stack.append(‘杜牧’) print(“追加后列表（栈）：”,end=’ ‘) print(stack) stack.pop() stack.pop() print(“出栈后的数据：”,end=’ ‘) print(stack) SomeList(stack=[‘李白’,’杜甫’, ‘白居易’])&lt;/pre&gt; 运行结果：(先进先出) 原始列表（栈）： [&#39;李白&#39;, &#39;杜甫&#39;, &#39;白居易&#39;] 追加后列表（栈）： [&#39;李白&#39;, &#39;杜甫&#39;, &#39;白居易&#39;, &#39;贺知章&#39;, &#39;杜牧&#39;] 出栈后的数据： [&#39;李白&#39;, &#39;杜甫&#39;, &#39;白居易&#39;] List :列表作队列(先进后出)把列表当作队列使用：使用队列时调用collections.deque，它为在首尾两端快速插入和删除而设计。from collections import dequedef SomeList2(queue): print(“原始列表：”,end=’ ‘) print(queue) queue.append(“李商隐”) queue.append(“杜牧”) print(“入队的列表：”,end=’ ‘) print(queue) queue.popleft() queue.popleft() print(“出队后列表：”,end=’ ‘) print(queue) SomeList2(queue = deque([‘李白’,’杜甫’, ‘白居易’]))&lt;/pre&gt; 运行结果：(先进后出) 原始列表： deque([&#39;李白&#39;, &#39;杜甫&#39;, &#39;白居易&#39;]) 入队的列表： deque([&#39;李白&#39;, &#39;杜甫&#39;, &#39;白居易&#39;, &#39;李商隐&#39;, &#39;杜牧&#39;]) 出队后列表： deque([&#39;白居易&#39;, &#39;李商隐&#39;, &#39;杜牧&#39;]) 列表推导式 # 列表推导式 def callList(nums): squares = [n**2 for n in nums] print(squares) callList(nums=[2,4,6,8]) 运行结果： [4, 16, 36, 64] 列表的矩阵转秩矩阵转秩def countList(matrix): result = [[row[i] for row in matrix] for i in range(4)] print(result) matrix = [ [1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12],]countList(matrix)&lt;/pre&gt; 运行结果： [[1, 5, 9], [2, 6, 10], [3, 7, 11], [4, 8, 12]] 元组虽然元组和列表很类似，但它们经常用在不同的情况和不同的用途上。元组有很多用途，例如 (x, y) 坐标对、数据库中的员工记录等等。元组就像字符串， 是不可变的；列表是可变的 ，它们的元素通常是相同类型的，并需要通过迭代访问。 操作元组 def calltuple(tuples): for t in tuples: print(t+"\t",end=" ") print() calltuple(tuples=('百度','阿里巴巴','腾讯')) # 元组参数 运行结果： 百度 阿里巴巴 腾讯 set集合集合是一个无序不重复元素的集。基本功能包括关系测试和消除重复元素。集合对象还支持 union（联合），intersection（交），difference（差）和 sysmmetric difference（对称差集）等数学运算。 操作set集合def callset(basket): result= set(basket) print(result) callset(basket = {‘apple’, ‘orange’, ‘apple’, ‘pear’, ‘orange’, ‘banana’})&lt;/pre&gt; 运行结果： {&#39;pear&#39;, &#39;orange&#39;, &#39;apple&#39;, &#39;banana&#39;} 字典理解字典的最佳方式是把它看做无序的键：值对（key:value 对）集合，键必须是互不相同的（在同一个字典之内）。一对大括号创建一个空的字典： {} 。初始化列表时，在大括号内放置一组逗号分隔的键：值对，这也是字典输出的方式。 操作字典def calldict(dicts): print(“原始字典”,end=’ ‘) print(dicts) # 原始字典 dicts[‘欧阳修’] = ‘宋朝’ # 追加字典 print(“追加后的字典”,end=’ ‘) print(dicts) # 追加后的字典 print(“字典键的集合”,end=’ ‘) print(list(dicts.keys())) # 字典键的集合 print(“字典键的排序”,end=’ ‘) print(sorted(dicts.keys())) # 字典键的排序 print(“字典值的集合”,end=’ ‘) print(list(dicts.values())) # 字典值的集合 calldict(dicts = {‘李白’: ‘唐朝’, ‘杜甫’: ‘唐朝’})&lt;/pre&gt; 运行结果： 原始字典 {&#39;杜甫&#39;: &#39;唐朝&#39;, &#39;李白&#39;: &#39;唐朝&#39;} 追加后的字典 {&#39;杜甫&#39;: &#39;唐朝&#39;, &#39;李白&#39;: &#39;唐朝&#39;, &#39;欧阳修&#39;: &#39;宋朝&#39;} 字典键的集合 [&#39;杜甫&#39;, &#39;李白&#39;, &#39;欧阳修&#39;] 字典键的排序 [&#39;李白&#39;, &#39;杜甫&#39;, &#39;欧阳修&#39;] 字典值的集合 [&#39;唐朝&#39;, &#39;唐朝&#39;, &#39;宋朝&#39;] 面向对象编程：类 通过案例理解python中的类： 父类是动物类，有初始化函数，且有动物讲话的方法；子类是一个狗类，继承父类所有属性，并扩展自己方法，调用子类讲话方法，并直接调用父类讲话方法。 """ # 欢迎进入我的主页：http://www.cnblogs.com/baiboy/. """ class BaseAnimal: # 父类：动物 def __init__(self,name,age): # 初始化方法：括号里是形参 self.name=name self.age=age def speak(self): # 父类的行为方法 print("我的名字是[ %s ],今年[ %d ]岁" %(self.name,self.age)) class SubDog(BaseAnimal): # 子类：小狗 def __init__(self,name,age,say): # 初始化方法：括号里是形参 BaseAnimal.__init__(self,name,age) self.say=say print("这是子类[ %s ]."%(self.name)) print('_'*20+'调用子函数方法'+'_'*20) def talk(self): # 子类的行为方法 # BaseAnimal.speak(self) # 调用父类的行为方法 print("我的名字是[ %s ],今年[ %d ]岁,我想说： %s" %(self.name,self.age,self.say)) ani=SubDog('dog',12,'汪汪...') print(ani.talk()) print('_'*20+'直接调用父函数方法'+'_'*20) BaseAnimal('tom',13).speak() 运行结果： 这是子类[ dog ]. ____________________调用子函数方法____________________ 我的名字是[ dog ],今年[ 12 ]岁,我想说： 汪汪... ____________________直接调用父函数方法____________________ 我的名字是[ tom ],今年[ 13 ]岁 解析： init 方法（双下划线）： init 方法在类的一个对象被建立时，马上运行。这个方法可以用来对你的对象初始化。注意，这个名称的开始和结尾都是双下划线。我们把init方法定义为取一个参数name（以及普通的参数self）。在 init 方法里，我们只是创建一个新的域，也称为name。注意:它们是两个不同的变量，尽管它们有相同的名字。点号使我们能够区分它们。最重要的是，在创建一个类的新实例的时候，把参数包括在圆括号内跟在类名后面，从而传递给 init 方法。这是这种方法的重要之处。现在，我们能够在我们的方法中使用self.name域。注释 init 方法类似于C++、C#和Java中的constructor 。 标准库Python拥有一个强大的标准库。Python语言的核心只包含数字、字符串、列表、字典、文件等常见类型和函数，而由Python标准库提供了系统管理、网络通信、文本处理、数据库接口、图形系统、XML处理等额外的功能。Python标准库的主要功能有： 文本处理，包含文本格式化、正则表达式匹配、文本差异计算与合并、Unicode支持，二进制数据处理等功能 文件处理，包含文件操作、创建临时文件、文件压缩与归档、操作配置文件等功能 操作系统功能，包含线程与进程支持、IO复用、日期与时间处理、调用系统函数、日志（logging）等功能 网络通信，包含网络套接字，SSL加密通信、异步网络通信等功能 网络协议，支持HTTP，FTP，SMTP，POP，IMAP，NNTP，XMLRPC等多种网络协议，并提供了编写网络服务器的框架 W3C格式支持，包含HTML，SGML，XML的处理。 其它功能，包括国际化支持、数学运算、HASH、Tkinter等 Python社区提供了大量的第三方模块，使用方式与标准库类似。它们的功能覆盖科学计算、Web开发、数据库接口、图形系统多个领域。第三方模块可以使用Python或者C语言编写。SWIG，SIP常用于将C语言编写的程序库转化为Python模块。Boost C++ Libraries包含了一组库，Boost.Python，使得以Python或C++编写的程序能互相调用。Python常被用做其他语言与工具之间的“胶水”语言。 Python深入—第三方库Web框架 Django： 开源Web开发框架，它鼓励快速开发并遵循MVC设计，开发周期短。 Flask： 轻量级的Web框架。 Pyramid： 轻量，同时有可以规模化的Web框架，Pylon projects 的一部分。 ActiveGrid： 企业级的Web2.0解决方案。 Karrigell： 简单的Web框架，自身包含了Web服务，py脚本引擎和纯python的数据库PyDBLite。 Tornado： 一个轻量级的Web框架，内置非阻塞式服务器，而且速度相当快 webpy： 一个小巧灵活的Web框架，虽然简单但是功能强大。 CherryPy： 基于Python的Web应用程序开发框架。 Pylons： 基于Python的一个极其高效和可靠的Web开发框架。 Zope： 开源的Web应用服务器。 TurboGears： 基于Python的MVC风格的Web应用程序框架。 Twisted： 流行的网络编程库，大型Web框架。 Quixote： Web开发框架。科学计算 Matplotlib： 用Python实现的类matlab的第三方库，用以绘制一些高质量的数学二维图形。 Pandas： 用于数据分析、数据建模、数据可视化的第三方库。 SciPy： 基于Python的matlab实现，旨在实现matlab的所有功能。 NumPy： 基于Python的科学计算第三方库，提供了矩阵，线性代数，傅立叶变换等等的解决方案。GUI PyGtk： 基于Python的GUI程序开发GTK+库。 PyQt： 用于Python的QT开发库。 WxPython： Python下的GUI编程框架，与MFC的架构相似。其他库 BeautifulSoup： 基于Python的HTML/XML解析器，简单易用。 gevent： python的一个高性能并发框架,使用了epoll事件监听、协程等机制将异步调用封装为同步调用。 PIL： 基于Python的图像处理库，功能强大，对图形文件的格式支持广泛。目前已无维护，另一个第三方库Pillow实现了对PIL库的支持和维护。 PyGame： 基于Python的多媒体开发和游戏软件开发模块。 Py2exe： 将python脚本转换为windows上可以独立运行的可执行程序。 Requests： 适合于人类使用的HTTP库，封装了许多繁琐的HTTP功能，极大地简化了HTTP请求所需要的代码量。 scikit-learn： 机器学习第三方库，实现许多知名的机器学习算法。 TensorFlow： Google开发维护的开源机器学习库。 Keras： 基于TensorFlow，Theano与CNTK的高级神经网络API。 SQLAlchemy： 关系型数据库的对象关系映射(ORM)工具。 Python实战：Django轻松搞定用户管理系统Django简介 Django是什么 Django是一个开放源代码的Web应用框架，由Python写成。采用了MVC的软件设计模式，即模型M，视图V和控制器C。它最初是被开发来用于管理劳伦斯出版集团旗下的一些以新闻内容为主的网站的。该框架于2005年7月在BSD许可证下发布。这套框架是以比利时的吉普赛爵士吉他手Django Reinhardt来命名的。 Django的主要目标是使得开发复杂的、数据库驱动的网站变得更简单。Django注重组件的重用性和“可插拔性”，敏捷开发和DRY法则（Don’t Repeat Yourself）。在Django中Python被普遍使用，甚至包括配置文件和数据模型。 Django开发模型 Django是一个基于MVC构造的框架。但是在Django中，控制器接受用户输入的部分由框架自行处理，所以 Django 里更关注的是模型（Model）、模板(Template)和视图（Views），称为 MTV模式。它们各自的职责如图2-1 所示： ![](https://i.imgur.com/smjaw0A.png) 图2-1 Django开发模型图 模型（Model），即数据存取层。处理与数据相关的所有事务： 如何存取、如何验证有效性、包含哪些行为以及数据之间的关系等。 视图（View），即表现层。处理与表现相关的决定： 如何在页面或其他类型文档中进行显示。 模板(Template)，即业务逻辑层。存取模型及调取恰当模板的相关逻辑。模型与模板的桥梁。 Django的架构 让我们一览 Django 全貌： urls.py 网址入口，关联到对应的views.py中的一个函数（或者generic类），访问网址就对应一个函数。 views.py 处理用户发出的请求，从urls.py中对应过来, 通过渲染templates中的网页可以将显示内容（比如登陆后的用户名，用户请求的数据等）输出到网页。 models.py 与数据库操作相关，存入或读取数据时用到这个，当然用不到数据库的时候你可以不使用。 forms.py 表单，用户在浏览器上输入数据提交，对数据的验证工作以及输入框的生成等工作，当然你也可以不使用。 templates 文件夹 views.py 中的函数渲染templates中的Html模板，得到动态内容的网页，当然可以用缓存来提高速度。 admin.py 后台，可以用很少量的代码就拥有一个强大的后台。 settings.py Django 的设置，配置文件，比如 DEBUG 的开关，静态文件的位置等。 上面的py文件不理解也没有关系，后面会详细介绍。一图胜千言，架构全貌工作机制如图2-2所示： ![](https://i.imgur.com/XUfmzHe.png) 图2-2 Django架构图 Django商业网站 Sohu 邮箱 、果壳网 、 豆瓣 、 爱调研 、 易度在线云办公 、 优容网 、 快玩游戏、九九房、贷帮网 、 趣奇网 、知乎、时尚时空 、游嘻板: YxPad webpy、DNSPod 国际版 、下厨房 、 贝太厨房 、 Wopus问答 、 咕咚网 、扇贝网 、站长工具、易度文档管理系统、个人租房、 在线文档查看-易度云查看 、 FIFA310 足球数据分析专家、 搜狐随身看等等。 Django安装配置 前置条件 预设读者具备Python、web开发、html、css、js、SQL基础。 Anaconda(包含pip和python插件)、sublime环境已经安装。其中pip用来在线安装工具包。 系统环境：WIN10 64bit 开发环境：sublime+Anaconda 数据库：Mysql 5.6.17 语言：python3.5 框架：django1.11 通过“Win+R”键打开运行对话框，输入“pip”查看是否安装成功。如果pip输入报错，则需要自行下载安装即可。如果成功安装如图2-3所示： ![](https://i.imgur.com/rboSP9T.png) 图2-3 pip安装成功图 在线安装Django 通过“Win+R”键打开运行对话框，输入“pip install django”代码，自动运行结束后。打开Sublime编辑器，点击“F6”切换的编辑环境，输入如下代码查看django是否安装成功。 >>> import django >>> django.VERSION (1, 11, 0, 'final', 1) 创建用户管理项目 创建项目和App （1） 通过“Win+R”键打开运行对话框，并在电脑的E盘根目录下创建名为UserProject的项目,输入如下创建命名： django-admin startproject xmjc_analysis 命令执行完成，通过Sublime Text3打开菜单“File->Open Folder”选择刚刚创建的UserProject文件夹，如图2-4所示： ![](https://i.imgur.com/h0AgYXf.png) 图2-4 新建django项目 - settings.py 项目的设置文件 - urls.py 总的urls配置文件 - wsgi.py 部署服务器文件 - __ init__.py python包的目录结构必须的，与调用有关 （2） 进入UserProject文件夹下创建App名为myuser，执行如下命令，命令成功后如图2-5所示： django-admin startapp myuser ![](https://i.imgur.com/Bc6jJGN.png) 图2-5 执行app创建命令 一个项目可以包含多个App，执行完app创建命令后，打开项目查看如图2-6所示： ![](https://i.imgur.com/2yOopSh.png) 图2-6 创建app成功图 - admin.py 后台，可以用很少量的代码就拥有一个强大的后台。 - apps.py 本app项目名称等配置。 - models.py 与数据库操作相关，存入或读取数据时用到这个，当然用不到数据库的时候 你可以不使用。 - tests.py 单元测试文件 - views.py 处理用户发出的请求，从urls.py中对应过来, 通过渲染templates中的网页可以将显示内容，比如登陆后的用户名，用户请求的数据，输出到网页。 （3） 将新定义名为“myuser”的app添加到UserProject项目中。修改UserProject/UserProject/settings.py代码如下： # Application definition INSTALLED_APPS = [ 'django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', # 添加新建app 'myuser', ] 到此，完成项目的创建和app的添加。是不是很简单呢？下面让我们创建一个新的欢迎界面。 第一个欢迎页面 （1） 在myuser文件夹下，打开UserProject/myuser/views.py文件,修改里面的代码如下： ''' 第一个页面 author：白宁超 site：http://www.cnblogs.com/baiboy/ ''' #coding:utf-8 from django.shortcuts import render from django.http import HttpResponse def index(request): return HttpResponse(u"欢迎进入第一个Django页面!") 第一行是声明编码为utf-8, 因为我们在代码中用到了中文,如果不声明就报错. 第二行引入HttpResponse，它是用来向网页返回内容的，就像Python中的 print 一样，只不过 HttpResponse 是把内容显示到网页上。 我们定义了一个index()函数，第一个参数必须是 request，与网页发来的请求有关，request 变量里面包含get或post的内容。 （2）我们打开UserProject/UserProject/urls.py 这个文件, 修改其中的代码 from django.conf.urls import url from django.contrib import admin # 导入myuser下的视图文件 from myuser import views as myuser_views urlpatterns = [ # 第一个欢迎界面的配置 url(r'^index/$', myuser_views.index,name='index'), url(r'^admin/', admin.site.urls), ] （3） 在UserProject目录下输入“python manage.py runserver”命令本地运行服务器，如图2-7所示： ![](https://i.imgur.com/fwjrzhr.png) 图2-7 运行本地服务器 （4） 将图2-7的本地网址复制到浏览器访问，本地网址后面输入刚刚配置的方法名“index”即可访问我们刚刚设计的网页。如图2-8所示： ![](https://i.imgur.com/JbWX3aW.png) 图2-8 django运行的欢迎界面 带参数的欢迎页面 （1） 打开UserProject/myuser/views.py文件,修改里面的代码如下： ''' #coding:utf-8 from django.shortcuts import render from django.http import HttpResponse # 访问第一个页面 def index(request): return HttpResponse(u"欢迎进入第一个Django页面!") # 访问第一个带参数的页面 def indexPara(request): name = request.GET['name'] return HttpResponse(u"欢迎进入\t"+name+",第一个Django页面!") （2）我们打开UserProject/UserProject/urls.py 这个文件, 修改其中的代码 from django.conf.urls import url from django.contrib import admin # 导入myuser下的视图文件 from myuser import views as myuser_views urlpatterns = [ # 带参数的欢迎界面的配置 url(r'^para/$', myuser_views.indexPara,name='para'), # 第一个欢迎界面的配置 url(r'^index/$', myuser_views.index,name='index'), url(r'^admin/', admin.site.urls), ] （3） 如果服务器关闭，需要如上方法重新打开。反之，直接输入网址查看，如图2-9所示： ![](https://i.imgur.com/CGTI72H.png) 图2-9 django运行带参数的欢迎界面 > 单数据库配置 （1） 在UserProject/UserProject/settings.py文件下,默认sqlites数据库，现在假设项目需求是mysql数据库。具体修改如下： # Database # https://docs.djangoproject.com/en/1.11/ref/settings/#databases DATABASES = { # 配置默认为mysql数据库 'default': { 'ENGINE': 'django.db.backends.mysql', # 设置数据库引擎 'NAME': 'test', # 数据库名 'USER': 'test', # 数据库登陆名 'PASSWORD': 'test123', # 数据库登陆密码 'HOST':'localhost', # 数据库服务器IP，默认设置本地 'PORT':'3306', # 端口号 }, # 默认配置为sqlite3数据库 # 'default': { # 'ENGINE': 'django.db.backends.sqlite3', # 'NAME': os.path.join(BASE_DIR, 'db.sqlite3'), # } } UserProject/UserProject/ init.py文件添加如下代码： import pymysql pymysql.install_as_MySQLdb() （2） 在UserProject/myuser/models.py下设计数据库表，采用ORM方式，完整代码如下： from django.db import models # Create your models here. class User(models.Model): username = models.CharField('用户名', max_length=30) userpass = models.CharField('密码',max_length=30) useremail = models.EmailField('邮箱',max_length=30) usertype = models.CharField('用户类型',max_length=30) def __str__(self): return self.username （3） 在analysis/admin.py中定义显示数据 from django.contrib import admin from .models import User class UserAdmin(admin.ModelAdmin): list_display = ('username','userpass','useremail') # 自定义显示字段 admin.site.register(User,UserAdmin) （4） 打开对话框，在“UserProject”下将生成的py文件应用到mysql数据库。输入如下命令： python manage.py makemigrations python manage.py migrate 运行效果如果2-10所示： ![](https://i.imgur.com/CblSJwf.png) 图2-10 ORM创建单例MySQL数据库 （5）创建超级管理员：用户名 admin1；密码密码 admin123，如图2-11、2-12所示： python manage.py createsuperuser ![](https://i.imgur.com/7YJgJ9v.png) 图2-11 创建后台超级管理员 ![](https://i.imgur.com/P8o9jPw.png) 图2-12 创建后台超级管理员 （6）登录后台查看信息 可以看到后台信息，并对数据表进行增删改查操作，但是后台全部英文如图2-13所示，可以改为中文显示？ ![](https://i.imgur.com/SWDbf6o.png) 图2-13 英文管理界面 后台管理设置为中文显示,UserProject/UserProject/settings.py下修改代码： LANGUAGE_CODE = 'zh-Hans' # 中文显示 再去查看： ![](https://i.imgur.com/yCz4Atj.png) 图2-14 中文后台管理界面 Django数据库操作 QuerySet API，shell玩转MySql 在UserProject项目下输入【 python manage.py shell】，然后查询数据表如图2-15所示： ![](https://i.imgur.com/MwMAay7.png) 图2-15 Shell操作MySQL数据库 创建一条用户信息，输入如下shell命令： User.objects.create(username="李白", userpass="libai123",useremail="libai@163.com",usertype="超级管理员") 登录项目后台查看，如图2-16所示。 ![](https://i.imgur.com/tDHK2vF.png) 图2-16 后台查看新加用户信息 > 其他shell操作语句 # 方法 1 User.objects.create(username="李白", userpass="libai123",useremail="libai@163.com",usertype="超级管理员") # 方法 2 twz =User(username="李白", userpass="libai123",useremail="libai@163.com",usertype="超级管理员") twz.save() # 获取对象： Person.objects.all() # 满足条件查询 User.objects.filter(username="李白") # 迭代查询： es = Entry.objects.all() for e in es: print(e.headline) # 查询排序： User.objects.all().order_by('username') # 链式查询： User.objects.filter(name__contains="WeizhongTu").filter(email="tuweizhong@163.com") # 去重查询： qs = qs.distinct() # 删除操作： User.objects.all().delete() # 更新操作： Person.objects.filter(name__contains="abc").update(name='xxx') # 数据的导出： python manage.py dumpdata [appname] > appname_data.json python manage.py dumpdata blog > blog_dump.json # 导出用户数据 python manage.py dumpdata auth > auth.json # 导出用户数据 批量向数据表导入数据 （1） 将name.txt导入数据库，数据内容如下： 张一|admin|zhang1@qq.com|超级管理员 张二|admin|zhang2@qq.com|超级管理员 张三|admin|zhang3@qq.com|超级管理员 张四|admin|zhang4@qq.com|超级管理员 张五|admin|zhang5@qq.com|超级管理员 张六|admin|zhang6@qq.com|超级管理员 张七|admin|zhang7@qq.com|超级管理员 张八|admin|zhang8@qq.com|超级管理员 张九|admin|zhang9@qq.com|超级管理员 （2） 文本数据批量导入mysql数据库中，核心源码如下： def main(): from analysis.models import User f = open('./readme/files/name.txt',encoding='utf-8') for line in f: name,pwd,email,type = line.split('|') User.objects.create(username=name,userpass=pwd,useremail=email,usertype=type) f.close() 重新登录“用户信息管理后台”，查看结果如图2-17所示： ![](https://i.imgur.com/g99epdw.png) 图2-17 导入文本数据到数据库 源码请访问github（[https://github.com/BaiNingchao/NLPDome/blob/master/01chapter.zip] 参考文献 数据挖掘十大算法：https://wizardforcel.gitbooks.io/dm-algo-top10/content/apriori.html 中文维基百科：https://zh.wikipedia.org/wiki/%E5%85%88%E9%AA%8C%E7%AE%97%E6%B3%95 GitHub：https://github.com/BaiNingchao/MachineLearning-1 图书：《机器学习实战》 图书：《自然语言处理理论与实战》 完整代码下载 源码请进【机器学习和自然语言QQ群：436303759】文件下载： 作者声明 本文版权归作者所有，旨在技术交流使用。未经作者同意禁止转载，转载后需在文章页面明显位置给出原文连接，否则相关责任自行承担。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Django</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自然语言处理之数据预处理]]></title>
    <url>%2F2019%2F02%2F13%2F%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[摘要： 数据预处理的整个步骤流程在自然语言处理的工程中要比其在机器学习的工程中精简一些，最大的区别就在数据清洗和特征构造这两个至关重要的过程。在自然语言处理中特征构造是否良好，很大程度上取决于所构造的特征数据集的数据特性与文本内容语义吻合程度的高低。比如，文本情感分类和文本内容分类都属于分类范畴，但对于同一种算法（参数都调整到最优），在两个不同分类的业务下，得到的结果可能会相差很大。通过仔细分析，我们将不难发现造成这种差异的最根本原因就是构造出来的特征数据集的数据模式没有很好地契合文本的真实语义，这也是自然语言处理的最大难点所在。（本文原创，转载必须注明出处.） 无论在机器学习还是在自然语言处理的过程中，数据预处理都是举足轻重的关键步骤，它常决定着一个好模型的训练难易程度,甚至决定着一个项目的成败。依作者愚见，相比于机器学习，数据预处理对自然语言处理而言更为重要。为什么这样说？因为自然语言对真实信息进行了额外的外表包装，只有剥开这层包装，才能更容易地去挖掘到真实信息。在自然语言处理过程中，数据预处理过程大致包括四个步骤：数据清洗、分词与数据转换、特征提取与构造、特征降维。 数据清洗在自然语言处理的过程中，通常我们得到的数据并非完全“干净”的，因此就需要将语言信息中的干扰信息除去，这样才能更有效去挖掘出其中包含的重要信息。文本内容形式多种多样，常见的包括：TXT文本、HTML文本、XML文本、word文档、excel文档等。在机器学习过程中，数据清洗的内容步骤是相当复杂的；但对文本来说，清洗的目的却很明确简单，即排除非关键信息。我们只需要保留文本内容所阐述的文字信息即可，并同时尽可能减小这些信息对算法模型构建的影响。以HTML文本为例，HTML文本中有很多HTML标签，如&quot;body&quot;,&quot;title&quot;,&quot;p&quot;等，毫无疑问将这些标签作为有效信息来训练模型是不可取的，因为它们与文本内容所要表达的主题没有任何关联。那么清洗这些标签需要自己写很多代码吗？答案是：当然不用！因为在Java语言中，有Jsoup、HtmlParser、Apache tika、HtmlCleaner以及XPath等功能包，它们可以来帮助你完成对HTML文本的清洗工作。而在Python语言中，有BeautifulSoup、SGMLParser、HTMLParaer等功能包也能完成清洗工作。所以无论你是用Java语言，还是Python语言来进行你的自然语言处理工程，行业先驱早已给后辈们种下了无数的“乘凉树”。当然在实际情况中我们可能还会遇到一些特殊的情况，以至于这些功能包并不能很好地帮我们完成所有的清洗工作；但是不用担心，正则表达式会满足你的需求。 分词处理在对文本进行清洗之后，需要给文本分词，这样能更容易挖掘出文本内容中的一些特征。在上一章中我们提到了一些常用的分词工作包，如Stanford NLP分词、jieba分词、中科院NLPIR汉语分词等。这些分词工具的分词效果都很不错，具体使用起来也很简单。比如有这样一句话：“自然语言处理（NLP）是计算机科学，人工智能，语言学关注计算机和人类（自然）语言之间的相互作用的领域！”，采用jieba分词后的效果：“自然语言处理/n （/x NLP/eng ）/n 是/v 计算机科学/n ，/x 人工智能/n ，/x 语言学/n 关注/v 计算机/n 和/c 人类/n （/x 自然/d ）/ x 语言/n 之间/f 的/uj 相互作用/l 的/uj 领域/n ！/x”。在分词时，一些停词和标点符号通常是需要排除的，如”是”、”和”、“的”、“，”、“（”、“）”等。但在某些业务情况下，有些停词和标点符号却对业务的建模有一定的帮助，比如对微博内容的情感判断。 特征构造在做文本特征构造的时候，需要先了解”向量空间模型”（VSM：Vector Space Model）这一个概念。向量空间模型把文本内容的处理简化为向量空间中的向量运算，并且以空间上的相似度表达语义的相似度，直观易懂。自然语言处理中，几乎所有的特征构造方法都基于这个概念。 （1）词袋模型 在传统的词袋模型当中，对于每一个词采用one-hot稀疏编码的形式。假设目标语料中共有N个唯一确认的词，那么需要一个长度N的词典，词典的每一个位置表达了文本中出现的某一个词。在某一种特征表达下，比如词频、binary、tf-idf等，我们可以将任意词或者文本表达在一个N维的向量空间里。 （2）N-gram模型 N-gram是一种统计语言模型，其作用是根据前(n-1)个item来预测第n个item。N-gram被广泛地应用于语音识别、输入法、分词等任务，当n分别为1、2、3时，又分别称为一元语法（unigram）、二元语法（bigram）与三元语法（trigram）。现在一些学者用N-gram模型来构造分类任务的数据特征，作者本人也利用N-gram模型来构造过新闻突发事件判断的数据特征，其判断的效果还是相当不错。 特征降维与选择在数据特征构造完成后，通常需要对特征数据集进行特征降维和特征选择。二者的目标都是要使得数据集的特征维数减少，但二者又存在一定的区别。数据降维，一般说的是维数约简（Dimensionality reduction）。它的思路是：将原始高维特征空间里的点向一个低维空间投影，从而使新空间的维度低于原特征空间，从而达到减少维数的目的。在这个过程中，特征发生了根本性的变化，原始的特征消失了（虽然新的特征也保持了原特征的一些性质）。而特征选择，是从 n 个特征中选择 d (d&lt;n) 个出来，而其它的 n-d 个特征舍弃，因此新的特征只是原来特征的一个子集，没有被舍弃的 d 个特征没有发生任何变化。 特征降维 当特征数据集构造完成后，可能会出现特征矩阵过大，从而导致计算量大、训练时间长等一系列问题，因此降低特征矩阵维度也是必不可少的。机器学习中常见的特征降维方法：L1惩罚项的模型、主成分分析法（PCA）、线性判别分析（LDA）。PCA和LDA有很多的相似点，他们的共同原理是将原始样本映射到维度更低的样本空间中，PCA是一种无监督的降维方法，而LDA是一种有监督的降维方法。在自然语言处理中，常采用的方法就是主题模型。主题模型同时具备了降维和语义表达的效果，比如LSI、LDA、PLSA、HDP等统计主题模型。这些模型寻求文本在低维空间（不同主题上）的表达，在降低维度的同时，尽可能保留原有文本的语义信息，主题模型在处理中长度文本分类任务时特别有效。 特征选择 当特征数据集特征过多时，选择相对更有意义的特征来进行建模是很有必要的。去掉无关特征、保留相关特征的过程，换句话说，是从所有的特征中选择一个最好特征子集的过程。特征选择本质上可以认为是降维的过程。 1）Filter（过滤法）：按照发散性或者相关性对各个特征进行评分，根据设定阈值或者待选择阈值的个数来选择特征。如：方差选择法、相关系数法、卡方检验法、互信息法。方差选择法：使用方差选择法，先要计算各个特征的方差，然后根据阈值，选择方差大于阈值的特征。相关系数法：使用相关系数法，先要计算各个特征对目标值的相关系数以及相关系数的P值。卡方检验法：经典的卡方检验主要是检验定性自变量对定性因变量的相关性。假设自变量有N种取值，因变量有M种取值，考虑自变量等于i且因变量等于j时样本频数的观察值与期望的差距，从而构建统计量。互信息法： 经典的互信息也是评价定性自变量对定性因变量的相关性的。 2）Wrapper（包装法）：根据目标函数（通常是预测效果评分），每次选择若干特征或者排除若干特征。如：递归特征消除法。递归特征消除法：递归消除特征法使用一个基模型来进行多轮训练，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行下一轮训练。 3）Embedded（嵌入法）：先使用某些机器学习的算法模型进行训练，得到各个特征的权值系数，再根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。基于惩罚项的特征选择法：使用带惩罚项的基模型，除了筛选出特征外，同时也进行了降维。使用feature_selection库的SelectFromModel类结合带L1惩罚项的逻辑回归模型。如：基于惩罚项的特征选择法、基于树模型的特征选择法。 基于树模型的特征选择法：树模型中（随机森林、GBDT和Xgboost）也可用来作为基模型进行特征选择，使用feature_selection库的SelectFromModel类结合树模型。 4）深度学习方法：从深度学习模型中选择某一神经层的特征用来进行最终目标模型的训练。 参考文献 数据挖掘十大算法：https://wizardforcel.gitbooks.io/dm-algo-top10/content/apriori.html 中文维基百科：https://zh.wikipedia.org/wiki/%E5%85%88%E9%AA%8C%E7%AE%97%E6%B3%95 GitHub：https://github.com/BaiNingchao/MachineLearning-1 图书：《机器学习实战》 图书：《自然语言处理理论与实战》 完整代码下载 源码请进【机器学习和自然语言QQ群：436303759】文件下载： 作者声明 本文版权归作者所有，旨在技术交流使用。未经作者同意禁止转载，转载后需在文章页面明显位置给出原文连接，否则相关责任自行承担。]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>数据清洗</tag>
        <tag>分词处理</tag>
        <tag>特征构造</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自然语言处理之中文自动分词]]></title>
    <url>%2F2019%2F02%2F13%2F%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%B9%8B%E4%B8%AD%E6%96%87%E8%87%AA%E5%8A%A8%E5%88%86%E8%AF%8D%2F</url>
    <content type="text"><![CDATA[摘要：中文分词技术属于自然语言处理技术范畴，中文分词是其他中文信息处理的基础，搜索引擎只是中文分词的一个应用。诸如机器翻译（MT）、语音合成、自动分类、自动摘要、自动校对等等。本章首先介绍中文分词及其特点和难点，其次对常用的中文分词方法进行阐述；紧接着我们将介绍几个典型的中文分词工具，有兴趣的读者还可对文中所列出的其他工具自行深入研究。最后，本章将对结巴中文分词进行详细介绍，从原理到使用逐渐深入，力求让读者快速掌握其思想及原理。（本文原创，转载必须注明出处.） 中文分词简介中文分词中文分词指的是将一个汉字序列切分成一个一个单独的词。分词就是将连续的字序列按照一定的规范重新组合成词序列的过程。我们知道，在英文的行文中，单词之间是以空格作为自然分界符的，而中文只是句段能通过明显的分界符来简单划界，而词是没有一个形式上的分界符的。虽然英文也同样存在短语的划分问题，不过在词这一层上，中文比之英文要复杂得多、困难得多。 例如: 英文句子: I am a student. 中文意思: 我是一名学生。 由于英文的语言使用习惯，通过空格我们很容易拆分出单词；而中文字词接线模糊往往不容易区别哪些是“字”，哪些是“词”。这也是为什么我们想把中文的词语进行切分的原因。 中文分词的发展与英文为代表的印欧语系语言相比，中文由于继承自古代汉语的传统，词语之间常没有分隔。古代汉语中除了连绵词和人名地名等，词通常就是单个汉字，所以当时没有分词书写的必要。而现代汉语中双字或多字词逐渐增多，一个字已经不再等同于一个词了。 在中文里，“词”和“词组”边界模糊。现代汉语的基本表达单元虽然为“词”，且以双字或者多字词居多，但由于人们认识水平的不同，对词和短语的边界还很难去区分。 例如：“对随地吐痰者给予处罚”，“随地吐痰者”本身是一个词还是一个短语，不同的人会有不同的标准，同样的“海上”“酒厂”等等，即使是同一个人也可能做出不同判断，如果汉语真的要分词书写，必然会出现混乱，难度很大。 中文分词的方法其实不局限于中文应用，也被应用到英文处理。例如手写识别，英文单词之间的空格就不很清楚，中文分词方法可以反过来帮助判别英文单词的边界。 中文分词的用途中文分词是文本处理的基础，对于输入的一段中文，成功的进行中文分词，可以达到电脑自动识别语句含义的效果。中文分词技术属于自然语言处理技术范畴，目前在自然语言处理技术中，中文处理技术比西文处理技术要落后很大一截，而许多西文的处理方法中文却不能直接采用，就是因为中文必需有分词这道工序。中文分词是其他中文信息处理的基础，搜索引擎只是中文分词的一个应用。其他的比如机器翻译（MT）、语音合成、自动分类、自动摘要、自动校对等等，都需要用到分词。因为中文需要分词，可能会影响一些研究，但同时也为一些企业带来机会，因为国外的计算机处理技术要想进入中国市场，首先也是要解决中文分词问题。在中文研究方面，相比外国人来说，中国人有十分明显的优势。 中文分词对于搜索引擎来说，最重要的并不是找到所有结果，因为在上百亿的网页中找到所有结果没有太多的意义，没有人能看得完；相反，最重要的是把最相关的结果排在最前面，这也称为相关度排序。中文分词的准确与否，常常直接影响到对搜索结果的相关度排序。从定性分析来说，搜索引擎的分词算法不同，词库的不同都会影响页面的返回结果。 中文分词的特点和难点中文分词简而言之就是让计算机在词之间加上边界标记。当前研究所面临的问题和困难主要体现在三个方面：分词的规范、歧义词的切分和未登录词识别。 分词的规范中文因其自身语言特性的局限，字(词)的界限往往很模糊，关于字(词)的抽象定义和词边界的划定尚没有一个公认的、权威的标准。曾经有专家对母语是汉语者调查结果显示，对汉语文本中“词”的认同率仅有百分之七十左右。正是由于这种不同的主观分词差异，给汉语分词造成了极大的困难。尽管在1992年国家颁布了《信息处理用现代词汉语分词规范》，但是这种规范很容易受主观因素影响，在处理现实问题时也不免相形见绌。 歧义词切分中文中的歧义词是很普遍的，歧义词即同一个词有多种切分方式，该如何去处理这种问题。普遍认为中文歧义词有三种类型： 交集型切分歧义，汉语词如AJB类型，满足AJ和JB分别成词。如“大学生”一种切分方式“大学/生”，另一种切分方式“大/学生”。你很难去判定那种切分正确，即使是人工切分也只能依据上下文，类似的有“结合成”、“美国会”等等。 组合型切分歧义，汉语词如AB，满足A，B，AB分别成词。如“郭靖有武功高超的才能”中的“才能”，一种切分“郭靖/有/武功/高超/的/才能”，另一种切分“中国/什么/时候/才/能/达到/发达/国家/水平”显示是不同的切分方式。 混合型切分歧义，汉语词包含如上两种共存情况。如“郭靖说这把剑太重了”，其中“太重了”是交集型字段，“太重”是组合型字段。 未登录词（新词）识别未登录词又称新词。这类词通常指两个方面，其一是词库中没有收录的词，其二是训练语料没有出现过的词。未登录词主要体现在以下几种： 新出现的网络用词：如“屌丝”、“蓝牙”、“蓝瘦香菇”、“房姐”、“奥特”、“累觉不爱”等。 研究领域名称：特定领域和新出现领域的专有名词。如“苏丹红”、“禽流感”、“埃博拉”、“三聚氰胺”等。 其他专有名词：诸如城市名、公司企业、职称名、电影、书籍、专业术语、缩写词等。如“成都”、“阿里巴巴”、“毛主席”、“三少爷的剑”、“NLP”、“川大”等。 综述所述，处理汉语词边界、歧义词切分和未登录词切分问题比较复杂，其中未登录词的影响大大超过了歧义词的影响，所以如何处理未登录词是关键问题。 常见中文分词方法早在80年代就有中文分词的研究工作，曾有人提出“正向最大匹配法”、“逆向最大匹配法”、“双向扫描匹配法”、“逐词遍历法”等方法，共计多达16种之多。由于这些分词方法多是基于规则和词表的方法，随着统计方法的发展，不少学者提出很多关于统计模型的中文分词方法。关于规则的中文自动方法主要有以下几种： 基于字符串匹配的分词方法基本思想是基于词典匹配，将待分词的中文文本根据一定规则切分和调整，然后跟词典中的词语进行匹配，匹配成功则按照词典的词分词，匹配失败通过调整或者重新选择，如此反复循环即可。代表方法有基于正向最大匹配和基于逆向最大匹配及双向匹配法。 基于理解的分词方法基本思想是通过专家系统或者机器学习神经网络方法模拟人的理解能力。前者是通过专家对分词规则的逻辑推理并总结形成特征规则，不断迭代完善规则，其受到资源消耗大和算法复杂度高的制约。后者通过机器模拟人类理解的方式，虽可以取得不错的效果，但是依旧受训练时间长和过拟合等因素困扰。 基于统计的分词方法关于统计的中文分词方法的基本思想本文整理如下： 基于隐马尔可夫模型的中文分词方法。基本思想是通过文本作为观测序列去确定隐藏序列的过程。该方法采用Viterbi算法对新词识别效果不错，但具有生成式模型的缺点，需要计算联合概率，因此随着文本增大存在计算量大问题。 基于最大熵模型的中文分词方法。基本思想是学习概率模型时，在可能的概率分布模型中，认为熵最大的进行切分。该法可以避免生成模型的不足，但是存在偏移量问题。 基于条件随机场模型的中文分词方法。基本思想主要来源最大熵马尔可夫模型，主要关注的字跟上下文标记位置有关，进而通过解码找到词边界。因此需要大量训练语料，而训练和解码又非常耗时。 综上所述，关于词典和规则的方法其分词速度较快，但是在不同领域取得效果差异很大，还存在构造费时费力、算法复杂度高、移植性差等缺点。基于统计的中文分词，虽然其相较于规则的方法取得不错的效果，但也依然存在模型训练时间长、分词速度慢等问题。针对这些问题，本文提出基于隐马尔可夫统计模型和自定义词典结合的方法，其在分词速度、歧义分析、新词发现和准确率方面都取得不错效果。 典型中文分词工具Stanford NLP分词 Stanford NLP介绍 Stanford NLP是由斯坦福大学的 NLP 小组开源的 Java 实现的 NLP 工具包，同样对 NLP 领域的各个问题提供了解决办法。斯坦福大学的 NLP 小组是世界知名的研究小组，能将 NLTK 和 Stanford NLP 这两个工具包结合起来使用，那对于自然语言开发者是再好不过了。2004 年 Steve Bird 在 NLTK 中加上了对 Stanford NLP 工具包的支持，通过调用外部的 jar 文件来使用 Stanford NLP 工具包的功能这样一来就变得更为方便好用。 Python 调用Stanford NLP进行中文分词 (1) 安装配置说明 本文以Python 3.5.2和java version “1.8.0_111”版本进行配置，具体安装需要注意以下几点： 1 Stanford NLP 工具包需要 Java 8 及之后的版本，如果出错请检查 Java 版本 2 本文的配置都是以 Stanford NLP 3.6.0 为例，如果使用的是其他版本，请注意替换相应的文件名 3 本文的配置过程以 NLTK 3.2 为例，如果使用 NLTK 3.1，需要注意该旧版本中 StanfordSegmenter 未实现，其余大致相同 4 下面的配置过程是具体细节可以参照：http://nlp.stanford.edu/software/ (2) 下载必要工具包 只需要下载以下3个文件就够了，stanfordNLTK文件里面就是StanfordNLP工具包在NLTK中所依赖的jar包和相关文件. 1 stanfordNLTK(https://pan.baidu.com/s/1nvEYdfj) ：作者已将所有需要的包和相关文件打包在了一起，下面有具体讲解。 2 Jar1.8(http://pan.baidu.com/s/1miubwq0) ：如果你本机是Java 8以上版本，可以不用下载。 3 NLTK(https://pan.baidu.com/s/1pKA9XuN) ：这个工具包提供Standford NLP接口。 注意：以上文件下载后，Jar如果是1.8的版本可以不用下载，另外两个压缩包下载到本地，解压后拷贝文件夹到你的python安装主路径下，然后cmd进入NLTK下通过python setup.py install即可。后面操作讲路径是进行简单修改即可。（如果不能正常进行分词等操作，查看python是否是3.2以上版本，java是否是8以后版本，jar环境变量是否配置正确） (3) 对中文进行分词 StanfordSegmenter 中文分词：下载52nlp改过的NLTK包nltk-develop(https://pan.baidu.com/s/1misFxna)，解压后将其拷贝到你的python目录下，进去E:\Python\nltk-develop采用python 编辑器打开setup.py文件，F5运行，输入以下代码： >>> from nltk.tokenize.stanford_segmenter import StanfordSegmenter >>> segmenter = StanfordSegmenter( path_to_jar=r"E:\tools\stanfordNLTK\jar\stanford-segmenter.jar", path_to_slf4j=r"E:\tools\stanfordNLTK\jar\slf4j-api.jar", path_to_sihan_corpora_dict=r"E:\tools\stanfordNLTK\jar\data", path_to_model=r"E:\tools\stanfordNLTK\jar\data\pku.gz", path_to_dict=r"E:\tools\stanfordNLTK\jar\data\dict-chris6.ser.gz" ) >>> str="我在博客园开了一个博客，我的博客名叫伏草惟存，写了一些自然语言处理的文章。" >>> result = segmenter.segment(str) >>> result 程序解读：StanfordSegmenter 的初始化参数说明: path_to_jar: 用来定位jar包，本程序分词依赖stanford-segmenter.jar（注: 其他所有 Stanford NLP 接口都有 path_to_jar 这个参数。） path_to_slf4j: 用来定位slf4j-api.jar作用于分词 path_to_sihan_corpora_dict: 设定为 stanford-segmenter-2015-12-09.zip 解压后目录中的 data 目录， data 目录下有两个可用模型 pkg.gz 和 ctb.gz 需要注意的是，使用 StanfordSegmenter 进行中文分词后，其返回结果并不是 list ，而是一个字符串，各个汉语词汇在其中被空格分隔开。 (4) 分词结果 我 在 博客 园 开 了 一个 博客 ， 我 的 博客 名叫 伏 草 惟 存 ， 写 了 一些 自然 语言 处理 的 文章 。 HanLP中文分词 HanLP HanLP是由一系列模型与算法组成的Java工具包，目标是普及自然语言处理在生产环境中的应用。HanLP具备功能完善、性能高效、架构清晰、语料时新、可自定义等特点。 在提供丰富功能的同时，HanLP内部模块坚持低耦合、模型坚持惰性加载、服务坚持静态提供、词典坚持明文发布，使用起来非常方便，同时自带一些语料处理工具，帮助用户训练自己的语料。 Python调用HanLP进行中文分词 (1) 下载Hanlp的jar包hanlp.jar(http://hanlp.linrunsoft.com/services.html)(2) 安装配置jre1.7+，本文省略具体安装步骤(3) 在py文件启动JVM from jpype import * startJVM(getDefaultJVMPath(), "-Djava.class.path=C:\hanlp\hanlp-1.3.2.jar; C:\hanlp", "-Xms1g", "-Xmx1g") # 启动JVM，Linux需替换分号;为冒号: 此处是hanlp的具体调用方法 shutdownJVM() Python调用hanlp分词 默认分词 paraStr1='中国科学院计算技术研究所的宗成庆教授正在教授自然语言处理课程' print("="*30+"HanLP分词"+"="*30) HanLP = JClass('com.hankcs.hanlp.HanLP') print(HanLP.segment(paraStr1)) 运行结果 ==============================HanLP分词============================== [中国科学院计算技术研究所/nt, 的/ude1, 宗成庆/nr, 教授/nnt, 正在/d, 教授/nnt, 自然语言处理/nz, 课程/n] 标准分词 print("="*30+"标准分词"+"="*30) StandardTokenizer = JClass('com.hankcs.hanlp.tokenizer.StandardTokenizer') print(StandardTokenizer.segment(paraStr1)) 运行结果 ==============================标准分词============================== [中国科学院计算技术研究所/nt, 的/ude1, 宗成庆/nr, 教授/nnt, 正在/d, 教授/nnt, 自然语言处理/nz, 课程/n] NLP分词 print("="*30+"NLP分词"+"="*30) NLPTokenizer = JClass('com.hankcs.hanlp.tokenizer.NLPTokenizer') print(NLPTokenizer.segment(paraStr1)) 运行结果 ==============================NLP分词============================== [中国科学院计算技术研究所/nt, 的/ude1, 宗成庆/nr, 教授/nnt, 正在/d, 教授/v, 自然语言处理/nz, 课程/n] 索引分词 print("="*30+"索引分词"+"="*30) IndexTokenizer = JClass('com.hankcs.hanlp.tokenizer.IndexTokenizer') termList= IndexTokenizer.segment(paraStr1); for term in termList : print(str(term) + " [" + str(term.offset) + ":" + str(term.offset + len(term.word)) + "]") 运行结果 ==============================索引分词============================== 中国科学院计算技术研究所/nt [0:12] 中国/ns [0:2] 中国科学院/nt [0:5] 科学/n [2:4] 科学院/nis [2:5] 学院/nis [3:5] 计算/v [5:7] 技术/n [7:9] 研究/vn [9:11] 研究所/nis [9:12] 的/ude1 [12:13] 宗成庆/nr [13:16] 自然语言/gm [13:17] 自然语言处理/nz [13:19] 教授/nnt [16:18] 正在/d [18:20] 教授/nnt [20:22] 自然语言处理/nz [22:28] 自然/n [22:24] 语言/n [24:26] 处理/vn [26:28] 课程/n [28:30] 极速词典分词 print("="*30+" 极速词典分词"+"="*30) SpeedTokenizer = JClass('com.hankcs.hanlp.tokenizer.SpeedTokenizer') print(NLPTokenizer.segment(paraStr1)) 运行结果 ============================== 极速词典分词============================== [中国科学院计算技术研究所/nt, 的/ude1, 宗成庆/nr, 教授/nnt, 正在/d, 教授/v, 自然语言处理/nz, 课程/n] 自定义分词 paraStr2 = '攻城狮逆袭单身狗，迎娶白富美，走上人生巅峰' print("="*30+" 自定义分词"+"="*30) CustomDictionary = JClass('com.hankcs.hanlp.dictionary.CustomDictionary') CustomDictionary.add('攻城狮') CustomDictionary.add('单身狗') HanLP = JClass('com.hankcs.hanlp.HanLP') print(HanLP.segment(paraStr2)) 运行结果 ============================== 自定义分词============================== [攻城狮/nz, 逆袭/nz, 单身狗/nz, ，/w, 迎娶/v, 白富美/nr, ，/w, 走上/v, 人生/n, 巅峰/n] ## 其他中文分词工具 - BosonNLP：玻森实验室开发的一款分词工具 - 语言云：以哈工大社会计算与信息检索研究中心研发的 “语言技术平台（LTP）” 为基础，为用户提供高效精准的中文自然语言处理云服务 - NLPIR：中科院分词系统 - 新浪云 - 搜狗分词 - 结巴分词 - SCWS：简易中文分词系统缩写。SCWS 由 hightman 开发， 并以 BSD 许可协议开源发布，源码托管在 github - 腾讯文智 - 盘古分词 - IKAnalyzer：一个开源的，基于java语言开发的轻量级的中文分词工具包 关于中文分词工具可分为基于规则的分词方法和基于统计的分词方法。本节所给出的相关常见分词工具，读者感兴趣可以自行深入研究。特别说明的是，随着深度学习的快速发展，深度学习在中文分词的应用也越来越流行。由于开发过程中多数开发人员及其相关研究者使用结巴中文分词的比较多，所以本文将其单独立为一节进行深入学习。 # 结巴中文分词 ## 基于Python的结巴中文分词 > 结巴中文分词的特点 (1) 支持三种分词模式： - 精确模式，试图将句子最精确地切开，适合文本分析； - 全模式，把句子中所有的可以成词的词语都扫描出来, 速度非常快，但是不能解决歧义； - 搜索引擎模式，在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。 (2) 支持繁体分词 (3) 支持自定义词典 (4) MIT 授权协议 - 在线演示：http://jiebademo.ap01.aws.af.cm/ - 网站代码：https://github.com/fxsjy/jiebademo > 安装说明：代码对 Python 2/3 均兼容 - 全自动安装：easy_install jieba 或者 pip install jieba / pip3 install jieba - 半自动安装：先下载 http://pypi.python.org/pypi/jieba/解压后运行 python setup.py install - 手动安装：将 jieba 目录放置于当前目录或者 site-packages 目录 - 通过 import jieba 来引用 > 结巴分词工具下载 - hanllp jar包（http://download.csdn.net/download/lb521200200/9686915） - ik 分词 5.0.0版本jar包（http://download.csdn.net/download/youyao816/9676084） - ik分词 1.10.1版本jar包（http://download.csdn.net/download/youyao816/9676082） - IKAnalyzer所需的jar包（http://download.csdn.net/download/jingjingchen1014/9659225） - jieba分词包（http://download.csdn.net/download/u014018025/9652341） > 主要分词功能 - jieba.cut 方法接受三个输入参数: 需要分词的字符串；cut_all 参数用来控制是否采用全模式；HMM 参数用来控制是否使用 HMM 模型 - jieba.cut_for_search 方法接受两个参数：需要分词的字符串；是否使用 HMM 模型。该方法适合用于搜索引擎构建倒排索引的分词，粒度比较细 - 待分词的字符串可以是 unicode 或 UTF-8 字符串、GBK 字符串。注意：不建议直接输入 GBK 字符串，可能无法预料地错误解码成 UTF-8 - jieba.cut 以及 jieba.cut_for_search 返回的结构都是一个可迭代的 generator，可以使用 for 循环来获得分词后得到的每一个词语(unicode) - jieba.lcut 以及 jieba.lcut_for_search 直接返回 list - jieba.Tokenizer(dictionary=DEFAULT_DICT) 新建自定义分词器，可用于同时使用不同词典。jieba.dt 为默认分词器，所有全局分词相关函数都是该分词器的映射。 代码示例 import jieba seg_list = jieba.cut("我来到北京清华大学", cut_all=True) print("Full Mode: " + "/ ".join(seg_list)) # 全模式 seg_list = jieba.cut("我来到北京清华大学", cut_all=False) print("Default Mode: " + "/ ".join(seg_list)) # 精确模式 seg_list = jieba.cut("他来到了网易杭研大厦") # 默认是精确模式 print(", ".join(seg_list)) seg_list = jieba.cut_for_search("小明硕士毕业于中国科学院计算所，后在日本京都大学深造") # 搜索引擎模式 print(", ".join(seg_list)) 输出结果 【全模式】: 我/ 来到/ 北京/ 清华/ 清华大学/ 华大/ 大学 【精确模式】: 我/ 来到/ 北京/ 清华大学 【新词识别】：他, 来到, 了, 网易, 杭研, 大厦 (此处，“杭研”并没有在词典中，但是也被Viterbi算法识别出来了) 【搜索引擎模式】： 小明, 硕士, 毕业, 于, 中国, 科学, 学院, 科学院, 中国科学院, 计算, 计算所, 后, 在, 日本, 京都, 大学, 日本京都大学, 深造 添加自定义词典 载入词典 开发者可以指定自己自定义的词典，以便包含 jieba 词库里没有的词。虽然 jieba 有新词识别能力，但是自行添加新词可以保证更高的正确率 用法： jieba.load_userdict(file_name) # file_name 为文件类对象或自定义词典的路径 词典格式和 dict.txt 一样，一个词占一行；每一行分三部分：词语、词频（可省略）、词性（可省略），用空格隔开，顺序不可颠倒。file_name 若为路径或二进制方式打开的文件，则文件必须为 UTF-8 编码。 词频省略时使用自动计算的能保证分出该词的词频。 例如： 创新办 3 i 云计算 5 凱特琳 nz 台中 自定义分词前后对比 之前： 李小福 / 是 / 创新 / 办 / 主任 / 也 / 是 / 云 / 计算 / 方面 / 的 / 专家 / 加载自定义词库后： 李小福 / 是 / 创新办 / 主任 / 也 / 是 / 云计算 / 方面 / 的 / 专家 / 结巴分词工具详解 结巴分词的算法策略 基于前缀词典实现高效的词图扫描，生成句子中汉字所有可能成词情况所构成的有向无环图 (DAG) 采用了动态规划查找最大概率路径, 找出基于词频的最大切分组合 对于未登录词，采用了基于汉字成词能力的 HMM 模型，使用了 Viterbi 算法 结巴源码组织形式 jieba |-- Changelog |-- extra_dict | |-- dict.txt.big | |-- dict.txt.small | |-- idf.txt.big | `-- stop_words.txt |-- jieba | |-- analyse | | |-- analyzer.py | | |-- idf.txt | | |-- __init__.py | | |-- textrank.py | | `-- tfidf.py | |-- _compat.py | |-- dict.txt | |-- finalseg | | |-- __init__.py | | |-- prob_emit.p | | |-- prob_emit.py | | |-- prob_start.p | | |-- prob_start.py | | |-- prob_trans.p | | `-- prob_trans.py | |-- __init__.py | |-- __main__.py | `-- posseg | |-- char_state_tab.p | |-- char_state_tab.py | |-- __init__.py | |-- prob_emit.p | |-- prob_emit.py | |-- prob_start.p | |-- prob_start.py | |-- prob_trans.p | |-- prob_trans.py | `-- viterbi.py |-- LICENSE |-- setup.py `-- test |-- *.py |-- parallel | |-- extract_tags.py | `-- test*.py `-- userdict.txt 算法实现分词 （1） 基于前缀词典实现高效的词图扫描，生成句子中汉字所有可能成词情况所构成的有向无环图 (DAG); 作者这个版本中使用前缀字典实现了词库的存储(即dict.txt文件中的内容)，而弃用之前版本的trie树存储词库，想想也是，python中实现的trie树是基于dict类型的数据结构而且dict中又嵌套dict 类型，这样嵌套很深，导致内存耗费严重，详情见作者把trie树改成前缀词典的缘由, 具体实现见 gen_pfdict(self, f_name)。接着说DAG有向无环图, 生成句子中汉字所有可能成词情况所构成的有向无环图。DAG根据我们生成的前缀字典来构造一个这样的DAG，对一个sentence DAG是以{key:list[i,j…], …}的字典结构存储，其中key是词的在sentence中的位置，list存放的是在sentence中以key开始且词sentence[key:i+1]在我们的前缀词典中以key开始i结尾的词的末位置i的列表，即list存放的是sentence中以位置key开始的可能词语的结束位置，这样通过查字典得到词, 开始位置+结束位置列表。 例如:句子“抗日战争”生成的DAG中{0:[0,1,3]} 这样一个简单的DAG, 就是表示0位置开始, 在0,1,3位置都是词, 就是说0~0,0~1,0~3 即 “抗”，“抗日”，“抗日战争”这三个词 在dict.txt中是词。 （2）采用了动态规划查找最大概率路径, 找出基于词频的最大切分组合;基于上面的DAG利用动态规划查找最大概率路径，这个理解DP算法的很容易就能明白了。根据动态规划查找最大概率路径的基本思路就是对句子从右往左反向计算最大概率。依次类推, 最后得到最大概率路径, 得到最大概率的切分组合（这里满足最优子结构性质，可以利用反证法进行证明），这里代码实现中有个小trick，即概率对数(可以让概率相乘的计算变成对数相加,防止相乘造成下溢，因为在语料、词库中每个词的出现概率平均下来还是很小的浮点数)。 （3）对于未登录词，采用了基于汉字成词能力的 HMM 模型，使用了 Viterbi 算法；未登录词(即jieba中文分词源码分析（一）)中说的OOV， 其实就是词典 dict.txt 中没有记录的词。这里采用了HMM模型,HMM是个简单强大的模型，可以参考这个网络资源进行学习，HMM在实际应用中主要用来解决3类问题: 评估问题(概率计算问题) ：即给定观测序列 O=O1,O2,O3…Ot和模型参数λ=(A,B,π)，怎样有效计算这一观测序列出现的概率. (Forward-backward算法) 解码问题(预测问题) ：即给定观测序列 O=O1,O2,O3…Ot和模型参数λ=(A,B,π)，怎样寻找满足这种观察序列意义上最优的隐含状态序列S。 (viterbi算法,近似算法) 学习问题 ：即HMM的模型参数λ=(A,B,π)未知，如何求出这3个参数以使观测序列O=O1,O2,O3…Ot的概率尽可能的大. (即用极大似然估计的方法估计参数,Baum-Welch,EM算法) 模型的关键相应参数λ=(A,B,π)，经过作者对大量语料的训练, 得到了finalseg目录下的三个文件（初始化状态概率（π）即词语以某种状态开头的概率，其实只有两种，要么是B，要么是S。这个就是起始向量, 就是HMM系统的最初模型状态，对应文件prob start.py；隐含状态概率转移矩A 即字的几种位置状态(BEMS四个状态来标记, B是开始begin位置, E是end, 是结束位置, M是middle, 是中间位置, S是single, 单独成词的位置)的转换概率，对应文件prob trans.py；观测状态发射概率矩阵B 即位置状态到单字的发射概率，比如P(“狗”|M)表示一个词的中间出现”狗”这个字的概率，对应文件prob_emit.py）。 结巴分词核心内容 结巴分词的算法策略 作者在这个版本(0.37)中使用前缀字典实现了词库的存储(即dict.txt文件中的内容)，而弃用之前版本的trie树存储词库，Python中实现的trie树是基于dict类型的数据结构而且dict中又嵌套dict 类型，这样嵌套很深，导致内存耗费严重，具体点这里，下面是@gumblex commit的内容: 对于get_DAG()函数来说，用Trie数据结构，特别是在Python环境，内存使用量过大。经实验，可构造一个前缀集合解决问题。 该集合储存词语及其前缀，如set([‘数’, ‘数据’, ‘数据结’, ‘数据结构’])。 在句子中按字正向查找词语，在前缀列表中就继续查找，直到不在前缀列表中或超出句子范围。大约比原词库增加40%词条。 该版本通过各项测试，与原版本分词结果相同。测试：一本5.7M的小说，用默认字典，64位Ubuntu，Python 2.7.6。 Trie：第一次加载2.8秒，缓存加载1.1秒；内存277.4MB，平均速率724kB/s 前缀字典：第一次加载2.1秒，缓存加载0.4秒；内存99.0MB，平均速率781kB/s 此方法解决纯Python中Trie空间效率低下的问题。 jieba0.37版本中实际使用是前缀字典具体实现(对应代码中Tokenizer.FREQ字典)，即就是利用python中的dict把dict.txt中出现的词作为key，出现频次作为value，比如sentece : “北京大学”,处理后的结果为：{u’北’:17860, u’北京’ :34488,u’北京大’: 0,u’北京大学’: 2053}，具体详情见代码：def gen_pfdict(self, f_name): > DAG DAG根据我们生成的前缀字典来构造一个这样的DAG，对sentence DAG是以{key:list[i,j…], …}的字典结构存储，其中key是词的在sentence中的位置，list存放的是在sentence中以key开始且词sentence[key:i+1]在我们的前缀词典中 的以key开始i结尾的词的末位置i的列表，即list存放的是sentence中以位置key开始的可能的词语的结束位置，这样通过查字典得到词, 开始位置+结束位置列表。 例如句子”去北京大学玩“对应的DAG为： {0 : [0], 1 : [1, 2, 4], 2 : [2], 3 : [3, 4], 4 : [4], 5 : [5]} 例如DAG中{0:[0]} 这样一个简单的DAG, 就是表示0位置对应的是词, 就是说0~0,即”去”这个词 在dict.txt中是词条。DAG中{1:[1,2,4]}, 就是表示1位置开始, 在1,2,4位置都是词, 就是说1~1,1~2,1~4 即 “北”，“北京”，“北京大学”这三个词 在dict.txt对应文件的词库中。 > 基于词频最大切分组合 通过上面两小节可以得知，我们已经有了词库(dict.txt)的前缀字典和待分词句子sentence的DAG，基于词频的最大切分 要在所有的路径中找出一条概率得分最大的路径，该怎么做呢？ jieba中的思路就是使用动态规划方法，从后往前遍历，选择一个频度得分最大的一个切分组合。具体实现见代码，已给详细注释。 #动态规划，计算最大概率的切分组合 def calc(self, sentence, DAG, route): N = len(sentence) route[N] = (0, 0) # 对概率值取对数之后的结果 logtotal = log(self.total) # 从后往前遍历句子 反向计算最大概率 for idx in xrange(N - 1, -1, -1): # [x+1][0]即表示取句子x+1位置对应元组(概率对数，词语末字位置)的概率对数 route[idx] = max((log(self.FREQ.get(sentence[idx:x + 1]) or 1) - logtotal + route[x + 1][0], x) for x in DAG[idx]) 从代码中可以看出calc是一个自底向上的动态规划(重叠子问题、最优子结构)，它从sentence的最后一个字(N-1)开始倒序遍历sentence的字(idx)的方式，计算子句sentence[isdx~N-1]概率对数得分（这里利用DAG及历史计算结果route实现，同时使用概率对数以有效防止下溢问题）。然后将概率对数得分最高的情况以（概率对数，词语最后一个字的位置）这样的tuple保存在route中。 根据上面的结束写了如下的测试：输出结果为： “去北京大学玩”的前缀字典: 去 123402 去北 0 去北京 0 去北京大 0 去北京大学 0 去北京大学玩 0 “去北京大学玩”的DAG: 0 : [0] 1 : [1, 2, 4] 2 : [2] 3 : [3, 4] 4 : [4] 5 : [5] route: {0: (-26.039894284878688, 0), 1: (-19.851543754900984, 4), 2: (-26.6931716802707, 2), 3: (-17.573864399983357, 4), 4: (-17.709674112779485, 4), 5: (-9.567048044164698, 5), 6: (0, 0)} 去/北京大学/玩 中文分词的未登录词 因此可以看到，未登录词是分词中的一个重要问题，jieba分词中对于OOV的解决方法是：采用了基于汉字成词能力的 HMM 模型，使用了 Viterbi 算法。 分词规范，词的定义还不明确 (《统计自然语言处理》宗成庆) 歧义切分问题，交集型切分问题，多义组合型切分歧义等。结婚的和尚未结婚的 =&gt; 结婚／的／和／尚未／结婚／的 结婚／的／和尚／未／结婚／的。 未登录词问题 有两种解释：一是已有的词表中没有收录的词，二是已有的训练语料中未曾出现过的词，第二种含义中未登录词又称OOV(Out of Vocabulary)。对于大规模真实文本来说，未登录词对于分词的精度的影响远超歧义切分。一些网络新词，自造词一般都属于这些词。 因此可以看到，未登录词是分词中的一个重要问题，jieba分词中对于OOV的解决方法是：采用了基于汉字成词能力的 HMM 模型，使用了 Viterbi 算法。 结巴分词基本用法 安装结巴分词 全自动安装：easy_install jieba 或者 pip install jieba / pip3 install jieba 半自动安装：先下载 http://pypi.python.org/pypi/jieba/ ，解压后运行 python setup.py install 手动安装：将 jieba 目录放置于当前目录或者 site-packages 目录 通过 import jieba 来引用 本机是win10 64位，已经安装了pip工具，关于pip下载安装（here），然后win+R，输入pip install jieba，效果如下： ![](https://i.imgur.com/5HskWFK.jpg) 图9-1 结巴分词安装成功图 结巴几种模式下的分词操作：（以下默认已导入：import jieba） 全模式分词 >>> import jieba >>> str="我是白宁超来自博客园" >>> seg_list=jieba.cut(str,cut_all=True) >>> print("Full Mode: " + "/ ".join(seg_list)) # 全模式 Full Mode: 我/ 是/ 白/ 宁/ 超/ 来自/ 博客/ 博客园 结果分析 显然我的名字：白宁超，没有正确分词，这是因为全模式把句子中所有可以成词的词语都扫描出来, 速度非常快，但是不能解决歧义。 精确模式分词 >>> seg_list=jieba.cut(str,cut_all=False) >>> print("Default Mode: " + "/ ".join(seg_list)) # 精确模式 Default Mode: 我/ 是/ 白宁超/ 来自/ 博客园 >>> seg_list=jieba.cut(str) >>> print("Default Mode: " + "/ ".join(seg_list)) # 默认模式 Default Mode: 我/ 是/ 白宁超/ 来自/ 博客园 结果分析 首先默认模式就是精确模式，即cut_all=False。这里很好的将“白宁超”划分为一个词。与全模式分词是有区别的。精确模式适合文本分析。 默认精确模式分词 >>> seg_list = jieba.cut("他来到了网易杭研大厦") # 默认是精确模式 >>> print("【新词发现】\t"+", ".join(seg_list)) 【新词发现】 他, 来到, 了, 网易, 杭研, 大厦 结果分析 此处杭研并没有在词典中，但是也被Viterbi算法识别出来了。实际上是基于汉字成词能力的 HMM 模型，使用了 Viterbi 算法可以发现新词。当然也可以到自定义字典中去收集新词。 搜索引擎模式分词 >>> seg_list = jieba.cut_for_search("小明硕士毕业于中国科学院计算所，后在日本京都大学深造") # 搜索引擎模式 >>> print("搜索引擎模式：\t"+", ".join(seg_list)) 搜索引擎模式： 小明, 硕士, 毕业, 于, 中国, 科学, 学院, 科学院, 中国科学院, 计算, 计算所, ，, 后, 在, 日本, 京都, 大学, 日本京都大学, 深造 结果分析 在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。 繁体分词 >>> str='''此開卷第一回也．作者自云：因曾歷過一番夢幻之后，故將真事隱去， 而借"通靈"之說，撰此《石頭記》一書也．故曰"甄士隱"云云．但書中所記 何事何人？自又云：“今風塵碌碌，一事無成，忽念及當日所有之女子，一 一細考較去，覺其行止見識，皆出于我之上．何我堂堂須眉，誠不若彼裙釵 哉？實愧則有余，悔又無益之大無可如何之日也！''' >>> str=jieba.cut(str) >>> print('/ '.join(str)) 此開卷/ 第一回/ 也/ ．/ 作者/ 自云/ ：/ 因曾/ 歷過/ 一番/ 夢/ 幻之后/ ，/ 故將/ 真事/ 隱去/ ，/ / 而/ 借/ "/ 通靈/ "/ 之/ 說/ ，/ 撰此/ 《/ 石頭記/ 》/ 一書/ 也/ ．/ 故/ 曰/ "/ 甄士/ 隱/ "/ 云云/ ．/ 但書中/ 所記/ / 何事何/ 人/ ？/ 自又云/ ：/ “/ 今風/ 塵碌碌/ ，/ 一事/ 無成/ ，/ 忽念及/ 當日/ 所/ 有/ 之/ 女子/ ，/ 一/ / 一細/ 考較/ 去/ ，/ 覺其/ 行止/ 見識/ ，/ 皆/ 出于/ 我/ 之/ 上/ ．/ 何/ 我堂/ 堂須/ 眉/ ，/ 誠不若/ 彼/ 裙釵/ / 哉/ ？/ 實愧則/ 有/ 余/ ，/ 悔/ 又/ 無益/ 之/ 大/ 無/ 可/ 如何/ 之/ 日/ 也/ ！ >>> 自定义分词器 #encoding=utf-8 from __future__ import print_function, unicode_literals import sys sys.path.append("../") import jieba jieba.load_userdict("userdict.txt") import jieba.posseg as pseg jieba.add_word('凱特琳') jieba.del_word('自定义词') test_sent = ( "李小福和李铁军是创新办主任也是云计算方面的专家; 什么是八一双鹿\n" "例如我输入一个带“韩玉赏鉴”的标题，在自定义词库中也增加了此词为N类\n" "「台中」正確應該不會被切開。mac上可分出「石墨烯」；此時又可以分出來凱特琳了。" ) words = jieba.cut(test_sent) print('/'.join(words)) print("="*40) result = pseg.cut(test_sent) for w in result: print(w.word, "/", w.flag, ", ", end=' ') print("\n" + "="*40) terms = jieba.cut('easy_install is great') print('/'.join(terms)) terms = jieba.cut('python 的正则表达式是好用的') print('/'.join(terms)) print("="*40) # test frequency tune testlist = [ ('今天天气不错', ('今天', '天气')), ('如果放到post中将出错。', ('中', '将')), ('我们中出了一个叛徒', ('中', '出')), ] for sent, seg in testlist: print('/'.join(jieba.cut(sent, HMM=False))) word = ''.join(seg) print('%s Before: %s, After: %s' % (word, jieba.get_FREQ(word), jieba.suggest_freq(seg, True))) print('/'.join(jieba.cut(sent, HMM=False))) print("-"*40 结果分析 首先对一段话分词处理,此处“李小福“和“李铁军”都是人名，结果却分词“李小福”和“李铁”，而“军是”当做一个词处理，显然不对。 我们可以将“李铁军”当着一个词加入自定义文本中.处理后结果显然经过自定义分词有所好转。而石墨/烯分词错误。 词性标注 print("="*40) result = pseg.cut(test_sent) for w in result: print(w.word, "/", w.flag, ", ", end=' ') print("\n" + "="*40) terms = jieba.cut('easy_install is great') print('/'.join(terms)) terms = jieba.cut('python 的正则表达式是好用的') print('/'.join(terms)) print("="*40) # 结果 ======================================== 李小福 / nr , 和 / c , 李铁军 / x , 是 / v , 创新办 / i , 主任 / b , 也 / d , 是 / v , 云计算 / x , 方面 / n , 的 / uj , 专家 / n , ; / x , / x , 什么 / r , 是 / v , 八一双鹿 / nz , / x , 例如 / v , 我 / r , 输入 / v , 一个 / m , 带 / v , “ / x , 韩玉赏鉴 / nz , ” / x , 的 / uj , 标题 / n , ， / x , 在 / p , 自定义词 / n , 库中 / nrt , 也 / d , 增加 / v , 了 / ul , 此 / r , 词 / n , 为 / p , N / eng , 类 / q , / x , 「 / x , 台中 / s , 」 / x , 正確 / ad , 應該 / v , 不 / d , 會 / v , 被 / p , 切開 / ad , 。 / x , mac / eng , 上 / f , 可 / v , 分出 / v , 「 / x , 石墨烯 / x , 」 / x , ； / x , 此時 / c , 又 / d , 可以 / c , 分出 / v , 來 / zg , 凱特琳 / x , 了 / ul , 。 / x , ======================================== easy_install/ /is/ /great python/ /的/正则表达式/是/好用/的 ======================================== 结果分析:李小福 / nr , 李铁军 / x 都是名字，属于名词，而李铁军 / x显然词性不对，这是由于刚刚jieba.addword(‘李铁军’)时候，没有进行词性参数输入，我们看看jieba.add word(‘李铁军’)源码： def add_word(self, word, freq=None, tag=None) jieba.add_word('李铁军',tag='nr')修改后结果 再次查看结果： 李小福 / nr , 和 / c , 李铁军 / nr , 是 / v , 创新办 / i , 主任 / b , 也 / d , 是 / v , 云计算 / x , 方面 / n ,的 / uj , 专家 / n , ; / x , / x , 什么 / r , 是 / v , 八一双鹿 / nz , / x , 例如 / v , 我 / r , 输入 / v ,一个 / m , 带 / v , “ / x , 韩玉赏鉴 / nz , ” / x , 的 / uj , 标题 / n , ， / x , 在 / p , 自定义词 / n ,库中 / nrt , 也 / d , 增加 / v , 了 / ul , 此 / r , 词 / n , 为 / p , N / eng , 类 / q , / x , 「 / x ,台中 / s , 」 / x , 正確 / ad , 應該 / v , 不 / d , 會 / v , 被 / p , 切開 / ad , 。 / x , mac / eng ,上 / f , 可 / v , 分出 / v , 「 / x , 石墨烯 / x , 」 / x , ； / x , 此時 / c , 又 / d , 可以 / c , 分出 / v , 來 / zg , 凱特琳 / x , 了 / ul , 。 / x , &lt;/pre&gt; 自定义调整词典 # test frequency tune testlist = [ ('今天天气不错', ('今天', '天气')), ('如果放到post中将出错。', ('中', '将')), ('我们中出了一个叛徒', ('一', '个')), ] for sent, seg in testlist: print('/'.join(jieba.cut(sent, HMM=False))) word = ''.join(seg) print('%s Before: %s, After: %s' % (word, jieba.get_FREQ(word), jieba.suggest_freq(seg, True))) print('/'.join(jieba.cut(sent, HMM=False))) print("-"*40) 结果： ======================================== 今天天气/不错 今天天气 Before: 3, After: 0 今天天气/不错 ---------------------------------------- 如果/放到/post/中将/出错/。 中将 Before: 763, After: 494 如果/放到/post/中/将/出错/。 ---------------------------------------- 我们/中/出/了/一个/叛徒 一个 Before: 142747, After: 454 我们/中/出/了/一/个/叛徒 ---------------------------------------- 结果分析：列表中的每一条数据如(‘今天天气不错’, (‘今天’, ‘天气’)),其中(‘今天’, ‘天气’)调整分词颗粒精度的。如第三句正常分词：我们/中/出/了/一个/叛徒。我们假设某些情况下一和个分别分词，可以做如上处理。 使用 add word(word, freq=None, tag=None) 和 del word(word) 可在程序中动态修改词典。使用 suggest_freq(segment, tune=True) 可调节单个词语的词频，使其能（或不能）被分出来。注意：自动计算的词频在使用 HMM 新词发现功能时可能无效。 自定义调节词典解决歧义分词问题 >>> import jieba >>> print('/'.join(jieba.cut('如果放到post中将出错。', HMM=False))) Building prefix dict from the default dictionary ... Loading model from cache C:\Users\cuitbnc\AppData\Local\Temp\jieba.cache Loading model cost 1.069 seconds. Prefix dict has been built succesfully. 如果/放到/post/中将/出错/。 >>> jieba.suggest_freq(('中', '将'), True) 494 >>> print('/'.join(jieba.cut('如果放到post中将出错。', HMM=False))) 如果/放到/post/中/将/出错/。 >>> print('/'.join(jieba.cut('「台中」正确应该不会被切开', HMM=False))) 「/台/中/」/正确/应该/不会/被/切开 >>> jieba.suggest_freq('台中', True) 69 >>> print('/'.join(jieba.cut('「台中」正确应该不会被切开', HMM=False))) 「/台中/」/正确/应该/不会/被/切开 词性标注 jieba.posseg.POSTokenizer(tokenizer=None) 新建自定义分词器，tokenizer 参数可指定内部使用的jieba.Tokenizer 分词器。jieba.posseg.dt 为默认词性标注分词器。标注句子分词后每个词的词性，采用和 ictclas 兼容的标记法。用法示例如下： >>> import jieba.posseg as pseg >>> words = pseg.cut("我爱北京天安门") >>> for word, flag in words: ... print('%s %s' % (word, flag)) ... 我 r 爱 v 北京 ns 天安门 ns 参考文献 数据挖掘十大算法：https://wizardforcel.gitbooks.io/dm-algo-top10/content/apriori.html 中文维基百科：https://zh.wikipedia.org/wiki/%E5%85%88%E9%AA%8C%E7%AE%97%E6%B3%95 GitHub：https://github.com/BaiNingchao/MachineLearning-1 图书：《机器学习实战》 图书：《自然语言处理理论与实战》 完整代码下载 源码请进【机器学习和自然语言QQ群：436303759】文件下载： 作者声明 本文版权归作者所有，旨在技术交流使用。未经作者同意禁止转载，转载后需在文章页面明显位置给出原文连接，否则相关责任自行承担。]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>中文分词</tag>
        <tag>jieba</tag>
        <tag>hanlp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自然语言处理之语料库技术]]></title>
    <url>%2F2019%2F02%2F13%2F%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%B9%8B%E8%AF%AD%E6%96%99%E5%BA%93%E6%8A%80%E6%9C%AF%2F</url>
    <content type="text"><![CDATA[摘要：大数据发展的基石就是数据量的快速增加，无论是自然语言处理、数据挖掘、文本处理还是机器学习领域，都是在此基础上通过规则或统计方法进行模型构建的。但是不是数据足够大就叫大数据了呢？是不是数据足够多就构成语料库了呢？带着这些疑问，本章将带你走进语料库的世界，对语料知识进行一次全面而深入的了解。（本文原创，转载必须注明出处.） 语料库浅谈自然语言自然语言（英语：Natural language）通常是指一种自然地随文化演化的语言（如英语、汉语、法语、日语等）。人类使用的语言都会被视为“自然”语言，以相对于如编程语言等为计算机而设的“人造”语言。这一种用法可见于自然语言处理一词中。自然语言是人类交流和思维的主要工具。 语料和语料库语料，通常在统计自然语言处理中实际上不可能观测到大规模的语言实例。所以人们简单地用文本作为替代，并把文本中的上下文关系作为现实世界中语言的上下文关系的替代品。 语料库，语料库一词在语言学上意指大量的文本，通常经过整理，具有既定格式与标记。其具备三个显著的特点： 语料库中存放的是在语言的实际使用中真实出现过的语言材料。 语料库是以电子计算机为载体承载语言知识的基础资源，但并不等于语言知识。 真实语料需要经过加工（分析和处理），才能成为有用的资源。 语料库语言学大多数学者普遍认为： “语言学的研究必须基于语言事实的基础，必须详尽的大量的占有材料，才有可能在理论上得出比较可靠的结论”。传统语言材料的搜集整理和加工完全以手工进行，费时费力，直到计算机出现并随之计算能力强大之后，原先手工的工作开始转向计算机去做，后来逐渐的方法完善中，提出一些初步的理论，形成了语料学这样一门语言学与计算机科学交叉的学科。 语料库语言学的研究范畴：主要研究机器可读自然语言文本的采集、存储、检索、统计、语法标注、句法语义分析，以及具有上述功能的语料库在语言教学、语言定量分析、词汇研究、词语搭配研究、词典编制、语法研究、语言文化研究、法律语言研究、作品风格分析、自然语言理解、机器翻译等方面的应用。 建立语料库的意义语料库是为一个或者多个应用目标而专门收集的，有一定结构的、有代表的、可被计算机程序检索的、具有一定规模的语料集合。本质上讲，语料库实际上是通过对自然语言运用的随机抽样，以一定大小的语言样本来代表某一研究中所确定的语言运用的总体。 语料库深入语料库划分与种类语料库的划分一直是标准各异，其中冯志伟教授的语料库划分比较有影响力且在学术界认可度较高。其划分类型如下： 按语料选取的时间划分，可分为历时语料库（ diachronic corpus）和共时语料库（synchronic corpus）。 按语料的加工深度划分，可分为标注语料库（ annotated corpus）和非标注语料库（non-annotated corpus）。 按语料库的结构划分，可分为平衡结构语料库（ balance structure corpus）和自然随机结构的语料库（random structure corpus）。 按语料库的用途划分，可分为通用语料库（general corpus）和专用语料库（specialized corpus）。 按语料库的表达形式划分，可分为口语语料库（ spoken corpus）和文本语料库（text corpus）。 按语料库中语料的语种划分，可分为单语种语料库（monolingual corpora）和多语种语料库（multilingual corpora）。多语种语料库又可以再分为比较语料库（comparable corpora）和平行语料库（ parallel corpora）。比较语料库的目的侧重于特定语言现象的对比，而平行语料库的目的侧重于获取对应的翻译实例。 按语料库的动态更新程度划分，可分为参考语料库（ reference corpus）和监控语料库（monitor corpus）。参考语料库原则上不作动态更新，而监控语料库则需要不断地进行动态更新。 语料库构建原则语料库应该具有代表性、结构性、平衡性、规模需求并制定语料的元数据规范，各个原则具体介绍如下： 代表性：在应用领域中，不是根据量而划分是否是语料库，而是在一定的抽样框架范围内采集而来的，并且能在特定的抽样框架内做到代表性和普遍性。 结构性：有目的地收集语料的集合，必须以电子形式存在，计算机可读的语料集合结构性体现在语料库中语料记录的代码，元数据项、数据类型、数据宽度、取值范围、完整性约束。 平衡性：主要体现在平缓因子：学科、年代、文体、地域、登载语料的媒体、使用者的年龄、性别、文化背景、阅历、预料用途（私信/广告等），根据实际情况选择其中一个或者几个重要的指标作为平衡因子，最常见的平衡因子有学科、年代、文体、地域等。 规模性：大规模的语料对语言研究特别是对自然语言研究处理很有用，但是随着语料库的增大，垃圾语料越来越多，语料达到一定规模以后，语料库功能不能随之增长，语料库规模应根据实际情况而定。 元数据：元数据对于研究语料库有着重要的意义，我们可以通过元数据了解语料的时间、地域、作者、文本信息等；还可以构建不同的子语料库；除此之外，还可以对不同的子语料对比；另外还可以记录语料知识版权、加工信息、管理信息等。 注意：汉语词与词之间没有空隙，不便于计算机处理，一般需要进行切词和词性标注。 语料标注的优缺点 优点： 研究方便。可重用、功能多样、分析清晰。 缺点： 语料不客观（手工标注准确率高而一致性差，自动或者半自动标注一致性高而准确率差）、标注不一致、准确率低。 自然语言处理工具包：NLTKNLTK简介 NLTK NLTK（Natural language Toolkit）：自然语言工具包，Python编程语言实现的统计自然语言处理工具。它是由宾夕法尼亚大学计算机和信息科学的史蒂芬-伯德和爱德华·洛珀编写的。 NLTK支持NLP研究和教学相关的领域，其收集的大量公开数据集、模型上提供了全面易用的接口，涵盖了分词、词性标注(Part-Of-Speech tag, POS-tag)、命名实体识别(Named Entity Recognition, NER)、句法分析(Syntactic Parse)等各项 NLP 领域的功能。广泛应用在经验语言学，认知科学，人工智能，信息检索和机器学习。 在25个国家中已有 32所大学将NLTK作为教学工具。 NLTK模块及功能如表8-1所示： 表8-1 NLTK模块功能 任务 模块 描述 获取语料库 nltk.corpus 语料库和词典的标准化接口 字符串处理 nltk.tokenize，nltk.stem 分词、句子分解和提取主干（不支持中文） 搭配研究 nltk.collocations t检验、卡方检验和互信息 词性标注 nltk.tag n-gram、backoff和HMM 分类 nltk.classify、nltk.cluster 决策树、最大熵、朴素贝叶斯、EM和K-means 分块 nltk.chunk 正则表达式、n-gram和命名实体 解析 nltk.parse 图标、基于特征、一致性和概率性 语义解释 nltk.sem、nltk.inference 演算、模型检验 指标评测 nltk.metrics 准确率、召回率和协议系数 概率与估计 nltk.probability 频率分布和平滑概率分布 应用 nltk.app、nltk.chat 图形化关键字排序、分析器，wordNet查看器 语言学领域工作 nltk.toolbox 处理SIL工具箱格式的数据 ## NLTK安装 > NLTK 安装 (1) 查看python版本，如图8-1所示 ![](https://i.imgur.com/2XNlMtE.png) 图8-1 Python版本 (2) windows系统下载NLTK如下文件nltk-3.2.1.win32.exe（https://pan.baidu.com/s/1qYzXFPy），并执行exe文件，会自动匹配到python安装路径，如果没有找到路径说明nltk版本不正确，去官网（https://pypi.python.org/pypi/nltk/3.2.1）选择正确版本号下载。如图8-2所示 ![](https://i.imgur.com/xWM8kUV.png) 图8-2 python安装根路径 (3) 安装成功后，打开python编辑器，输入“import nltk”和“nltk.download()”下载NLTK数据包（如图8-3所示）选中book，修改下载路径“D:\Users\Administrator\Anaconda3\nltk_data”。（book包含了数据案例和内置函数）。 >>> import nltk >>> nltk.download() ![](https://i.imgur.com/0hWOEkk.png) 图8-3 下载NLTK的book数据包 (4) 环境变量配置：计算机-&gt;属性-&gt;高级系统设置-&gt;高级-&gt;环境变量-系统变量-&gt;path，输入如下路径： D:\Users\Administrator\Anaconda3\nltk_data (5) 打开python解释器输入如下代码，出现如图8-4则安装成功。 >>> from nltk.book import * ![](https://i.imgur.com/kJ1DrqR.png) 图8-4 成功安装NLTK数据包 > NLTK 核心包 NLTK核心包主要包括如下，在安装Anaconda时候已经预安装过了。如果读者没有采用Anaconda安装方式，可以点击（[http://www.lfd.uci.edu/~gohlke/pythonlibs/#numpy](http://www.lfd.uci.edu/~gohlke/pythonlibs/#numpy)）下载安装即可。 - NLTK-Data:分析和处理语言的语料库 - NumPy:科学计算库 - Matplotlib：数据可视化2D绘图库 - NetworkX：存储和操作有节点和边组成的网络结构函数库 ## NLTK使用 > NLTK加载book模块 >>> import nltk >>> from nltk.book import * >>> text1 Text: Moby Dick by Herman Melville 1851 如上代码，第一句是导入NLTK模块，第二句是导入book模块下的全部文件，第三句是显示text1文本信息，第四句是text1文本书名和字节数。 函数concordance搜索指定内容 我们想要在text1《白鲸记》一文中检索美国（即“American”），输入如下代码，执行结果首先显示总共出现12次，它不仅可以展示全文所有出现“American”出现的地方及其上下文，也可以对齐方式打印出来，便于对比分析。 >>> text1.concordance('America') Displaying 12 of 12 matches: of the brain ." -- ULLOA ' S SOUTH AMERICA . " To fifty chosen sylphs of speci , in spite of this , nowhere in all America will you find more patrician - like hree pirate powers did Poland . Let America add Mexico to Texas , and pile Cuba , how comes it that we whalemen of America now outnumber all the rest of the b mocracy in those parts . That great America on the other side of the sphere , A f age ; though among the Red Men of America the giving of the white belt of wam and fifty leagues from the Main of America , our ship felt a terrible shock , , in the land - locked heart of our America , had yet been nurtured by all thos some Nor ' West Indian long before America was discovered . What other marvels d universally applicable . What was America in 1492 but a Loose - Fish , in whi w those noble golden coins of South America are as medals of the sun and tropic od of the last one must be grown in America ." " Aye , aye ! a strange sight th >>> 函数similar查找相似上下文 我们想要在text1《白鲸记》中检索与‘very’相似的上下文，输入如下代码即可： >>> text1.similar('very') a same so last first pretty the too only other one rather as great entire next white strange long broad 函数common_contexts共用多个词汇的上下文 那么，当我们不满足在text1《白鲸记》中检索某个单词，而是想搜索共用多个词汇的上下文时，请输入如下代码： >>> text1.common_contexts(['a','very']) by_heedful was_good was_clear is_curious had_little of_great was_calm s_queer 函数dispersion_plot离散图表示词汇分布情况 判断词在文本中的位置，从开头算起有多少词出现，可以离散图表示，每一列代表一个单词，每一行代表有个文本。还是以text1《白鲸记》为例，如下代码：运行结果如图8-5所示： >>> text1.dispersion_plot(["The","Moby","Dick","America"]) ![](https://i.imgur.com/gePEK81.png) 图8-5 词汇分布情况 函数len()计数词汇 >> len(text1) 260819 词汇表排序 >> sorted(set(text1)) 词汇表大小 >> len(set(text1)) 每个词平均使用次数 >> len(text1)/len(set(text1)) 特定词在文本中出现的次数 >> text1.count("smote") 特定词在文本中所占的百分比 >> 100*text1.count('a')/len(text1) NLTK搜索函数FreqDist() 查询文本text1中词汇分布情况，诸如the使用了13721次 >>> fdist1=FreqDist(text1) >>> fdist1 FreqDist({',': 18713, 'the': 13721, '.': 6862, 'of': 6536, 'and': 6024, 'a': 4569, 'to': 4542, ';': 4072, 'in': 3916, 'that': 2982, ...}) 指定查询某个词的使用频率 >>> fdist1['whale'] 906 指定常用词累积频率图 fdist1.plot(50,cumulative=True)，text1中50个常用词的累积频率图，这些词占了所有标识的将近一半，如图8-6所示。 ![](https://i.imgur.com/yaoku81.png) 图8-6 常用词累积频率图 注意：函数fdist1.hapaxes()低频词出现1次查找 细粒度查询 >> V=set(text1) >> longwords=[w for w in V if len(w) > 15] >> sorted(longwords) ['CIRCUMNAVIGATION', 'Physiognomically', 'apprehensiveness', 'cannibalistically', 'characteristically', 'circumnavigating', 'circumnavigation', 'circumnavigations', 'comprehensiveness', 'hermaphroditical', 'indiscriminately', 'indispensableness', 'irresistibleness', 'physiognomically', 'preternaturalness', 'responsibilities', 'simultaneousness', 'subterraneousness', 'supernaturalness', 'superstitiousness', 'uncomfortableness', 'uncompromisedness', 'undiscriminating', 'uninterpenetratingly'] >> 查询文本中单词长度大于10并且出现次数超过10次的 >>> sorted(w for w in set(text1) if len(w) > 10 and fdist1[w] > 10) ['Nantucketer', 'Nevertheless', 'circumstance', 'circumstances', 'considerable', 'considering', 'continually', 'countenance', 'disappeared', 'encountered', 'exceedingly', 'experienced', 'harpooneers', 'immediately', 'indifferent', 'indispensable', 'involuntarily', 'naturalists', 'nevertheless', 'occasionally', 'peculiarities', 'perpendicular', 'significant', 'simultaneously', 'straightway', 'unaccountable'] >>> 词语搭配和双连词 搭配：不经常在一起出现的词序列，如red wine是搭配而the wine就不是。另一个特点就是词不能被类似的词置换，如maroon wine（栗色酒）就不行 bigrams（）：获取搭配，提前文本词汇的双连词 >>> from nltk import bigrams >>> from collections import Counter >>> b = bigrams('This is a test') >>> Counter(b) Counter({('s', ' '): 2, ('i', 's'): 2, ('s', 't'): 1, (' ', 'i'): 1, (' ', 'a'): 1, (' ', 't'): 1, ('a', ' '): 1, ('h', 'i'): 1, ('e', 's'): 1, ('t', 'e'): 1, ('T', 'h'): 1}) >>> NLTK频率分布类中定义的函数 fdist=FreqDist(Samples) 创建包含给定样本的频率分布 fdist.inc(Sample) 增加样本 fdist[‘monstrous’] 计数给定样本出现的次数 fdist.freq(‘monstrous’) 给定样本的频率 fdist.N() 样本总数 fdist.keys() 以频率递减顺序排序样本链表 for sample in fdist: 以频率递减顺序遍历样本 fdist.max() 数值最大的样本 fdist.tabulate() 绘制频率分布表 fdist.plot() 绘制频率分布图 fdist.plot(cumulative=True)绘制累积频率分布图 fdist1&lt;fdist2 测试样本在fdist1中出现的频率是否小于fdist2 词汇比较运算 s.startswith(t) 测试是否t开头 s.endswith(t) 测试是否t结尾 t in s 测试s是否包含t s.islower() 测试s所有字符是否都是小写字母 s.isupper() 测试s所有字符是否都是大写字母 s.isalpha() 测试s所有字符是否都是字母 s.isalnum() 测试s所有字符是否都是字母或数字 s.isdigit() 测试s所有字符是否都是数字 s.istitle() 测试s所有词首字母都是大写 Python NLTK下使用stanford NLP NLTK和StandfordNLP简介 NLTK：由宾夕法尼亚大学计算机和信息科学系使用python语言实现的一种自然语言工具包，其收集的大量公开数据集、模型上提供了全面易用的接口，涵盖了分词、词性标注(Part-Of-Speech tag, POS-tag)、命名实体识别(Named Entity Recognition, NER)、句法分析(Syntactic Parse)等各项 NLP 领域的功能。 Stanford NLP：是由斯坦福大学的 NLP 小组开源的 Java 实现的 NLP 工具包，同样对 NLP 领域的各个问题提供了解决办法。斯坦福大学的 NLP 小组是世界知名的研究小组，能将 NLTK 和 Stanford NLP 这两个工具包结合起来使用，那对于自然语言开发者是再好不过了。2004 年 Steve Bird 在 NLTK 中加上了对 Stanford NLP 工具包的支持，通过调用外部的 jar 文件来使用 Stanford NLP 工具包的功能这样一来就变得更为方便好用。 本文在主要介绍NLTK 中提供 Stanford NLP 中的以下几个功能: 中英文分词: StanfordTokenizer 中英文词性标注: StanfordPOSTagger 中英文命名实体识别: StanfordNERTagger 中英文句法分析: StanfordParser 中英文依存句法分析: StanfordDependencyParser 安装配置过程中注意事项 本文以Python 3.5.2和java version “1.8.0_111”版本进行配置，具体安装需要注意以下几点： Stanford NLP 工具包需要 Java 8 及之后的版本，如果出错请检查 Java 版本 本文的配置都是以 Stanford NLP 3.6.0 为例，如果使用的是其他版本，请注意替换相应的文件名 本文的配置过程以 NLTK 3.2 为例，如果使用 NLTK 3.1，需要注意该旧版本中 StanfordSegmenter 未实现，其余大致相同 下面的配置过程是具体细节可以参照：http://nlp.stanford.edu/software/ StandfordNLP必要工具包下载 必要包下载：只需要下载以下3个文件就够了，stanfordNLTK文件里面就是StanfordNLP工具包在NLTK中所依赖的jar包和相关文件 stanfordNLTK(https://pan.baidu.com/s/1nvEYdfj) ：自己将所有需要的包和相关文件已经打包在一起了，下面有具体讲解. Jar1.8(http://pan.baidu.com/s/1miubwq0) ：如果你本机是Java 8以上版本，可以不用下载了. NLTK(https://pan.baidu.com/s/1pKA9XuN) ：这个工具包提供Standford NLP接口. 以上文件下载后，Jar如果是1.8的版本可以不用下载，另外两个压缩包下载到本地，解压后拷贝文件夹到你的python安装主路径下，然后cmd进入NLTK下通过python setup.py install即可。后面操作讲路径简单修改即可。（如果不能正常分词等操作，查看python是否是3.2以上版本，java是否是8以后版本，jar环境变量是否配置正确） StanfordNLTK目录结构如下：（从各个压缩文件已经提取好了，如果读者感兴趣，下面有各个功能的源码文件） ![](https://i.imgur.com/eguMwDH.jpg) 图8-7 Stanford NLTK源码解析 分词依赖：stanford-segmenter.jar、 slf4j-api.jar、data文件夹相关子文件 命名实体识别依赖：classifiers、stanford-ner.jar 词性标注依赖：models、stanford-postagger.jar 句法分析依赖：stanford-parser.jar、stanford-parser-3.6.0-models.jar、classifiers 依存语法分析依赖：stanford-parser.jar、stanford-parser-3.6.0-models.jar、classifiers 压缩包下载和源码分析 分词压缩包：StanfordSegmenter和StanfordTokenizer:下载stanford-segmenter-2015-12-09.zip （https://pan.baidu.com/s/1kVc20ib） 解压获取目录中的 stanford-segmenter-3.6.0.jar 拷贝为 stanford-segmenter.jar和 slf4j-api.jar 词性标注压缩包：下载stanford-postagger-full-2015-12-09.zip (https://pan.baidu.com/s/1hrVMSE4) 解压获取stanford-postagger.jar 命名实体识别压缩包：下载stanford-ner-2015-12-09.zip (https://pan.baidu.com/s/1skOJb5r) ，将解压获取stanford-ner.jar和classifiers文件 句法分析、句法依存分析：下载stanford-parser-full-2015-12-09.zip （http://pan.baidu.com/s/1nv6Q2bZ） 解压获取stanford-parser.jar 和 stanford-parser-3.6.0-models.jar Standford NLP的应用 (1) 分词 StanfordSegmenter 中文分词：下载52nlp改过的NLTK包nltk-develop(https://pan.baidu.com/s/1misFxna)，解压后将其拷贝到你的python目录下，进去E:\Python\nltk-develop采用python 编辑器打开setup.py文件，F5运行，输入以下代码： >>> from nltk.tokenize.stanford_segmenter import StanfordSegmenter >>> segmenter = StanfordSegmenter( path_to_jar=r"E:\tools\stanfordNLTK\jar\stanford-segmenter.jar", path_to_slf4j=r"E:\tools\stanfordNLTK\jar\slf4j-api.jar", path_to_sihan_corpora_dict=r"E:\tools\stanfordNLTK\jar\data", path_to_model=r"E:\tools\stanfordNLTK\jar\data\pku.gz", path_to_dict=r"E:\tools\stanfordNLTK\jar\data\dict-chris6.ser.gz" ) >>> str="我在博客园开了一个博客，我的博客名叫伏草惟存，写了一些自然语言处理的文章。" >>> result = segmenter.segment(str) >>> result 执行结果： [我 在 博客 园 开 了 一个 博客 ， 我 的 博客 名 叫 伏 草 惟 存 ， 写 了 一些 自然 语言 处理 的 文章。] 程序解读：StanfordSegmenter 的初始化参数说明: - path_to_jar: 用来定位jar包，本程序分词依赖stanford-segmenter.jar（注: 其他所有 Stanford NLP 接口都有 path_to_jar 这个参数。） - path_to_slf4j: 用来定位slf4j-api.jar作用于分词 - path_to_sihan_corpora_dict: 设定为 stanford-segmenter-2015-12-09.zip 解压后目录中的 data 目录， data 目录下有两个可用模型 pkg.gz 和 ctb.gz 需要注意的是，使用 StanfordSegmenter 进行中文分词后，其返回结果并不是 list ，而是一个字符串，各个汉语词汇在其中被空格分隔开。 StanfordTokenizer 英文分词 Python 3.5.2 (v3.5.2:4def2a2901a5, Jun 25 2016, 22:01:18) [MSC v.1900 32 bit (Intel)] on win32 Type "copyright", "credits" or "license()" for more information. >>> from nltk.tokenize import StanfordTokenizer >>> tokenizer = StanfordTokenizer(path_to_jar=r"E:\tools\stanfordNLTK\jar\stanford-parser.jar") >>> sent = "Good muffins cost $3.88\nin New York. Please buy me\ntwo of them.\nThanks." >>> print(tokenizer.tokenize(sent)) 运行结果： [‘Good’, ‘muffins’, ‘cost’, ‘$’, ‘3.88’, ‘in’, ‘New’, ‘York’, ‘.’, ‘Please’, ‘buy’, ‘me’, ‘two’, ‘of’, ‘them’, ‘.’, ‘Thanks’, ‘.’] (2) 命名实体识别 StanfordNERTagger 英文命名实体识别 >>> from nltk.tag import StanfordNERTagger >>> eng_tagger = StanfordNERTagger(model_filename=r'E:\tools\stanfordNLTK\jar\classifiers\english.all.3class.distsim.crf.ser.gz',path_to_jar=r'E:\tools\stanfordNLTK\jar\stanford-ner.jar') >>> print(eng_tagger.tag('Rami Eid is studying at Stony Brook University in NY'.split())) 运行结果 [(&#39;Rami&#39;, &#39;PERSON&#39;), (&#39;Eid&#39;, &#39;PERSON&#39;), (&#39;is&#39;, &#39;O&#39;), (&#39;studying&#39;, &#39;O&#39;), (&#39;at&#39;, &#39;O&#39;), (&#39;Stony&#39;, &#39;ORGANIZATION&#39;), (&#39;Brook&#39;, &#39;ORGANIZATION&#39;), (&#39;University&#39;, &#39;ORGANIZATION&#39;), (&#39;in&#39;, &#39;O&#39;), (&#39;NY&#39;, &#39;O&#39;)] StanfordNERTagger 中文命名实体识别 >>> result '四川省 成都 信息 工程 大学 我 在 博客 园 开 了 一个 博客 ， 我 的 博客 名叫 伏 草 惟 存 ， 写 了 一些 自然语言 处理 的 文章 。\r\n' >>> from nltk.tag import StanfordNERTagger >>> chi_tagger = StanfordNERTagger(model_filename=r'E:\tools\stanfordNLTK\jar\classifiers\chinese.misc.distsim.crf.ser.gz',path_to_jar=r'E:\tools\stanfordNLTK\jar\stanford-ner.jar') >>> for word, tag in chi_tagger.tag(result.split()): print(word,tag) 运行结果： 四川省 ORG 成都 ORG 信息 ORG 工程 ORG 大学 ORG 我 O 在 O 博客 O 园 O 开 O 了 O 一个 O 博客 O ， O 我 O 的 O 博客 O ... ... (3) 词性标注 StanfordPOSTagger 英文词性标注 >>> from nltk.tag import StanfordPOSTagger >>> eng_tagger = StanfordPOSTagger(model_filename=r'E:\tools\stanfordNLTK\jar\models\english-bidirectional-distsim.tagger',path_to_jar=r'E:\tools\stanfordNLTK\jar\stanford-postagger.jar') >>> print(eng_tagger.tag('What is the airspeed of an unladen swallow ?'.split())) 运行结果： [(&#39;What&#39;,&#39;WP&#39;),(&#39;is&#39;,&#39;VBZ&#39;),(&#39;the&#39;,&#39;DT&#39;),(&#39;airspeed&#39;,&#39;NN&#39;),(&#39;of&#39;,&#39;IN&#39;),(&#39;an&#39;,&#39;DT&#39;),(&#39;unladen&#39;,&#39;JJ&#39;),(&#39;swallow&#39;,&#39;VB&#39;),(&#39;?&#39;,&#39;.&#39;)] StanfordPOSTagger 中文词性标注 >>> from nltk.tag import StanfordPOSTagger >>> chi_tagger = StanfordPOSTagger(model_filename=r'E:\tools\stanfordNLTK\jar\models\chinese-distsim.tagger',path_to_jar=r'E:\tools\stanfordNLTK\jar\stanford-postagger.jar') >>> result '四川省 成都 信息 工程 大学 我 在 博客 园 开 了 一个 博客 ， 我 的 博客 名叫 伏 草 惟 存 ， 写 了 一些 自然语言 处理 的 文章 。\r\n' >>> print(chi_tagger.tag(result.split())) 运行结果 四川省 NR 成都 NR 信息 NN 工程 NN 大学 NN 我 PN 在 P 博客 NN 园 NN 开 VV 了 AS 一个 CD 博客 NN ， PU 我 PN 的 DEG 博客 NN ... ... (4) 句法分析 StanfordParser英文句法分析 >>> from nltk.parse.stanford import StanfordParser >>> eng_parser = StanfordParser(r"E:\tools\stanfordNLTK\jar\stanford-parser.jar",r"E:\tools\stanfordNLTK\jar\stanford-parser-3.6.0-models.jar",r"E:\tools\stanfordNLTK\jar\classifiers\englishPCFG.ser.gz") >>> print(list(eng_parser.parse("the quick brown fox jumps over the lazy dog".split()))) 运行结果 [Tree(&#39;ROOT&#39;,[Tree(&#39;NP&#39;,[Tree(&#39;DT&#39;,[&#39;the&#39;]),Tree(&#39;JJ&#39;,[&#39;quick&#39;]),Tree(&#39;JJ&#39;,[&#39;brown&#39;]),Tree(&#39;NN&#39;,[&#39;for&#39;])]),Tree(&#39;NP&#39;,[Tree(&#39;NP&#39;,[Tree(&#39;NNS&#39;,[&#39;jumps&#39;])]),Tree(&#39;PP&#39;,[Tree(&#39;IN&#39;,[&#39;over&#39;]),Tree(&#39;NP&#39;,[Tree(&#39;DT&#39;,[&#39;the&#39;]),Tree(&#39;JJ&#39;,[&#39;lazy&#39;]),Tree(&#39;NN&#39;,[&#39;dog&#39;])])])])]] StanfordParser 中文句法分析 >>> from nltk.parse.stanford import StanfordParser >>> chi_parser = StanfordParser(r"E:\tools\stanfordNLTK\jar\stanford-parser.jar",r"E:\tools\stanfordNLTK\jar\stanford-parser-3.6.0-models.jar",r"E:\tools\stanfordNLTK\jar\classifiers\chinesePCFG.ser.gz") >>> sent = u'北海 已 成为 中国 对外开放 中 升起 的 一 颗 明星' >>> print(list(chi_parser.parse(sent.split()))) 运行结果 [Tree(&#39;ROOT&#39;,[Tree(&#39;NP&#39;,[Tree(&#39;NR&#39;,[&#39;北海&#39;]),Tree(&#39;VP&#39;,[Tree(&#39;ADVP&#39;),[Tree(&#39;AD&#39;,[&#39;已&#39;])]),Tree(&#39;VP&#39;,Tree(&#39;VV&#39;,[&#39;成为&#39;]),Tree(&#39;NP&#39;,[Tree(&#39;NP&#39;,[Tree(&#39;NR&#39;,[&#39;中国&#39;])]),Tree(&#39;LCP&#39;,[Tree(&#39;NP&#39;),[Tree(&#39;NN&#39;,[&#39;对外开放&#39;])]),Tree(&#39;LC&#39;,[&#39;中&#39;])]),Tree(&#39;CP&#39;,Tree(&#39;IP&#39;)[Tree(&#39;VP&#39;,[Tree(&#39;VV&#39;,[&#39;升起&#39;])]),Tree(&#39;DEC&#39;,[&#39;的&#39;])]),Tree(&#39;QP&#39;,[Tree(&#39;CD&#39;,[&#39;一&#39;]),Tree(&#39;CLR&#39;,[Tree(&#39;M&#39;,[&#39;颗&#39;])])]),Tree(&#39;NP&#39;,[Tree(&#39;NN&#39;,[&#39;明星&#39;])])])])])])])])] (5) 依存句法分析 StanfordDependencyParser 英文依存句法分析 >>> from nltk.parse.stanford import StanfordDependencyParser >>> eng_parser = StanfordDependencyParser(r"E:\tools\stanfordNLTK\jar\stanford-parser.jar",r"E:\tools\stanfordNLTK\jar\stanford-parser-3.6.0-models.jar",r"E:\tools\stanfordNLTK\jar\classifiers\englishPCFG.ser.gz") >>> res = list(eng_parser.parse("the quick brown fox jumps over the lazy dog".split())) >>> for row in res[0].triples(): print(row) 运行结果 ((&#39;for&#39;,&#39;NN&#39;),&#39;det&#39;,(&#39;the&#39;,&#39;DT&#39;)) ((&#39;for&#39;,&#39;NN&#39;),&#39;amod&#39;,(&#39;quick&#39;,&#39;JJ&#39;)) ((&#39;for&#39;,&#39;NN&#39;),&#39;amod&#39;,(&#39;brown&#39;,&#39;JJ&#39;)) ((&#39;for&#39;,&#39;NN&#39;),&#39;dep&#39;,(&#39;jumps&#39;,&#39;NNS&#39;)) ((&#39;jumps&#39;,&#39;NNS&#39;),&#39;nmod&#39;,(&#39;dog&#39;,&#39;NN&#39;)) ((&#39;dog&#39;,&#39;NN&#39;),&#39;case&#39;,(&#39;over&#39;,&#39;IN&#39;)) ((&#39;dog&#39;,&#39;NN&#39;),&#39;det&#39;,(&#39;the&#39;,&#39;DT&#39;)) ((&#39;dog&#39;,&#39;NN&#39;),&#39;amod&#39;,(&#39;lazy&#39;,&#39;JJ&#39;)) StanfordDependencyParser 中文依存句法分析 >>> from nltk.parse.stanford import StanfordDependencyParser >>> chi_parser = StanfordDependencyParser(r"E:\tools\stanfordNLTK\jar\stanford-parser.jar",r"E:\tools\stanfordNLTK\jar\stanford-parser-3.6.0-models.jar",r"E:\tools\stanfordNLTK\jar\classifiers\chinesePCFG.ser.gz") >>> res = list(chi_parser.parse(u'四川 已 成为 中国 西部 对外开放 中 升起 的 一 颗 明星'.split())) >>> for row in res[0].triples(): print(row) 运行结果 ((&#39;成为&#39;,&#39;VV&#39;),&#39;nsubj&#39;,(&#39;四川&#39;,&#39;NR&#39;)) ((&#39;成为&#39;,&#39;VV&#39;),&#39;advmod&#39;,(&#39;已&#39;,&#39;AD&#39;)) ((&#39;成为&#39;,&#39;VV&#39;),&#39;dobj&#39;,(&#39;明星&#39;,&#39;NN&#39;)) ((&#39;明星&#39;,&#39;NN&#39;),&#39;dep&#39;,(&#39;对外开放&#39;,&#39;NN&#39;)) ((&#39;对外开放&#39;,&#39;NN&#39;),&#39;nn&#39;,(&#39;中国&#39;,&#39;NR&#39;)) ((&#39;对外开放&#39;,&#39;NN&#39;),&#39;nn&#39;,(&#39;西部&#39;,&#39;NN&#39;)) ((&#39;对外开放&#39;,&#39;NN&#39;),&#39;case&#39;,(&#39;中&#39;,&#39;LC&#39;)) ((&#39;明星&#39;,&#39;NN&#39;),&#39;relcl&#39;,(&#39;升起&#39;,&#39;VV&#39;)) ((&#39;升起&#39;,&#39;VV&#39;),&#39;mark&#39;,(&#39;的&#39;,&#39;DEC&#39;)) ((&#39;明星&#39;,&#39;NN&#39;),&#39;clf&#39;,(&#39;颗&#39;,&#39;M&#39;)) ((&#39;颗&#39;,&#39;M&#39;),&#39;nummod&#39;,(&#39;一&#39;,&#39;CD&#39;)) 获取语料库语料库是语料库语言学研究的基础资源，也是经验主义语言研究的主要资源。应用于词典编纂，语言教学，传统语言研究，自然语言处理中基于统计或实例的研究等方面。关于语料库有三点基本认识：语料库中存放的是在语言的实际使用中真实出现过的语言材料；语料库是以电子计算机为载体承载语言知识的基础资源；真实语料需要经过加工（分析和处理），才能成为有用的资源。 国内外著名语料库 多语料库 点通多语言语音语料库： https://archive.is/20121208123647/http://www.dmcbc.com.cn/ 宾州大学语料库： https://www.ldc.upenn.edu/ Wikipedia XML 语料库 ： http://www-connex.lip6.fr/~denoyer/wikipediaXML/ 中英双语知识本体词网（http://bow.sinica.edu.tw/）：结合词网，知识本体，与领域标记的词汇知识库。 英文语料库 古滕堡语料库： http://www.gutenberg.org/ 语料库在线： http://www.aihanyu.org/cncorpus/index.aspx#P0 中文语料库 搜狗实验室新闻|互联网数据： http://www.sogou.com/labs/ 北京大学语言研究中心： http://ccl.pku.edu.cn/term.asp 计算机语言研究所： http://www.icl.pku.edu.cn/icl_res/ 数据堂： http://www.datatang.com/ 中央研究院平衡语料库（https://www.sinica.edu.tw/SinicaCorpus/）：专门针对语言分析而设计的，每个文句都依词断开并标示词类。语料的搜集也尽量做到现代汉语分配在不同的主题和语式上，是现代汉语无穷多的语句中一个代表性的样本。现有语料库主要针对语言分析而设计，由中央研究院信息所、语言所词库小组完成，内含有简介、使用说明，现行的语料库是4.0版本。 LIVAC汉语共时语料库： http://www.livac.org/index.php?lang=tc 兰开斯特大学汉语平衡语料库： http://www.lancaster.ac.uk/fass/projects/corpus/ 兰开斯特-洛杉矶汉语口语语料库： http://www.lancaster.ac.uk/fass/projects/corpus/ 语料库语言学在线： http://www.corpus4u.org/ 北京森林工作室汉语句义结构标注语料库： http://www.isclab.org.cn/csa/bfs-ctc.htm 国家语委现代汉语语料库（http://www.cncorpus.org/）：现代汉语通用平衡语料库现在重新开放网络查询了。重开后的在线检索速度更快，功能更强，同时提供检索结果下载。现代汉语语料库在线提供免费检索的语料约2000万字，为分词和词性标注语料。 古代汉语语料库（http://www.cncorpus.org/login.aspx）：网站现在还增加了一亿字的古代汉语生语料，研究古代汉语的也可以去查询和下载。网站同时还提供了分词、词性标注软件，词频统计、字频统计软件。基于国家语委语料库的字频词频统计结果和发布的词表等进行建库，以供学习研究语言文字的同学老师使用。 《人民日报》标注语料库（http://www.icl.pku.edu.cn/icl_res/）：《人民日报》标注语料库中一半的语料(1998年上半年)共1300万字，已经通过《人民日报》新闻信息中心公开并提供许可使用权。其中一个月的语料(1998年1月)近200万字在互联网上公布，供自由下载。 古汉语语料库（http://www.sinica.edu.tw/ftms-bin/ftmsw）：古汉语语料库包含以下五个语料库: 上古汉语、中古汉语(含大藏经)、近代汉语、出土文献、其他。部分数据取自史语所汉籍全文数据库，故两者间内容略有重叠。此语料库之出土文献语料库，全部取自史语所汉简小组所制作的数据库。 近代汉语标记语料库（http://www.sinica.edu.tw/Early_Mandarin/）：为应对汉语史研究需求而建构的语料库。目前语料库所搜集的语料已涵盖上古汉语（先秦至西汉）、中古汉语（东汉魏晋南北朝）、近代汉语（唐五代以后）大部分的重要语料，并己陆续开放使用；在标记语料库方面，上古汉语及近代汉语都已有部分语料完成标注的工作，并视结果逐步提供上线检索。 树图数据库（http://treebank.sinica.edu.tw/） 搜文解字（http://words.sinica.edu.tw/）：包含「搜词寻字」、「文学之美」、「游戏解惑」、「古文字的世界」四个单元，可由部件、部首、字、音、词互查，并可查询在四书、老、庄、唐诗中的出处，及直接链接到出处并阅读原文。 文国寻宝记（http://www.sinica.edu.tw/wen/）：在搜文解字的基础之上，以华语文学习者为对象，进一步将字、词、音的检索功能与国编、华康、南一等三种版本的国小国语课本结合。与唐诗三百首、宋词三百首、红楼梦、水浒传等文学典籍结合，提供网络上国语文学习的素材。 唐诗三百首（http://cls.admin.yzu.edu.tw/300/）：以国中、小学学生为主要使用对象，提供吟唱、绘画、书法等多媒体数据，文字数据包含作者生平、读音标注、翻译、批注、评注、典故出处等资料；检索点包含作 者、诗题、诗句、综合资料、体裁分类等；检索结果可以列出全文，并选择标示相关之文字及多媒体数据。并提供了一套可以自动检查格律、韵脚、批改的「依韵入诗格律自动检测索引教学系统」，协助孩子们依韵作诗，协助教师批改习作。 汉籍电子文献（http://www.sinica.edu.tw/~tdbproj/handy1/）：包含整部25史 整部阮刻13经、超过2000万字的台湾史料、1000万字的大正藏以及其他典籍。 红楼梦网络教学研究数据中心（http://cls.hs.yzu.edu.tw/HLM/home.htm）：元智大学中国文学网络系统研究室所开发的「网络展书读—中国文学网络系统」，为研究中心负责人罗凤珠老师主持。红楼梦是其中一个子系统，其他还包括善本书、诗经、唐宋诗词、作诗填词等子系统。此网站为国内Internet最大中国文学研究数据库，提供用户最完整的中国文学研究数据。 中国传媒大学文本语料库检索系统（http://ling.cuc.edu.cn/RawPub/） 在线分词标注系统（http://ling.cuc.edu.cn/cucseg/） 新词语研究资源库（http://ling.cuc.edu.cn/newword/web/index.asp） 音视频语料检索系统（http://ling.cuc.edu.cn/mmcpub） 哈工大信息检索研究室对外共享语料库资源（http://ir.hit.edu.cn/demo/ltp/Sharing_Plan.htm）：该语料库为汉英双语语料库，10万对齐双语句对，文本文件格式，同义词词林扩展版，77,343条词语，秉承《同义词词林》的编撰风格。同时采用五级编码体系，多文档自动文摘语料库，40个主题，文本文件格式，同一主题下是同一事件的不同报道。汉语依存树库，不带关系5万句，带关系1万句；LTML化，分词、词性、句法部分人工标注，可以图形化查看，问答系统问题集，6264句；已标注问题类型，LTML化，分词、词性、句法、词义、浅层语义等程序处理得到，单文档自动文摘语料库共211篇。 清华大学汉语均衡语料库TH-ACorpus（http://www.lits.tsinghua.edu.cn/ainlp/source.htm） 中国科学院计算技术研究所，跨语言语料库（http://mtgroup.ict.ac.cn/new/resource/index.php）目前的双语句对数据库中有约180,000对已对齐的中英文句子。 本数据库支持简单的中英文查询服务。 查询结果包括句对编号、中文句子、英文句子、句对来源等。 网络数据获取 从网络和硬盘访问文本（在线获取伤寒杂病论） >>> from __future__ import division >>> import nltk,re,pprint >>> from urllib.request import urlopen >>> url=r'http://www.gutenberg.org/files/24272/24272-0.txt' >>> raw=urlopen(url).read() >>> raw = raw.decode('utf-8') >>> len(raw) 70306 >>> raw[2000:2500] 运行结果 '。\r\n陽脈浮大而濡，陰脈浮大而濡，陰脈與陽脈同等者，名曰緩也。 脈浮而緊者，名\r\n曰弦也。弦者狀如弓弦，按之不移也。脈緊者，如轉索無常也。\r\n脈弦而大，弦則為減，大則為芤。減則為寒，芤則為虛。寒虛相搏，此名為革。\r\n婦人則半產、漏下，男子則亡血、失精。\r\n問曰：病有戰而汗出，因得解者，何也？答曰：脈浮而緊，按之反芤，此為本虛，\r\n故當戰而汗出也。其人本虛，是以發戰。以脈浮，故當汗出而解也。\r\n若脈浮而數，按之不芤，此人本不虛；若欲自解，但汗出耳，不發戰也。\r\n問曰：病有不戰而汗出解者，何也？答曰：脈大而浮數，故知不戰汗出而解也。\r\n問曰：病有不戰，不汗出而解者，何也？答曰：其脈自微，此以曾經§發汗、若吐、\r\n若下、若亡血，以內無津液，此陰陽自和，必自愈，故不戰、不汗出而解也。\r\n問曰：傷寒三日，脈浮數而微，病人身涼和者，何也？答曰：此為欲解也。解以\r\n夜半。脈浮而解者，濈然汗出也；脈數而解者，必能食也；脈微而解者，必大汗\r\n出也。\r\n問曰：病脈，欲知愈未愈者，何以別之？答曰：寸口、關上、尺中三處，大小、\r\n浮沉、遲數同等，雖有寒熱不解者，此脈陰陽為和平，雖劇當愈。\r\n立夏得洪（一作浮）' > 在线获取处理HTML文本(红楼梦) >>> import re,nltk >>> from urllib.request import urlopen >>> url='http://www.gutenberg.org/cache/epub/24264/pg24264-images.html' >>> html=urlopen(url).read() >>> html=html.decode('utf-8') >>> html[5000:5500] 运行结果 &#39;五次，纂成目錄，分出章回，則題曰《金陵十二釵》．并題一絕云：\r\n\u3000\u3000滿紙荒唐言，一把辛酸淚！\r\n\u3000\u3000都云作者痴，誰解其中味？\r\n\u3000\u3000出則既明，且看石上是何故事．按那石上書云：\r\n\u3000\u3000當日地陷東南，這東南一隅有處曰姑蘇，有城曰閶門者，最是紅塵中一\r\n二等富貴風流之地．這閶門外有個十里街，街內有個仁清巷，巷內有個古廟\r\n，因地方窄狹，人皆呼作葫蘆廟．廟旁住著一家鄉宦，姓甄，名費，字士隱\r\n．嫡妻封氏，情性賢淑，深明禮義．家中雖不甚富貴，然本地便也推他為望\r\n族了．因這甄士隱稟性恬淡，不以功名為念，每日只以觀花修竹，酌酒吟詩\r\n為樂，倒是神仙一流人品．只是一件不足：如今年已半百，膝下無儿，只有\r\n一女，乳名喚作英蓮，年方三歲．\r\n\u3000\u3000一日，炎夏永晝，士隱于書房閒坐，至手倦拋書，伏几少憩，不覺朦朧\r\n睡去．夢至一處，不辨是何地方．忽見那廂來了一僧一道，且行且談．只听\r\n道人問道：“你攜了這蠢物，意欲何往？&quot;那僧笑道：“你放心，如今現有\r\n一段風流公案正該了結，這一干風流冤家，尚未投胎入世．趁此机會，就將\r\n此蠢物夾帶于中，使他去經歷經歷。”那道人道：“原來近日風流冤孽又將\r\n造劫歷世去不成？但&#39; 处理RSS订阅 >>> import feedparser #feedparser需要在python库中下载,或者在cmd中'pip install feedparser' >>> llog=feedparser.parse(url) 相关正则知识 \d 匹配一个数字 \w 匹配一个字母或者数字 任意个字符（包括0个） 至少一个字符 ? 0个或1个字符 {n} n个字符 {n,m} n-m个字符 \s 匹配一个空格 \s+ 至少有一个空格 \d{3,8} 表示3-8个数字，例如’1234567’ \d{3}\s+\d{3,8} [0-9a-zA-Z_] 匹配一个数字、字母或者下划线 [0-9a-zA-Z_]+ 匹配至少由一个数字、字母或者下划线组成的字符串，比如’a100’，’0_Z’，’Py3000’等等 [a-zA-Z_][0-9a-zA-Z_]*可以匹配由字母或下划线开头，后接任意个由一个数字、字母或者下划线组成的字符串，也就是Python合法的变量 [a-zA-Z_][0-9a-zA-Z_]{0, 19}更精确地限制了变量的长度是1-20个字符（前面1个字符+后面最多19个字符） A|B可以匹配A或B，所以(P|p)ython可以匹配’Python’或者’python’ ^表示行的开头，^\d表示必须以数字开头 表示行的结束，表示行的结束，\d表示必须以数字结束 NLTK获取语料库 古腾堡语料库 (1) 直接获取语料库的所有文本：nltk.corpus.gutenberg.fileids() >>> import nltk >>> nltk.corpus.gutenberg.fileids() 运行效果： [&#39;austen-emma.txt&#39;, &#39;austen-persuasion.txt&#39;, &#39;austen-sense.txt&#39;, &#39;bible-kjv.txt&#39;, &#39;blake-poems.txt&#39;, &#39;bryant-stories.txt&#39;, &#39;burgess-busterbrown.txt&#39;, &#39;carroll-alice.txt&#39;, &#39;chesterton-ball.txt&#39;, &#39;chesterton-brown.txt&#39;, &#39;chesterton-thursday.txt&#39;, &#39;edgeworth-parents.txt&#39;, &#39;melville-moby_dick.txt&#39;, &#39;milton-paradise.txt&#39;, &#39;shakespeare-caesar.txt&#39;, &#39;shakespeare-hamlet.txt&#39;, &#39;shakespeare-macbeth.txt&#39;, &#39;whitman-leaves.txt&#39;] (2) 导入包获取语料库的所有文本 >>> from nltk.corpus import gutenberg >>> gutenberg.fileids() 运行效果： ['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt'] (3) 查找某个文本 >>> persuasion=nltk.corpus.gutenberg.words("austen-persuasion.txt") >>> len(persuasion) 98171 >>> persuasion[:200] 运行效果： [&#39;[&#39;, &#39;Persuasion&#39;, &#39;by&#39;, &#39;Jane&#39;, &#39;Austen&#39;, &#39;1818&#39;, ...] 网络和聊天文本 (1) 获取网络聊天文本 >>> from nltk.corpus import webtext >>> for fileid in webtext.fileids(): print(fileid,webtext.raw(fileid)) (2) 查看网络聊天文本信息 >>> for fileid in webtext.fileids(): print(fileid,len(webtext.words(fileid)),len(webtext.raw(fileid)),len(webtext.sents(fileid)),webtext.encoding(fileid)) 运行效果： firefox.txt 102457 564601 1142 ISO-8859-2 grail.txt 16967 65003 1881 ISO-8859-2 overheard.txt 218413 830118 17936 ISO-8859-2 pirates.txt 22679 95368 1469 ISO-8859-2 singles.txt 4867 21302 316 ISO-8859-2 wine.txt 31350 149772 2984 ISO-8859-2 (3) 即时消息聊天会话语料库 >>> from nltk.corpus import nps_chat >>> chatroom = nps_chat.posts('10-19-20s_706posts.xml') >>> chatroom[123] 运行效果： [&#39;i&#39;, &#39;do&#39;, &quot;n&#39;t&quot;, &#39;want&#39;, &#39;hot&#39;, &#39;pics&#39;, &#39;of&#39;, &#39;a&#39;, &#39;female&#39;, &#39;,&#39;, &#39;I&#39;, &#39;can&#39;, &#39;look&#39;, &#39;in&#39;, &#39;a&#39;, &#39;mirror&#39;, &#39;.&#39;] 布朗语料库 (1) 查看语料信息 >>> from nltk.corpus import brown >>> brown.categories() 运行效果： [&#39;adventure&#39;, &#39;belles_lettres&#39;, &#39;editorial&#39;, &#39;fiction&#39;, &#39;government&#39;, &#39;hobbies&#39;, &#39;humor&#39;, &#39;learned&#39;, &#39;lore&#39;, &#39;mystery&#39;, &#39;news&#39;, &#39;religion&#39;, &#39;reviews&#39;, &#39;romance&#39;, &#39;science_fiction&#39;] (2) 比较文体中情态动词的用法 >>> import nltk >>> from nltk.corpus import brown >>> new_texts=brown.words(categories='news') >>> fdist=nltk.FreqDist([w.lower() for w in new_texts]) >>> modals=['can','could','may','might','must','will'] >>> for m in modals: print(m + ':',fdist[m]) 运行效果： can: 94 could: 87 may: 93 might: 38 must: 53 will: 389 (3) NLTK条件概率分布函数 >>> cfd=nltk.ConditionalFreqDist((genre,word) for genre in brown.categories() for word in brown.words(categories=genre)) >>> genres=['news','religion','hobbies','science_fiction','romance','humor'] >>> modals=['can','could','may','might','must','will'] >>> cfd.tabulate(condition=genres,samples=modals) 运行结果 can could may might must will adventure 46 151 5 58 27 50 belles_lettres 246 213 207 113 170 236 editorial 121 56 74 39 53 233 fiction 37 166 8 44 55 52 government 117 38 153 13 102 244 hobbies 268 58 131 22 83 264 humor 16 30 8 8 9 13 learned 365 159 324 128 202 340 lore 170 141 165 49 96 175 mystery 42 141 13 57 30 20 news 93 86 66 38 50 389 religion 82 59 78 12 54 71 reviews 45 40 45 26 19 58 romance 74 193 11 51 45 43 science_fiction 16 49 4 12 8 16 > 路透社语料库 (1) 包括10788个新闻文档，共计130万字，这些文档分90个主题，安装训练集和测试分组，编号‘test/14826’的文档属于测试文档 >>> from nltk.corpus import reuters >>> print(reuters.fileids()[:50]) 运行结果 [&#39;test/14826&#39;, &#39;test/14828&#39;, &#39;test/14829&#39;, &#39;test/14832&#39;, &#39;test/14833&#39;, &#39;test/14839&#39;, &#39;test/14840&#39;, &#39;test/14841&#39;, &#39;test/14842&#39;, &#39;test/14843&#39;, &#39;test/14844&#39;, &#39;test/14849&#39;, &#39;test/14852&#39;, &#39;test/14854&#39;, &#39;test/14858&#39;, &#39;test/14859&#39;, &#39;test/14860&#39;, &#39;test/14861&#39;, &#39;test/14862&#39;, &#39;test/14863&#39;, &#39;test/14865&#39;, &#39;test/14867&#39;, &#39;test/14872&#39;, &#39;test/14873&#39;, &#39;test/14875&#39;, &#39;test/14876&#39;, &#39;test/14877&#39;, &#39;test/14881&#39;, &#39;test/14882&#39;, &#39;test/14885&#39;, &#39;test/14886&#39;, &#39;test/14888&#39;, &#39;test/14890&#39;, &#39;test/14891&#39;, &#39;test/14892&#39;, &#39;test/14899&#39;, &#39;test/14900&#39;, &#39;test/14903&#39;, &#39;test/14904&#39;, &#39;test/14907&#39;, &#39;test/14909&#39;, &#39;test/14911&#39;, &#39;test/14912&#39;, &#39;test/14913&#39;, &#39;test/14918&#39;, &#39;test/14919&#39;, &#39;test/14921&#39;, &#39;test/14922&#39;, &#39;test/14923&#39;, &#39;test/14926&#39;] (2) 查看语料包括的前100个类别： print(reuters.categories()[:100]) 运行结果： [&#39;acq&#39;, &#39;alum&#39;, &#39;barley&#39;, &#39;bop&#39;, &#39;carcass&#39;, &#39;castor-oil&#39;, &#39;cocoa&#39;, &#39;coconut&#39;, &#39;coconut-oil&#39;, &#39;coffee&#39;, &#39;copper&#39;, &#39;copra-cake&#39;, &#39;corn&#39;, &#39;cotton&#39;, &#39;cotton-oil&#39;, &#39;cpi&#39;, &#39;cpu&#39;, &#39;crude&#39;, &#39;dfl&#39;, &#39;dlr&#39;, &#39;dmk&#39;, &#39;earn&#39;, &#39;fuel&#39;, &#39;gas&#39;, &#39;gnp&#39;, &#39;gold&#39;, &#39;grain&#39;, &#39;groundnut&#39;, &#39;groundnut-oil&#39;, &#39;heat&#39;, &#39;hog&#39;, &#39;housing&#39;, &#39;income&#39;, &#39;instal-debt&#39;, &#39;interest&#39;, &#39;ipi&#39;, &#39;iron-steel&#39;, &#39;jet&#39;, &#39;jobs&#39;, &#39;l-cattle&#39;, &#39;lead&#39;, &#39;lei&#39;, &#39;lin-oil&#39;, &#39;livestock&#39;, &#39;lumber&#39;, &#39;meal-feed&#39;, &#39;money-fx&#39;, &#39;money-supply&#39;, &#39;naphtha&#39;, &#39;nat-gas&#39;, &#39;nickel&#39;, &#39;nkr&#39;, &#39;nzdlr&#39;, &#39;oat&#39;, &#39;oilseed&#39;, &#39;orange&#39;, &#39;palladium&#39;, &#39;palm-oil&#39;, &#39;palmkernel&#39;, &#39;pet-chem&#39;, &#39;platinum&#39;, &#39;potato&#39;, &#39;propane&#39;, &#39;rand&#39;, &#39;rape-oil&#39;, &#39;rapeseed&#39;, &#39;reserves&#39;, &#39;retail&#39;, &#39;rice&#39;, &#39;rubber&#39;, &#39;rye&#39;, &#39;ship&#39;, &#39;silver&#39;, &#39;sorghum&#39;, &#39;soy-meal&#39;, &#39;soy-oil&#39;, &#39;soybean&#39;, &#39;strategic-metal&#39;, &#39;sugar&#39;, &#39;sun-meal&#39;, &#39;sun-oil&#39;, &#39;sunseed&#39;, &#39;tea&#39;, &#39;tin&#39;, &#39;trade&#39;, &#39;veg-oil&#39;, &#39;wheat&#39;, &#39;wpi&#39;, &#39;yen&#39;, &#39;zinc&#39;] (3) 查看某个编号的语料下类别尺寸 >>> reuters.categories('training/9865') 运行结果： [&#39;barley&#39;, &#39;corn&#39;, &#39;grain&#39;, &#39;wheat&#39;] (4) 查看某几个联合编号下语料的类别尺寸 >>> reuters.categories(['training/9865','training/9880']) 运行结果： [&#39;barley&#39;, &#39;corn&#39;, &#39;grain&#39;, &#39;money-fx&#39;, &#39;wheat&#39;] (5) 查看哪些编号的文件属于指定的类别 >>> reuters.fileids('barley') 运行结果： [&#39;test/15618&#39;, &#39;test/15649&#39;, &#39;test/15676&#39;, &#39;test/15728&#39;, &#39;test/15871&#39;, &#39;test/15875&#39;, &#39;test/15952&#39;, &#39;test/17767&#39;, &#39;test/17769&#39;, &#39;test/18024&#39;, &#39;test/18263&#39;, &#39;test/18908&#39;, &#39;test/19275&#39;, &#39;test/19668&#39;, &#39;training/10175&#39;, &#39;training/1067&#39;, &#39;training/11208&#39;, &#39;training/11316&#39;, &#39;training/11885&#39;, &#39;training/12428&#39;, &#39;training/13099&#39;, &#39;training/13744&#39;, &#39;training/13795&#39;, &#39;training/13852&#39;, &#39;training/13856&#39;, &#39;training/1652&#39;, &#39;training/1970&#39;, &#39;training/2044&#39;, &#39;training/2171&#39;, &#39;training/2172&#39;, &#39;training/2191&#39;, &#39;training/2217&#39;, &#39;training/2232&#39;, &#39;training/3132&#39;, &#39;training/3324&#39;, &#39;training/395&#39;, &#39;training/4280&#39;, &#39;training/4296&#39;, &#39;training/5&#39;, &#39;training/501&#39;, &#39;training/5467&#39;, &#39;training/5610&#39;, &#39;training/5640&#39;, &#39;training/6626&#39;, &#39;training/7205&#39;, &#39;training/7579&#39;, &#39;training/8213&#39;, &#39;training/8257&#39;, &#39;training/8759&#39;, &#39;training/9865&#39;, &#39;training/9958&#39;] 就职演说语料库 (1) 查看语料信息 >>> from nltk.corpus import inaugural >>> len(inaugural.fileids()) 56 >>> inaugural.fileids() 运行结果： [&#39;1789-Washington.txt&#39;, &#39;1793-Washington.txt&#39;, &#39;1797-Adams.txt&#39;, &#39;1801-Jefferson.txt&#39;, &#39;1805-Jefferson.txt&#39;, &#39;1809-Madison.txt&#39;, &#39;1813-Madison.txt&#39;, &#39;1817-Monroe.txt&#39;, &#39;1821-Monroe.txt&#39;, &#39;1825-Adams.txt&#39;, &#39;1829-Jackson.txt&#39;, &#39;1833-Jackson.txt&#39;, &#39;1837-VanBuren.txt&#39;, &#39;1841-Harrison.txt&#39;, &#39;1845-Polk.txt&#39;, &#39;1849-Taylor.txt&#39;, &#39;1853-Pierce.txt&#39;, &#39;1857-Buchanan.txt&#39;, &#39;1861-Lincoln.txt&#39;, &#39;1865-Lincoln.txt&#39;, &#39;1869-Grant.txt&#39;, &#39;1873-Grant.txt&#39;, &#39;1877-Hayes.txt&#39;, &#39;1881-Garfield.txt&#39;, &#39;1885-Cleveland.txt&#39;, &#39;1889-Harrison.txt&#39;, &#39;1893-Cleveland.txt&#39;, &#39;1897-McKinley.txt&#39;, &#39;1901-McKinley.txt&#39;, &#39;1905-Roosevelt.txt&#39;, &#39;1909-Taft.txt&#39;, &#39;1913-Wilson.txt&#39;, &#39;1917-Wilson.txt&#39;, &#39;1921-Harding.txt&#39;, &#39;1925-Coolidge.txt&#39;, &#39;1929-Hoover.txt&#39;, &#39;1933-Roosevelt.txt&#39;, &#39;1937-Roosevelt.txt&#39;, &#39;1941-Roosevelt.txt&#39;, &#39;1945-Roosevelt.txt&#39;, &#39;1949-Truman.txt&#39;, &#39;1953-Eisenhower.txt&#39;, &#39;1957-Eisenhower.txt&#39;, &#39;1961-Kennedy.txt&#39;, &#39;1965-Johnson.txt&#39;, &#39;1969-Nixon.txt&#39;, &#39;1973-Nixon.txt&#39;, &#39;1977-Carter.txt&#39;, &#39;1981-Reagan.txt&#39;, &#39;1985-Reagan.txt&#39;, &#39;1989-Bush.txt&#39;, &#39;1993-Clinton.txt&#39;, &#39;1997-Clinton.txt&#39;, &#39;2001-Bush.txt&#39;, &#39;2005-Bush.txt&#39;, &#39;2009-Obama.txt&#39;] (2) 查看演说语料的年份 >>> [fileid[:4] for fileid in inaugural.fileids()] 运行结果： [&#39;1789&#39;, &#39;1793&#39;, &#39;1797&#39;, &#39;1801&#39;, &#39;1805&#39;, &#39;1809&#39;, &#39;1813&#39;, &#39;1817&#39;, &#39;1821&#39;, &#39;1825&#39;, &#39;1829&#39;, &#39;1833&#39;, &#39;1837&#39;, &#39;1841&#39;, &#39;1845&#39;, &#39;1849&#39;, &#39;1853&#39;, &#39;1857&#39;, &#39;1861&#39;, &#39;1865&#39;, &#39;1869&#39;, &#39;1873&#39;, &#39;1877&#39;, &#39;1881&#39;, &#39;1885&#39;, &#39;1889&#39;, &#39;1893&#39;, &#39;1897&#39;, &#39;1901&#39;, &#39;1905&#39;, &#39;1909&#39;, &#39;1913&#39;, &#39;1917&#39;, &#39;1921&#39;, &#39;1925&#39;, &#39;1929&#39;, &#39;1933&#39;, &#39;1937&#39;, &#39;1941&#39;, &#39;1945&#39;, &#39;1949&#39;, &#39;1953&#39;, &#39;1957&#39;, &#39;1961&#39;, &#39;1965&#39;, &#39;1969&#39;, &#39;1973&#39;, &#39;1977&#39;, &#39;1981&#39;, &#39;1985&#39;, &#39;1989&#39;, &#39;1993&#39;, &#39;1997&#39;, &#39;2001&#39;, &#39;2005&#39;, &#39;2009&#39;] (3) 条件概率分布 >>> import nltk >>> cfd=nltk.ConditionalFreqDist((target,fileid[:4]) for fileid in inaugural.fileids() for w in inaugural.words(fileid) for target in ['america','citizen'] if w.lower().startswith(target)) >>> cfd.plot() 运行结果 综合案例：走进大秦帝国数据采集和预处理下载孙皓晖先生的《大秦帝国》.zip文件，里面按照语料大小包含5个文件。分别是30852词的p1.txt、70046词的p2.txt、111970词的p3.txt、1182769词的p5.txt、419275词的p10.txt。本书节选了大秦帝国第一部673167字的dqdg.txt。 构建本地语料库构建自己语料库 >>> from nltk.corpus import PlaintextCorpusReader >>> corpus_root=r'E:\dict' >>> wordlists=PlaintextCorpusReader(corpus_root,'.*') >>> wordlists.fileids() ['dqdg.txt', 'q0.txt', 'q1.txt', 'q10.txt', 'q2.txt', 'q3.txt', 'q5.txt', 'text.txt'] >>> len(wordlists.words('text.txt')) #如果输入错误或者格式不正确，notepad++转换下编码格式即可 152389 语料库信息 ![](https://i.imgur.com/m0Hyyoz.jpg) 构建完成自己语料库之后，利用python NLTK内置函数都可以完成对应操作。换言之，其他语料库的方法，在自己语料库中通用。唯一的问题是，部分方法NLTK是针对英文语料的，中文语料不通用（典型的就是分词）。这个问题的解决方法很多，诸如你通过插件等在NLTK工具包内完成对中文的支持。另外也可以在NLTK中利用StandfordNLP工具包完成对自己语料的操作，这部分知识上节讲解过。 ## 大秦帝国语料操作 打开Python编辑器，导出NLTK，并统计大秦帝国第一部共计多少字。（注：在读取文本的时候，python 3.5 IDLE 执行起来比较卡比较慢，采用pycharm就效率高很多了） >>> with open(r"E:\dict\dqdg.txt","r+") as f: str=f.read() 查看大秦帝国第一部总共有多大的用字量，即不重复词和符合的尺寸： >>> len(set(str)) 4053 >>> len(str)/len(set(str)) 166.09104367135456 实验可知用了4053个尺寸的词汇表，平均每个词使用了166次，那么常用词分布如何呢？既然是大秦帝国，那么秦字使用了多少次呢？ >>> str.count("秦") 3538 >>> str.count("大秦") 14 >>> str.count("国") 6536 可以知道，秦用词3538次，大秦用了14次，因为讲的各国之间的事情，国也是高频词6536次。如上所述大秦帝国第一部总词汇表673167，那么整个词汇累积分布如何？ >>> fdist=FreqDist(str) >>> fdist.plot() 运行结果： ![](https://i.imgur.com/byUr0J2.jpg) 这个图横坐标表示词的序列，纵坐标表示词频。表说明词频大于5000的非常少，说明高频词不多。低频词特别多。后面进一步探究下。 看看整本书的累积分布情况如何？ ![](https://i.imgur.com/WsSocrw.jpg) 分析上图我们不难发现，3万以下是低频词大于30%，高频词大于1.4%，中频占68.6%（偏低中频2万左右占29.85%，偏高中频占8.96%） 研究下高频率的1000个词情况？看看都有哪些？ ![](https://i.imgur.com/F3iCotU.jpg) 查看1000个高频词分布如何？ ![](https://i.imgur.com/FmzWNKb.jpg) 1000个高频的累计分布又如何？ ![](https://i.imgur.com/FUFUL68.jpg) 粗略估计下大约占了80%以上。频率最高的前100词的分布如何？ ![](https://i.imgur.com/lsymUkF.jpg) 前100个词也就是大约0.02%的词在本书的累积分布情况怎样呢？ >>> fdist=FreqDist(str) >>> fdist.plot(100,cumulative=True) ![](https://i.imgur.com/FydT69K.jpg) 如图可知，前0.2%词汇占据整本书的50%以上的比例。国、旗、秦、魏、队、阅等跟战争相关词汇使用较多。那么低频词如何呢？有时候低频词也具有其特殊的研究价值。 ![](https://i.imgur.com/kNZ9c2l.png) 统计可知大约有4053个词出现一次，占比0.6%.词语内部搭配又是如何？ >>> from collections import Counter >>> V=Counter(str) ![](https://i.imgur.com/dLFRpkk.png) 查询词频在[0—100]的词有多少？ >>> len([w for w in V.values() if w>> len([w for w in V.values() if w>100 and w>> len([w for w in V.values() if w>1000 and w>> len([w for w in V.values() if w>5000]) 14 参考文献 数据挖掘十大算法：https://wizardforcel.gitbooks.io/dm-algo-top10/content/apriori.html 中文维基百科：https://zh.wikipedia.org/wiki/%E5%85%88%E9%AA%8C%E7%AE%97%E6%B3%95 GitHub：https://github.com/BaiNingchao/MachineLearning-1 图书：《机器学习实战》 图书：《自然语言处理理论与实战》 完整代码下载 源码请进【机器学习和自然语言QQ群：436303759】文件下载： 作者声明 本文版权归作者所有，旨在技术交流使用。未经作者同意禁止转载，转载后需在文章页面明显位置给出原文连接，否则相关责任自行承担。]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>语料库</tag>
        <tag>NLTK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自然语言处理入门]]></title>
    <url>%2F2019%2F02%2F13%2F%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[摘要：随着人工智能的快速发展，自然语言处理和机器学习技术的应用愈加广泛。然而身为初学者，要想快速入门这些前沿技术总是存在着各种各样的困难。为使读者对该领域整体概况有一个更为系统明晰的认识，本章1.1节将主要从发展历程、研究现状、应用前景等角度出发，概要地介绍自然语言处理及相关的机器学习技术。接下来1.2节我们将介绍自然语言处理和机器学习之间的关系。古语说“工欲善其事，必先利其器”，本课程的“器”就是开发环境部署，本书主要介绍Sublime的安装部署与使用。最后，在章末1.5节我们将用一个简单的实战案例让读者亲身领略编程之美。（本文原创，转载必须注明出处.） 初识自然语言处理自然语言处理概述 什么是自然语言处理 自然语言处理（英语：Natural Language Processing，简称NLP）是人工智能和语言学交叉领域下的分支学科。该领域主要探讨如何处理及运用自然语言；自然语言认知（即让电脑“ 懂”人类的语言）；自然语言生成系统（将计算机数据转化为自然语言）以及自然语言理解系统（将自然语言转化为计算机程序更易于处理的形式）。 所谓“自然语言”，其实就是我们日常生活中使用的语言（在这里还包括书面文字和语音视频等），人们熟知的汉语、日语、韩语、英语、法语等语言都属于此范畴。至于“自然语言处理”，则是对自然语言进行数字化处理的一种技术；是通过语音文字等形式与计算机进行通信，从而实现“人机交互”的技术。 自然语言处理学科领域 多交叉学科：自然语言处理是一门多学科交叉的技术，其中包括：语言学、计算机科学（提供模型表示、算法设计、计算机实现）、数学（数学模型）、心理学（人类言语心理模型和理论）、哲学（提供人类思维和语言的更深层次理论）、统计学（提供样本数据的预测统计技术）、电子工程（信息论基础和语言信号处理技术）、生物学（人类言语行为机制理论）。 自然语言处理研究方向 自然语言处理热门的研究方向包括：语音的自动合成与识别、机器翻译、自然语言理解、人机对话、信息检索、文本分类、自动文摘等等。总分为4大方向： 语言学方向 数据处理方向 语言工程方向 人工智能和认知科学方向 细分为13个方面： 口语输入：语音识别、信号表示、鲁棒的语音识别、语音识别中的隐马尔科夫模型方法、语言模型、说话人识别、口语理解； 书面语输入：文献格式识别、光学字符识别（OCR）：印刷体识别/手写体识别、手写界面、手写文字分析； 语言分析理解：小于句子单位的处理、语法的形式化、针对基于约束的语法编写的词表、计算语义学、句子建模和剖析技术、鲁棒的剖析技术； 语言生成：句法生成、深层生成； 口语输入技术：合成语音技术、语音合成的文本解释、口语生成； 话语分析与对话：对话建模、话语建模口语对话系统； 文献自动处理：文献检索、文本解释：信息抽取、文本内容自动归纳、文本写作和编辑的计算机支持、工业和企业中使用的受限语言； 多语问题的计算机处理：机器翻译、人助机译、机助人译、多语言信息检索、多语言语音识别、自动语种验证； 多模态的计算机处理：空间和时间表示方法、文本与图像处理、口语与手势的模态结合、口语与面部信息的模态结合：面部运动和语音识别； 信息传输和信息存储：语音压缩、语音品质的提升； 自然语言处理中的数学方法：统计建模和分类的数学理论、数字信号处理技术、剖析算法的数学基础研究、神经网络、有限状态分析技术、语音和语言处理中的最优化技术和搜索技术； 语言资源：书面语料库、口语语料库、机器词典与词网的建设、术语编撰和术语数据库、网络数据挖掘和信息提取； 自然语言处理系统的评测：面向任务的文本分析评测、机器翻译系统和翻译工具的评测、大覆盖面的自然语言剖析器的评测、语音识别：评估和评测、语音合成评测、系统的可用性和界面的评测、语音通信质量的评测、文字识别系统的评测。 自然语言处理发展历史 机器翻译发展历程 自然语言处理的相关研究，最早是从机器翻译系统的研究开始的。20世纪60年代，国外对机器翻译曾有大规模的研究工作，投入了大量的人力物力财力。但是受客观历史因素的限制，当时人们低估了自然语言的复杂性，语言处理的理论和技术均不成熟，所以进展并不大。当时的主要做法是存储两种语言的大辞典，翻译时词汇与短语一一对应，技术上只是调整语言的同条顺序。但日常生活中语言的翻译远远没有那么简单，人们很多时候做翻译还必须参考某句话前后文的意思。机器翻译的发展大致可分为三个时期 : 初创期(1947 -1970)： 电子计算机问世(1946)的第二年，英国工程师布斯(A.D.Booth)和美国工程师威弗(W.Weaver)最早提出了利用计算机进行自动翻译。1952年，在洛克菲勒基金会的大力支持下，一些西方学者在美国麻省理工学院召开了第一次机器翻译会议。1954年，《机械翻译》(Mechanical Translation)杂志开始公开发行。同年， 世界上第一次机器翻译试验成功开展。虽然这次试验用的机器词汇仅仅包含了250个俄语单词和6条机器语法规则，但是它第一次向公众和学界展示了机器翻译的可行性，也激发了美国政府在随后十余年对机器翻译进行大量资助的兴趣。然而随着研究的深入，人们看到的不是机器翻译的成功，而是一个又一个它无法克服的局限。第一代机器翻译系统设计上的粗糙所带来的翻译质量低劣最终导致了一些人对机器翻译失去了信心，甚至有人认为机器翻译研究追求“全自动、高质量”的目标是不可能实现的。这一系列挫折标志着机器翻译研究在60年代末期逐步陷入低谷。 复苏期(1970 -1976)： 尽管机器翻译困难重重，但是法国、日本、加拿大等国仍然没有放弃研究。在70年代初期，机器翻译终于出现了复苏的局面。在这个时期，研究者们普遍认识到“原语”和“译语”两种语言的差异。这不仅仅是词汇上的不同，而且还存在句法结构上的区别。为了得到可读性更强的译文，就必须在自动句法分析上多下功夫。通过大量的科学实验，机器翻译的研究者们终于逐渐认识到机器翻译过程本身必须保持“原语”和“译语”在语义上的一致，一个好的机器翻译系统应该把“原语”的语义准确无误地在“译语”中表现出来。于是，语义分析在机器翻译中越来越受到重视。美国斯坦福大学的威尔克斯(Y.A.Wilks)于1974年提出了“优选语义学”，并在此基础上设计了英法机器翻译系统。由于这个系统的语义表示方法比较细致，能够解决仅用句法分析难于解决的歧义现象、代词所指等困难问题，译文质量较高，从而受到专家学者们的一致肯定。 繁荣期(1976 —至今)： 繁荣期最突出的特点是机器翻译研究走上了实用化的道路，出现了一大批实用化的机器翻译系统。紧接着，机器翻译产品又开始进入市场，逐渐由实用化步入商业化。第二代机器翻译系统以基于转换的方法为代表，普遍采用以句法分析为主、语义分析为辅的基于规则的方法，采用由抽象的转换表示的分层次实现策略。比如加拿大蒙特利尔大学开发研制的实用性机器翻译系统TAUM-METEO就是采用了典型的转换方法，整个翻译过程分为5个阶段(英-法翻译):英语形态分析、英语句法分析、转换 、法语句法生成和法语形态生成。这个翻译系统投入使用之后，每小时可以翻译6万-30万个词，每天可以翻译1500-2000篇天气预报的讯息并能够通过电视、报纸立即公布。TAUM-METEO系统是“机器翻译发展史上的一个里程碑 ，它标志着机器翻译由复苏走向了繁荣”。 我国机器翻译发展历程 我国机器翻译的起步并不算太晚，是继美国、前苏联、英国之后世界上第四个开展机器翻译研究的国家。早在20世纪50年代机器翻译就被列入我国科学研究的发展规划。一些研究人员还进行了俄汉机器翻译实验，取得了一定的研究成果。我国机器翻译研究的全面开展始于80年代中期，特别是90年代以降，一批机器翻译系统相继问世，其中影响力较大的有:中软总公司开发的汉英-汉日翻译系统(1993);中科院计算所研制的IMTEC英汉翻译系统(1992)等。 在自然语言处理的形成和发展进程中， 除机器翻译外，自然语言理解(Natural Language Understanding)所起到的作用也是不可忽视的。自然语言理解的发展始于20世纪60年代中期机器翻译处于举步维艰之时，到了70年代初，它的研究已获得了累累硕果。自然语言理解，又称“人机对话”，就是“让计算机理解自然语言，使计算机获得人类理解自然语言的智能，并对人给计算机提出的问题，通过对话的方式，用自然语言进行回答”。60年代中期，人们开始由“词对词”的翻译方式逐步转入对自然语言的语法、语义和语用等基本问题的研究，并尝试着让计算机来理解自然语言。许多学者认为，判断计算机是否理解了自然语言的最直观方法，就是让人们同计算机对话，如果计算机对人提出的问题能做出有意义的回答，那就证明计算机已经理解了自然语言。最初的“人机对话”系统(或“自然语言理解”系统)研究工作主要在美国。冯志伟教授把第一代自然语言理解系统分为四种类型 : 特殊格式系统。即根据人机对话内容的特点 ，采用特定的格式来进行人机对话; 以文本为基础的系统。某些研究者不满意在特殊格式系统中种种格式限制，因为就一个专门领域来说，最方便的还是使用不受特殊格式结构限制的系统来进行人机对话; 有限逻辑系统。在这种系统中，自然语言的句子以某种更加形式化的记号来替代，这些记号自成一个有限逻辑系统， 可以进行某些推理; 一般演绎系统。它使用某些标准数学符号来表达信息，可以表达那些在有限逻辑系统中不容易表达出来的复杂信息从而进一步提高了自然语言理解系统的能力。随着研究的进一步深入，第二代自然语言理解系统应运而生。这些系统绝大多数是程序演绎系统，大量地进行语义、语境以至语用的分析，输入输出都是用书面文字。口头的自然语言理解系统还牵扯到语音识别、语音合成等复杂的技术，发展速度比较缓慢。 自然语言处理发展历程 1948年，香农（Shannon）把离散马尔可夫过程的概率模型应用于描述语言的自动机;同时又把“熵” (entropy)的概念引用到语言处理中。而克莱尼(Kleene)在同一时期研究了有限自动机和正则表达式。 1956年，乔姆斯基（Chomsky）提出了上下文无关语法。这一工作导致了基于规则和基于概率两种不同的自然语言处理方法的诞生，使得该领域的研究分成了采用规则方法的符号派(symbolic)和采用概率方法的随机派(stochastic)两大阵营 ， 进而引发了数十年有关这两种方法孰优孰劣的争执 。同年，人工智能诞生以后，自然语言处理迅速融入了人工智能的研究中。随机派学者在这一时期利用贝叶斯方法等统计学原理取得了一定的进步;而以 Chomsky为代表的符号派也进行了形式语言理论生成句法和形式逻辑系统的研究。由于这一时期多数学者注重研究推理和逻辑问题， 只有少数学者在研究统计方法和神经网络，因此符号派的势头明显强于随机派的势头。 1967年，美国心理学家 Neisser提出了认知心理学， 从而把自然语言处理与人类的认知联系起来。 70年代初，由于自然语言处理研究中的一些问题未能在短时间内得到解决，而新的问题又不断涌现，许多人因此丧失了信心，自然语言处理的研究进入了低谷时期。尽管如此，一些发达国家的学者依旧没有停止。基于隐马尔可夫模型 (Hidden Markov Model，HMM)的统计方法和话语分析 (Discourse Analysis)在这一时期取得了重大进展 。 80年代， 在人们对过去的工作进行反思之后 ， 有限状态模型和经验主义的研究方法开始复苏 。 90年代以后，随着计算机的速度和存储量大幅增加，自然语言处理的物质基础大幅改善，语音和语言处理的商品化开发成为可能。同时，网络技术的发展和Internet的逐步商业化使得基于自然语言的信息检索和信息抽取需求变得更加突出，自然语言处理的应用面渐渐不再局限于机器翻译、语音控制等早期研究领域。 从90年代末到21世纪初 ，人们逐渐认识到仅用基于规则的方法或仅用基于统计的方法，都是无法成功进行自然语言处理的。基于统计、基于实例和基于规则的语料库技术在这一时期开始蓬勃发展， 各种处理技术开始融合，自然语言处理的研究又开始兴旺起来。 自然语言处理工作原理计算机处理自然语言的过程：形式化描述——数学模型算法化——程序化——实用化。具体步骤如下： 形式化描述： 把需要研究的问题在语言上建立形式化模型，使其可以以数学形式表示出来； 数学模型算法化： 把数学模型表示为算法的过程称之为“算法化“； 程序化：计算机根据算法进行实现，建立各种自然语言处理系统，这个过程是“程序化“； 实用化： 对系统进行评测和改进最终满足现实需求，这个过程是“实用化“。 自然语言处理应用前景随着自然语言处理的蓬勃发展和深入研究，新的应用方向会不断呈现出来。自然语言处理发展前景十分广阔，主要研究领域有： 文本方面：基于自然语言理解的智能搜索引擎和智能检索、智能机器翻译、自动摘要与文本综合、文本分类与文件整理、智能自动作文系统、自动判卷系统、信息过滤与垃圾邮件处理、文学研究与古文研究、语法校对、文本数据挖掘与智能决策、基于自然语言的计算机程序设计等； 语音方面：机器同声传译、智能远程教学与答疑、语音控制、智能客户服务、机器聊天与智能参谋、智能交通信息服务、智能解说与体育新闻实时解说 、语音挖掘与多媒体挖掘、多媒体信息提取与文本转化、对残疾人智能帮助系统等。 自然语言处理与机器学习机器学习概述 什么是机器学习 机器学习是人工智能的一个分支，研究从以“推理”为重点到以“知识”为重点，再到以“学习”为重点。显然，机器学习是实现人工智能的一个途径，即以机器学习为手段解决人工智能中的问题。机器学习在近30多年的时间里已发展成为一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、计算复杂性理论等多种知识。机器学习理论主要是设计和分析一些让计算机可以自动“学习”的算法。机器学习算法是一类从数据中自动分析获得规律，并利用规律对未知数据进行预测的算法。因为学习算法中涉及了大量的统计学理论，机器学习与推断统计学联系尤为密切，也被称为统计学习理论。算法设计方面，机器学习理论关注可以实现的，行之有效的学习算法。很多推论问题属于无程序可循难度，所以部分的机器学习研究是开发容易处理的近似算法。机器学习现已广泛应用于数据挖掘、计算机视觉、自然语言处理、生物特征识别、搜索引擎、医学诊断、检测信用卡欺诈、证券市场分析、DNA序列测序、语音和手写识别、战略游戏和机器人等领域。 机器学习是“计算机利用经验自动改善系统自身性能的行为”。换言之，机器学习是指通过计算机学习数据中的内在规律性信息，获得新的经验和知识，以提高计算机的智能性，让计算机能够像人那样去决策。本质上机器学习是一个从未知到已知的过程。假设计算机拥有这样一个程序，随着机器解决问题的增多，在该程序的作用下，机器性能或解决问题的能力增强，我们就说这台机器拥有学习能力。机器解决问题能力的增强主要表现在：初始状态下，对于问题Q，机器给出结果A，该机器在解决一系列问题{Q1，Q2，… ，Qn}后，再次遇到问题Q时给出结果A1，而结果 A1比结果A更精确，我们就说机器解决问题的能力得到了增强。 机器学习发展历程 机器学习发展简史 1943年， Warren McCulloch 和 Walter Pitts 提出了神经网络层次结构模型 ，确立了神经网络的计算模型理论，从而为机器学习的发展奠定了基础。 1950年， “人工智能之父”图灵提出了著名的“图灵测试”，使人工智能成为了计算机领域一个重要的研究课题。 1957年， 康内尔大学教授 Frank Rosenblatt 提出Perceptron 概念，并且首次用算法精确定义了自组织自学习的神经网络数学模型， 设计出了第一个计算机神经网络，这个机器学习算法成为神经网络模型的开山鼻祖。 1959年， 美国 IBM 公司的 A．M．Samuel设计了一个具有学习能力的跳棋程序，它曾经战胜了美国一个保持 8 年不败的冠军。这个程序向人们初步展示了机器学习的能力。 1962年， Hubel 和 Wiesel 发现猫脑皮层中独特的神经网络结构可以有效降低学习的复杂性， 从而提出著名的 Hubel－Wiesel 生物视觉模型， 以后提出的神经网络模型均受此启迪。 1969 年， 人工智能研究的先驱者 Marvin Minsky和 Seymour Papert 出版了对机器学习研究具有深远影响的著作《Perceptron》， 虽然提出的 XOR 问题把感知机研究送上了不归路，此后的十几年基于神经网络的人工智能研究进入低潮。 但其对于机器学习基本思想的论断（解决问题的算法能力和计算复杂性）却影响深远。 1980 年， 在美国卡内基·梅隆大学举行了第一届机器学习国际研讨会， 标志着机器学习研究在世界范围内兴起。1986 年， 《Machine Learning》创刊，标志着机器学习逐渐为世人瞩目并开始加速发展。 1982 年， Hopfield 发表了一篇关于神经网络模型的论文，构造出能量函数并把这一概念引入Hopfield 网络，同时通过对动力系统性质的认识， 实现了 Hopfield 网络的最优化求解， 推动了神经网络的深入研究和发展应用。 1986 年，Rumelhart、Hinton 和 Williams 联合在《自然》杂志发表了著名的反向传播算法(BP) ， 首次阐述了 BP 算法在浅层前向型神经网络模型的应用。这一算法不但明显降低了最优化问题求解的运算量，还通过增加一个隐层解决了感知器无法解决的 XOR Gate 难题。BP算法很快成为神经网络的最基本算法，从此神经网络的研究与应用开始复苏。 1989 年， 美国贝尔实验室学者 Yann LeCun 教授提出了目前最为流行的卷积神经网络( CNN) 计算模型，推导出基于 BP 算法的高效训练方法， 并成功地应用于英文手写体识别。CNN 是第一个被成功训练的人工神经网络，也是后来深度学习最成功、应用最广泛的模型之一。 90 年代后， 多种浅层机器学习模型相继问世，诸如逻辑回归、支持向量机等， 这些机器学习算法的共性是数学模型为凸代价函数的最优化问题，理论分析相对简单，训练方法也容易掌握，易于从训练样本中学习到内在模式，来完成对象识别、任务分类等初级智能工作。基于统计规律的浅层学习方法比起传统的基于规则的方法具备很多优越性， 取得了不少成功的商业应用的同时， 浅层学习的问题逐渐暴露出来，由于有限的样本和计算单元导致对数据间复杂函数的表示能力有限，学习能力不强，只能提取初级特征。 2006 年， 在学界及工业界巨大需求的刺激下， 计算机硬件技术迅速发展并为机器学习进行超强度的计算提供了可能。机器学习领域的泰斗 Geoffrey Hinton 和 Ruslan Salakhutdinov 发表文章，提出了深度学习模型， 主要论点包括：多个隐层的人工神经网络具有良好的特征学习能力；通过逐层初始化来克服训练的难度，实现网络整体调优。这个模型的提出开启了深度神经网络机器学习的新时代。 2012 年， Hinton 研究团队采用深度学习模型赢得计算机视觉领域最具影响力的 ImageNet 比赛冠军，从而标志着深度学习进入第二个阶段。 至今， 随着Hinton、LeCun 和 Andrew Ng 对深度学习的不断深入研究，以及云计算、大数据、计算机硬件技术的不断发展，深度学习许多领域逐渐取得了令人赞叹的进展，推出了一批成功的商业应用，诸如谷歌翻译、苹果语音工具 Siri、微软的 Cortana 个人语音助手、蚂蚁金服的 Smile to Pay 扫脸技术等。深度学习是目前最接近人类大脑分层的智能学习方法，它通过建立类似于人脑的分层结构模型，突破浅层学习的限制，能够表征更为复杂的函数关系。通过对输入数据逐层提取特征并加以抽象，建立从底层简单特征到高层抽象语义的非线性映射关系，机器学习在智能方向又获得了进一步提升，这成为了机器学习的一个里程碑。 机器学习应用前景2016年，举世瞩目的人机大战以 AlphaGo四比一李世石而告终。这一事件在震惊世人的同时，更让人感受到了机器学习的强大威力，昭示出机器学习研究与应用的灿烂前景。以此为契机， 机器学习理论研究势必成为一个新的热点，在认知计算、类脑计算的支撑下向更高阶段发展，出现性能更好、结构优化、学习高效、功能强大的机器模型，非监督机器学习也将会取得实质性的进展。机器学习的自主学习能力将进一步提高，逐渐跨越弱人工智能阶段，不断提高智能性。随着机器学习与大数据、云计算、物联网的深度融合，将会掀起一场新的数字化技术革命。借助自然语言理解、情感及行为理解将会开启更加友好的人机交互新界面；自动驾驶汽车将成为现实；我们的工作生活中将出现更多的智能机器人；在医疗、金融、教育等行业将能够给我们提供更多智能化、个性化服务定制服务；机器学习一定会造福于我们整个人类，使明天的生活更加美好! 自然语言处理与机器学习的联系自然语言处理和机器学习的联系语言是人类区别于其他动物的本质特性。在所有生物中，只有人类才具有语言能力，人类的多种智能都与语言有着密切的关系。人类的逻辑思维以语言为形式，人类的绝大部分知识也是以语言文字的形式记载和流传下来的。因而，语言无疑是人工智能（以机器学习和深度学习为代表）的一个重要，乃至核心部分。用自然语言与计算机进行通信，这是人们长久以来的梦想。实现人机间的自然语言通信意味着要使计算机既能理解自然语言文本的意义，也能以自然语言文本来表达给定的意图与思想。前者称为自然语言理解，后者称为自然语言生成，它们都是自然语言处理的重要组成部分。无论是实现自然语言理解，还是自然语言生成，事实上都远不如人们原先想象得那么简单。从现有的理论和技术看，通用的、高质量的自然语言处理系统仍然是较长期的努力目标。但是针对一定应用，具有相当自然语言处理能力的实用系统已经出现，甚至商品化、产业化。典型的例子有：多语种数据库和专家系统的自然语言接口、各种机器翻译系统、全文信息检索系统、自动文摘系统等。 现代NLP算法主要基于机器学习，特别是统计机器学习。机器学习范式不同于之前一般的尝试语言处理。语言处理任务的实现，通常涉及直接用手的大套规则编码。许多不同类的机器学习算法已应用于自然语言处理任务。这些算法的输入是一大组从输入数据中生成的“特征”。一些最早使用的算法如决策树，产生硬的if-then规则类似于手写的规则，是再普通不过的系统体系。然而，越来越多的研究集中于统计模型，这使得基于附加实数值的权重，每个输入要素柔软，概率的决策。此类模型能够表达许多不同的可能答案，而不仅仅只有一个相对的确定性。自然语言处理研究已逐渐从词汇语义转移到更进一步地叙事理解。然而要想达到人类的语言水平，这对人工智能而言尚是一个十分复杂的问题。自然语言处理与AI相互依赖、相互影响，二者的未来必将紧密结合起来。 寄语机器学习本身比较偏底层和理论，只有结合了具体的自然语言处理以及数据挖掘的问题才足够炫酷。机器学习好像内力一样，是一个武者的基础，而自然语言和数据挖掘的东西都是招式。如果你内功足够深厚，招式对你来说都是小意思。另外，机器学习还需要有足够的数学基础。现实生活中有很多学生一方面对机器学习爱得狂热，而另一方面却对矩阵、概率论等数学知识恨得深沉。其实，现在如果我们只讲工程实现，网络上有很多开源工具可供大家使用，你只需要照着攻略“依葫芦画瓢”就可以了。但我窃以为，如果你真的学不好矩阵、概率、微积分，不如早日勤动手、多编程。大家一定不要舍本逐末地忽视了程序员最基础的编程功夫，即使日后咱们玩不转机器学习，过硬的基本功也对日后工作研究十分有利！ 自然语言处理开发工具与环境Sublime Text和Anaconda介绍 Sublime Text简介 Sublime Text是一套跨平台的文本编辑器，支持基于Python的插件。Sublime Text 是专有软件，可通过包Package扩充本身的功能。大多数的包使用自由软件授权发布，并由社区建设维护。Sublime Text是由程序员Jon Skinner于2008年1月份所开发出来，它最初被设计为一个具有丰富扩展功能的Vim。其具有漂亮的用户界面和强大的功能，例如代码缩略图，Python的插件，代码段等。还可自定义键绑定，菜单和工具栏。Sublime Text 的主要功能包括：拼写检查，书签，完整的 Python API ， Goto 功能，即时项目切换，多选择，多窗口等等。Sublime Text 是一个跨平台的编辑器，同时支持Windows、Linux、Mac OS X等操作系统。 Sublime Text 支持众多编程语言，并支持语法上色。内置支持的编程语言包含：ActionScript、AppleScript、ASP、batch files、C、C++、C#、Clojure、CSS、D、Diff、Erlang、Go、Graphviz (DOT)、Groovy、Haskell、HTML、Java、JSP、JavaScript、JSON、LaTeX、Lisp、Lua、Makefiles、Markdown、MATLAB、Objective-C、OCaml、Perl、PHP、Python、R、Rails、Regular Expressions、reStructuredText、Ruby、Scala、shell scripts (Bash)、SQL、Tcl、Textile、XML、XSL 和 YAML。用户可通过下载外挂支持更多的编程语言。 Sublime Text优点 主流前端开发编辑器 体积较小，运行速度快 文本功能强大 支持编译功能且可在控制台看到输出 内嵌python解释器支持插件开发以达到可扩展目的 Package Control：ST支持的大量插件可通过其进行管理 Anaconda简介 Anaconda是一个用于科学计算的Python发行版，支持 Linux, Mac, Windows系统，提供了包管理与环境管理的功能，可以很方便地解决多版本python并存、切换以及各种第三方包安装问题。Anaconda利用工具/命令conda来进行package和environment的管理，并且已经包含了Python和相关的配套工具。这里先解释下conda、anaconda这些概念的差别。conda可以理解为一个工具，也是一个可执行命令，其核心功能是包管理与环境管理。包管理与pip的使用类似，环境管理则允许用户方便地安装不同版本的python并可以快速切换。Anaconda则是一个打包的集合，里面预装好了conda、某个版本的python、众多packages、科学计算工具等等，所以也称为Python的一种发行版。其实还有Miniconda，顾名思义，它只包含最基本的内容——python与conda，以及相关的必须依赖项，对于空间要求严格的用户，Miniconda是一个不错的选择。 开发环境安装与配置 工具包的准备 本文主要介绍Windows下的安装配置，关于Linux和MacOS下的安装，后文给出了扩展文章，需要的读者可自行在线下载。Sublime Text3安装包下载地址（http://www.sublimetext.com/3）。点击该网址进入Sublime主页，根据本机操作系统选择相关工具包下载。本文示范下载Windows64 bit。具体详见下图1-1： ![](https://i.imgur.com/MUCnx5E.png) 图1-1 Sublime Text3下载页面 Anaconda安装包下载地址（https://www.anaconda.com/download/）进入下载页面显示Python3.0以上版本和Python2.0以上版本。关于Python3和Python2的区别请访问（http://www.runoob.com/python/python-2x-3x.html）查看，在此不做赘述。一般推荐下载Python3.0以上版本。具体详见图1-2所示： ![](https://i.imgur.com/ZBzco0P.png) 图1-2 Anaconda下载页面 > Anaconda安装 (1) 安装Anaconda集成环境，将下载后的Anaconda包双击打开如图1-3所示： ![](https://i.imgur.com/Xl3Vmg0.png) 图1-3 Anaconda安装页面 (2) 然后一直“Next”下去，直到完成配置。（环境变量自动配置），配置完成后，查看是否成功。打开主菜单->所有应用查看安装，如图1-4所示： ![](https://i.imgur.com/0oh6nE1.png) 图1-4 主菜单Anaconda完成界面 (3) 打开cmd进入dos命令下，输入conda list 查看集成的python包。如图1-5所示： ![](https://i.imgur.com/DaLA98q.png) 图1-5 查看Python集成包 (4) 如果想添加新的python包，打开Anaconda官网：https://anaconda.org/search进行查找，比如想找到机器学习工具包scikit-learn如图1-6所示： ![](https://i.imgur.com/r73tT7n.png) 图1-6 下载scikit-learn包 至此我们就完成了Anaconda安装配置工作，以及对包文件的自定义下载安装。需要注明的是Anaconda自身集成了Python、pip、nltk、numpy、matplotlib等一系列常用包。现在，我们已经可以对python进行操作了。考虑到熟悉python开发的人员，常用Pycharm开发工具，熟悉java的开发人员常用Eclipse开发工具，熟悉C#的开发人员常用VS开发工具。然后我们将Anaconda集成到PyDev、Pycharm、Eclipse、VS等编译环境即可，诸如此类就不一一列举了。考虑到新学一种语言要重新学习一种编程环境，这样极其不方便。那么能不能找到一款编程工具可以通用以上语言？或许这样还不够，如果它还能跨Linux、Windows、MacOS那就更好了。本书强烈推荐的Sublime跨平台跨语言编辑器事实上就是这样一款强大的工具。我们接下来唯一要做的，就是将Anaconda集成到sublime中就可以了。 扩展：linux和MacOS安装教程请访问（https://docs.continuum.io/anaconda/install/）。 > Sublime Text3 安装 (1) 将下载好的Sublime Text3工具包双击到如下界面：如图1-7所示： ![](https://i.imgur.com/jIUG68H.png) 图1-7 Sublime Text3安装界面 (2) 一直执行“Next”一路安装即可，中间保存路径可以自定义。最终安装成功将如图1-8所示： ![](https://i.imgur.com/9gZEPNn.png) 图1-8 Sublime Text3安装成功 (3) 安装插件Package Control。打开（ https://packagecontrol.io/installation）复制Sublime Text3中的代码如图1-9所示： ![](https://i.imgur.com/yeIK9sK.png) 图1-9 Package Control代码 (4) 点击“Ctrl+`”，将3中文本代码内容复制粘贴到文本框中，按Enter即可。如图1-10所示： ![](https://i.imgur.com/0wOqdIU.png) 图1-10 安装Package Control (5) 成功安装后，在Sublime Text3中同时按住“Ctrl+Shift+P”键盘。最终安装成功：如图1-11所示： ![](https://i.imgur.com/K0ELBU9.png) 图1-11 成功安装Package Control (6) 点击“Packeage Control:Install Package”进入查找python环境配置插件“SublimeREPL”，下载安装完成后，点击“Preferences->Browse Package...”查看安装的包如图1-12所示： ![](https://i.imgur.com/l6h3uYW.png) 图1-12 查看安装插件 (7) 自定义快捷键盘配置：打开Preferences > Key Bindings输入如下代码，F5运行程序，F6切换IDEL工具，Ctrl+D自定义删除行，其他快捷键是通用的，网上有很多快捷键的资料，这里不赘述。 [ { "keys": ["f5"], "caption": "SublimeREPL: Python - RUN current file", "command": "run_existing_window_command", "args": { "id": "repl_python_run", "file": "config/Python/Main.sublime-menu" } }, { "keys": ["f6"], "caption": "SublimeREPL: Python", "command": "run_existing_window_command", "args": { "id": "repl_python", "file": "config/Python/Main.sublime-menu" } },{ "keys": ["ctrl+d"], "command":"run_macro_file", "args": {"file":"res://Packages/Default/Delete Line.sublime-macro"} } ] 至此完成了Sublime Text3安装配置工作，详细插件安装参考网址（[http://www.open-open.com/news/view/26d731](http://www.open-open.com/news/view/26d731)），快捷键使用请查看（[https://segmentfault.com/a/1190000004463984](https://segmentfault.com/a/1190000004463984)）。还有问题的读者可以自行上网检索，由于资料较多且比较容易实现，本书不再详写。 # 实战：第一个小程序的诞生 ## 实例介绍 > 编写一个可以智能数据计算的小程序，用户输入公式如“10/(2+3)”，自动提示计算结果。 ## 源码实现 本实例设计思路如下： - 如下①中采用def定义函数名，python不采用花括号，而是用冒号代替代码块，形参中param是一个自动识别类型的参数。 - 如下②中基本的计算公式，记住结尾没有分号 - 如下③是对结果的输入。 - 如下④类似于C、C#、Java中的主函数，后面章节会项目介绍。 - 如下⑤是对函数名的调用，并且直接传递列表参数，暂时不理解也没有关系，详见第二章。 源码如下（源码下载见https://github.com/BaiNingchao/NLP&ML/01chapter/1.1py）： def countNum(param): ① result = param[0]/(param[1]+param[2]) ② print("this count: "+str(result)) ③ if __name__=="__main__": ④ countNum([10,2,3]) ⑤ 运行结果如1-13所示： ![](https://i.imgur.com/YXTvFgr.png) 图1-13 第一个小程序 参考文献 数据挖掘十大算法：https://wizardforcel.gitbooks.io/dm-algo-top10/content/apriori.html 中文维基百科：https://zh.wikipedia.org/wiki/%E5%85%88%E9%AA%8C%E7%AE%97%E6%B3%95 GitHub：https://github.com/BaiNingchao/MachineLearning-1 图书：《机器学习实战》 图书：《自然语言处理理论与实战》 完整代码下载 源码请进【机器学习和自然语言QQ群：436303759】文件下载： 作者声明 本文版权归作者所有，旨在技术交流使用。未经作者同意禁止转载，转载后需在文章页面明显位置给出原文连接，否则相关责任自行承担。]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>自然语言处理</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[漫谈马尔可夫模型]]></title>
    <url>%2F2019%2F02%2F13%2F%E6%BC%AB%E8%B0%88%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[摘要：最早接触马尔可夫模型的定义源于吴军先生《数学之美》一书，起初觉得深奥难懂且没什么用场。直到学习自然语言处理时，我才真正使用到隐马尔可夫模型，并体会到此模型的奇妙之处。马尔可夫模型在处理序列分类时具有强大的功能，诸如解决：词类标注、语音识别、句子切分、字素音位转换、局部句法剖析、语块分析、命名实体识别、信息抽取等。此外它还广泛应用于自然科学、工程技术、生物科技、公用事业、信道编码等多个领域。（本文原创，转载必须注明出处.） 马尔可夫链马尔可夫简介安德烈·马尔可夫，俄罗斯人，物理-数学博士、圣彼得堡科学院院士、彼得堡数学学派的代表人物，以数论和概率论方面的工作著称，他的主要著作有《概率演算》等。1878年荣获金质奖章，1905年被授予功勋教授称号。在数论方面，他研究了连分数和二次不定式理论 ，还解决了其他多难题 。在概率论中，他发展了矩阵法，扩大了大数律和中心极限定理的应用范围。马尔可夫最重要的工作是在1906到1912年间，提出并研究了一种能用数学分析方法研究自然过程的一般图式——马尔可夫链，同时开创了对一种无后效性的随机过程——马尔可夫过程的研究。马尔可夫经多次观察试验发现，一个系统的状态转换过程中第n次转换获得的状态常取决于前一次（第（n-1）次）试验的结果。马尔可夫进行深入研究后指出：对于一个系统，在由一个状态转至另一个状态的过程中存在着转移概率；并且这种转移概率可以依据其紧接的前一种状态推算出来，与该系统的原始状态和此次转移前的马尔可夫过程无关。马尔可夫链的理论与方法在现代已经被广泛应用于自然科学、工程技术和公用事业中。 马尔可夫链的基本概念 序列分类器 序列分类器或序列标号器是给序列中的某个单元指派类或者标号的模型。马尔可夫模型（又叫显马尔可夫模型VMM）和隐马尔可夫模型（HMM）都是序列分类器。诸如词类标注、语音识别、句子切分、字素音位转换、局部句法剖析、语块分析、命名实体识别、信息抽取等都属于序列分类。 随机过程的两层含义 随机过程是一个时间函数，其随着时间变化而变化； 随机过程在每个时刻上的函数值是不确定的、随机的，即每个时刻上函数值按照一定的概率进行分布。 独立链 随机过程中各个语言符号或者词是独立的、不相互影响，则称这种链是独立链。反之，各语言词或者符号彼此有关则是非独立链。 等概率独立链与非等概率独立链 在独立链中，各个语言符号或者词是等概率出现的是等概率独立链，各个语言词或者语言符号是非等概率出现的则为非等概率链。 马尔可夫链 马尔可夫过程：在独立链中，前面语言符号对后面的语言符号无影响，是无记忆没有后效的随机过程。在已知当前状态下，过程的未来状态与它的过去状态无关，这种形式就是马尔可夫过程。 马尔可夫链：在随机过程中，每个语言符号的出现概率不相互独立，每个随机试验的当前状态依赖于此前状态，这种链就是马尔可夫链。 链的解析：链可以当做一种观察序列，诸如：“2016年是建党95周年”，就可以看成是一个字符串链。其中如果上述字符串中每个字符出现是随机、独立的就是独立链，如果每个字符出现与前面字符相关，即不独立并具有依赖性，我们称之为马尔可夫链。 N元马尔可夫链： 考虑前一个语言符号对后一个语言符号出现概率的影响，这样得出的语言成分的链叫做一重马尔可夫链，也是二元语法。 考虑前两个语言符号对后一个语言符号出现概率的影响，这样得出的语言成分的链叫做二重马尔可夫链，也是三元语法。 考虑前三个语言符号对后一个语言符号出现概率的影响，这样得出的语言成分的链叫做三重马尔可夫链，也是四元语法。 类似的，考虑前（4,5，….,N-1）个语言符号对后一个语言符号出现概率的影响，这样得出的语言成分的链叫做（4,5，….,N-1）重马尔可夫链，也是（5，6，….,N）元语法。 有限自动机 马尔可夫链在数学上描述了自然语言句子的生成过程，是一个自然语言形式的早期模型。后来N元语法的研究，都是建立在马尔可夫模型的基础之上的。马尔可夫链和隐马尔可夫模型都是有限自动机（状态集合状态之间的转移集）的扩充。 加权有限状态机 加权有限状态机中每个弧与一个概率有关，这个概率说明通过这个弧的可能性，且某一个点出发的弧具有归一化的性质，即某点出发的弧概率之和为1。 注意：马尔可夫链不能表示固有歧义的问题，当概率指派没有歧义时，马尔可夫链才有用。 马尔可夫链描述 (1) 具有初始状态和终结状态的马尔可夫链描述如下表11-1所示： 表11-1 具有初始状态和终结状态的马尔可夫链 ![](https://i.imgur.com/tnWnZoZ.png) (2) 没有初始状态和终结状态的马尔可夫链描述如下表11-2所示： 表11-2 没有初始状态和终结状态的马尔可夫链 ![](https://i.imgur.com/rR0H9sZ.png) 在一个一阶马尔可夫链中，我们假设一个特定的概率只与它前面一个状态有关，马尔可夫假设可以表示如下： 从一个状态i出发的所有弧的概率之和为1，即： 隐马尔可夫模型形式化描述 爱依斯讷（Jason Eisner）对隐马尔可夫模型的描述 隐马尔可夫模型即一个隐藏马尔可夫链随机生成不可观察的随机序列，再由各个状态生成一个观测随机序列的过程。其中隐藏马尔可夫链随机生成的状态序列，称为状态序列；每个状态生成一个观测的随机序列，称为观测序列。序列的每个位置可以看成一个时刻，它可以被形式化的描述为一个五元组HMM=。Jason Eisner对隐马尔可夫模型的描述如下表11-3所示： 表11-3 HMM形式化描述 ![](https://i.imgur.com/UJ2RQbw.png) 拉宾纳（Rabiner）关于隐马尔可夫模型思想的三个问题 问题1（似然度问题）：给一个HMM λ=（A,B） 和一个观察序列O，确定观察序列的似然度问题 P(O|λ) 。 问题2（解码问题）：给定一个观察序列O和一个HMM λ=（A,B），找出最好的隐藏状态序列Q。 问题3（学习问题）：给定一个观察序列O和一个HMM中的状态集合，自动学习HMM的参数A和B。 根据隐马尔可夫模型的描述，可以将一个长度为T的观测序列的生成过程描述为下表11-4所示的算法： 表11-4 隐马尔可夫模型算法描述 ![](https://i.imgur.com/HIn78B2.png) 综上所述，一个已知的HMM模型由参数(N，T)、观测序列和三个概率(A，B，π )构成，下文皆用 =(A，B， π )表示完整HMM模型参数。 数学形式描述在命名实体识别过程中，序列标注是一个重要的子任务。利用HMM模型去解决类似序列标注这样的问题，给定一个观测序列 ，为求得最佳标记序列为 ，要求条件概率 最大。由朴素贝叶斯公式可知： 在命名实体识别中， X 是给定的一个文本序列，文本粒度可以是一个句子或者一段话，待观测的值 到 为词，而P(X)对于所有类别都是一样的，可以看成是一个常量从而忽略不考虑。则上式可以简化为： 隐马尔可夫模型本质上就是求解联合概率。则由上式可得： 其中 给出了非独立假设情况下的求解命名实体识别的概率模型。命名实体识别任务就转化为求解 ，期望P(Y|X)值最大。 公式假设理想状态下的命名实体识别的概率模型，实际这种参数估计是不科学的。故而，我们给出上文提到两个独立假设： (1) 马尔可夫假设 假设求解状态只是跟它前面的状态N-1有关即： (2) 输出独立假设 一个输出观察 的概率只与产生该观察的状态 有关，而与其他状态无关即： 可知一阶HMM公式可以化简为： 其中， 为发射概率， 为转移概率。 向前算法解决HMM似然度向前算法定义向前算法的递归定义如表11-5所示： 表11-5 向前算法形式化描述 ![](https://i.imgur.com/whHZoBx.png) 向前算法原理向前算法解决：问题1（似然度问题）：给一个HMM λ=（A,B） 和一个观察序列O，确定观察序列的似然度问题 P=(O|λ) 。对于马尔可夫链，表面观察和实际隐藏是相同的，只需要标记吃冰淇淋数目“3 1 3”的状态，并把加权（弧上）的对应概率相乘即可。而隐马尔可夫模型就不那么简单了，因为状态是隐藏的，所以我们并不知道隐藏的状态序列是什么。 简化下问题：假如我们知道天气冷热状况，并且知道小明吃冰淇淋的数量，然后我们去观察序列似然度。如：对于给定的隐藏状态序列“hot hot cold”我们来计算观察序列“3 1 3”的输出似然度。 如何进行计算？首先，隐马尔可夫模型中，每个隐藏状态只产生一个单独的观察即一一映射。隐藏状态序列与观察序列长度相同，即：给定这种一对一的映射以及马尔可夫假设，对于一个特定隐藏状态序列 以及一个观察序列 观察序列的似然度为： 故从隐藏状态“hot hot cold”到所吃冰淇淋观察序列“3 1 3”的向前概率为： P(3 1 3|hot hot cold)=P(3|hot)P(1|hot)P(3|cold)=0.40.20.1=0.008 实际上，隐藏状态序列“hot hot cold”是我们的假设，因为我们并不知道隐藏状态序列，我们要考虑的是所有可能的天气序列。如此一来，我们将计算所有可能的联合概率，但这样的计算特别复杂。我们来计算天气序列Q生产一个特定的冰淇淋事件序列O的联合概率： 如果隐藏序列只有一个是“hot hot cold”，那么我们的冰淇淋观察“3 1 3”和一个可能的隐藏状态“hot hot cold”的联合概率为： P(313|hot hot cold)=P(hot|start)P(hot|hot)P(hot|cold)P(3|hot)P(1|hot)P(3|cold)=0.80.70.30.40.2*0.1=0.001344 P(3 1 3)= P(313| cold cold cold)+ P(313| cold cold hot) + P(313| hot hot cold ) + P(313| cold hot cold) + P(313|hot cold cold) + P(313| hot hot hot) + P(313| hot cold hot) + P(313| cold hot hot) 如果我们有N个隐藏状态和T个观察序列，那么之后我们将得到 个可能隐藏序列。在实际中T往往很大，比如文本处理中可能有数万几十万个词汇，计算量将是指数上升的。在隐含马尔可夫模型中有种向前算法可以有效代替这种指数增长的复杂算法，大大降低复杂度。实验证明向前算法的复杂度是 。 现实应用：预测成都天气冷热向前算法是一种动态规划算法，当得到观察序列的概率时，它使用的是一个表来存储中间值。向前算法也使用对于生成观察序列，所有可能的隐藏状态路径上的概率求和方法来计算观察概率。在向前算法中横向表示观察序列，纵向表示状态序列。对于给定的隐藏状态序列“hot hot cold”计算观察序列“3 13 ”的似然度的向前网格的例子。 每个单元 表示对于给定的自动机λ，在前面t个观察之后，在状态j的概率： ，其中 表示第t个状态是状态j的概率。如：表示状态1即数3时，q1的概率。 上面公式的3个因素如表11-6所示： 表11-6 公式参数解释 ![](https://i.imgur.com/fysvysp.png) 向前网格如下图11-1 所示： ![](https://i.imgur.com/rI3TSWn.png) 图11-1 计算成都天气冷热事件“3 1 3”的向前网络 (1) 在时间1和状态1的向前概率： 从状态cold开始吃3根冰淇淋的似然度0.02 (2) 在时间1和状态2的向前概率： 从状态hot始吃3根冰淇淋的似然度0.32 (3) 在时间2和状态1的向前概率： 从开始到cold再到cold以及从开始到hot再到cold的天气状态，吃冰淇淋3 1 的观察似然度0.54 (4) 在时间2和状态2的向前概率： 从开始到cold再到hot以及从开始到hot再到hot的天气状态，吃冰淇淋3 1 的观察似然度0.0464 用同样方法，我们可以计算时间步3和状态步1的向前概率以及时间步3和状态步2的概率等等，以此类推，直到结束。显而易见，使用向前算法来计算观察似然度可以表示局部观察似然度。这种局部观察似然度比使用联合概率表示的全局观察似然度更有用。 文本序列标注案例：Viterbi算法Viterbi算法给定HMM 和观察序列 ，求解最大概率的状态序列 就是Viterbi算法解码问题。采用维特比算法求解隐藏序列后面的最大序列，针对可能的每一个隐藏状态序列，运用向前算法求解最大似然度的隐藏状态序列，从而完成解码工作，算法复杂度 ，状态序列很大时呈现指数级别增长，造成过大的计算量，由此采用一种动态规划Viterbi算法降低算法复杂度。 按照观察序列由左向右顺序，每个 表示自动机λ，HMM观察了前t个状态之后，每个 的值递归计算，并找出最大路径。Viterbi算法如表11-7所示： 表11-7 Viterbi算法 ![](https://i.imgur.com/u48oIC0.png) 维特比算法实例解读例子：通过吃冰淇淋的数量（观察序列状态）计算隐藏状态空间的最佳路径（维特比网格）如下图11-2所示: ![](https://i.imgur.com/AF5oPAi.png) 图11-2 计算隐藏状态空间的最佳路径网络图 其中： 圆圈：隐藏状态 方框：观察状态 虚线圆圈：非法转移 虚线：计算路径 q：隐藏空间状态 o：观察时间序列状态 ：在时间步t的观察状态下，隐藏状态j的概率。 给定一个HMMλ=（A,B），HMM把最大似然度指派给观察序列，算法返回状态路径，从而找到最优的隐藏状态序列。上图是小明夏天吃冰淇淋‘3 1 3’，据此使用Viterbi算法推断最可能出现的天气状态（天气的热|冷）。 1）计算时间步1的维特比概率 计算时间步t=1和状态1的概率： 路径：start—C 计算时间步t=1和状态2的概率： 路径：start—H 较大 2）计算时间步2的维特比概率，在（1） 基础计算 计算时间步t=2和状态1的概率： 路径:start—H—C 较大 计算时间步t=2和状态2的概率： 路径：start—H—H 3）计算时间步3的维特比概率，在（2） 基础计算 计算时间步t=3和状态1的概率： 路径:start—H—C —C 计算时间步t=3和状态2的概率： 路径：start—H—C—H 较大 4）维特比反向追踪路径 路径为：start—H—C—-H ![](https://i.imgur.com/6EvA8Dt.png) 图11-3 维特比反向追踪路径图 综上所述可知：利用Viterbi算法我们可以通过小明夏季某天吃冰淇淋的观察值（3 1 3）推断出天气为（热 冷 热）。其实这个结果是符合客观事实的，人们天热吃3根冰淇淋，天冷吃一根。 参考文献 数据挖掘十大算法：https://wizardforcel.gitbooks.io/dm-algo-top10/content/apriori.html 中文维基百科：https://zh.wikipedia.org/wiki/%E5%85%88%E9%AA%8C%E7%AE%97%E6%B3%95 GitHub：https://github.com/BaiNingchao/MachineLearning-1 图书：《机器学习实战》 图书：《自然语言处理理论与实战》 完整代码下载 源码请进【机器学习和自然语言QQ群：436303759】文件下载： 作者声明 本文版权归作者所有，旨在技术交流使用。未经作者同意禁止转载，转载后需在文章页面明显位置给出原文连接，否则相关责任自行承担。]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>马尔可夫模型</tag>
        <tag>隐马尔可夫模型</tag>
        <tag>HMM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[漫谈条件随机场]]></title>
    <url>%2F2019%2F02%2F13%2F%E6%BC%AB%E8%B0%88%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA%2F</url>
    <content type="text"><![CDATA[摘要：条件随机场常用于序列标注、数据分割等自然语言处理任务中，此外其在中文分词、中文人名识别和歧义消解等任务中也有应用。本文基于笔者在做语句识别序列标注过程中，对条件随机场产生的了解。全篇内容主要源于自然语言处理、机器学习、统计学习方法和部分网上资料对CRF的相关介绍，最后由笔者进行大量研究整理后汇总成体系知识。本章首先介绍条件随机场的相关概念，然后结合实例以期让读者深入理解条件随机场的应用。（本文原创，转载必须注明出处.） 条件随机场介绍条件随机场（Condition Random Fields），简称CRF条件随机场概念：条件随机场就是对给定的输出标识序列Y和观察序列X，通过定义条件概率P(X|Y)，而不是联合概率分布P(X,Y)来描述模型。 概念解析： 标注一篇文章中的句子，即语句标注。使用标注方法BIO标注，B代表句子的开始，I代表句子中间，O代表句子结束。观察序列X就是一个语料库（此处假设一篇文章，x代表文章中的每一句，X是x的集合），标识序列Y是BIO，即对应X序列的识别。从而我们可以根据条件概率P(标注|句子)，推测出正确的句子标注。显然，这里我们针对的是序列状态，即CRF是用来标注或划分序列结构数据的概率化结构模型的。CRF在自然语言处理和图像处理领域都得到了广泛的应用，我们可以把它看作是无向图模型或者马尔可夫随机场。 生产式模型与判别式模型有监督机器学习方法可以分为生成方法和判别方法： 生产式模型：直接对联合分布进行建模，如：混合高斯模型、隐马尔科夫模型、马尔科夫随机场等 判别式模型：对条件分布进行建模，如：条件随机场、支持向量机、逻辑回归等 生成模型优缺点介绍优点： 生成给出的是联合分布，不仅能够由联合分布计算条件分布（反之则不行），还可以给出其他信息。如果一个输入样本的边缘分布很小的话，那么可以认为学习出的这个模型可能不太适合对这个样本进行分类，分类效果可能会不好。 生成模型收敛速度比较快，即当样本数量较多时，生成模型能更快地收敛于真实模型。 生成模型能够应付存在隐变量的情况，比如混合高斯模型就是含有隐变量的生成方法。 缺点： 天下没有免费的午餐，联合分布是能提供更多的信息，但也需要更多的样本与计算；尤其是为了更准确估计类别条件分布，需要增加样本的数目。而且类别条件概率的许多信息是我们做分类所用不到的，因而如果我们只需要做分类任务，就浪费了计算资源。 另外，实践中多数情况下判别模型效果更好。 判别模型优缺点介绍优点： 与生成模型缺点对应，首先是节省计算资源。另外，需要的样本数量也少于生成模型。 准确率往往较生成模型高。 由于直接学习，而不需要求解类别条件概率，所以允许我们对输入进行抽象（比如降维、构造等），从而能够简化学习问题。 缺点： 没有生成模型的上述优点。 简单易懂的条件随机场CRF的形式化表示设G=(V,E)为一个无向图，V为结点的集合，E为无向边的集合。，即V中的每个结点对应一个随机变量Yv，其取值范围为可能的标记集合{Y}。如果观察序列X为条件，每一个随机变量都满足以下马尔可夫特性: 其中，w–v表示两个结点在图G中是邻近结点，（X,Y）为条件随机变量。 以语句识别的案例理解条件随机场的形式化表示，如图12-1所示。 ![](https://i.imgur.com/BYhLVKV.png) 图12-1 语句识别无向图 &nbsp;G=（V,E表示识别语句：【我爱中国】的标注是一个无向图，X为观察序列，Y为标注序列，V是每个标注状态的结点。E的无向边，边上的权值为概率值。表示每个X的Y的标注，如:X1:我，y1：O，y2：I，y3：B；取值范围，而中w—v表示我与爱是相邻的结点。这样的（X,Y）为一个条件随机场，真正的标注再采用Viterbi算法，如：寻求最大概率即，记录下我的标注路径，同理可知： ![](https://i.imgur.com/SJDdq0q.png) 图12-2 标注流程图 &lt;/p&gt;如上便是对条件随机场与Viterbi算法的综合运用，其中Viterbi标注问题本质是隐马尔可夫模型三大问题之解码问题的算法模型，具体参考（揭秘马尔科夫模型系列文章） CRF的公式化表示在给定的观察序列X时，某个待定标记序列Y的概率可以定义为其中是转移概率；是状态函数，表示观察序列X其中i的位置的标记概率；和分别是t和s的权重，需要从训练样本中估计出来。实例解析：我爱中国，其中x2是爱字。表示在观察状态2中，我到爱的转移概率；其中j∈{B,I,O}，可知的生成概率或者发射概率的特征函数。用观察序列{0,1}二值特征b（x,i）来表示训练样本中某些分布特征，其中采用{0,1}二值特征即符合条件标为1，反之为0；如图12-3所示。 ![](https://i.imgur.com/hO5EN2y.png) 图12-3 发射函数实例解析图 转移函数定义如图12-4所示： ![](https://i.imgur.com/gQz9WFG.png) 图12-4 转移函数实例解析图 &lt;/p&gt;为了便于描述，可以将状态函数书写以下形式：&nbsp;特征函数：&nbsp;其中每个局部特征表示状态特征，或者专业函数，由此条件随机场的定义条件概率如下：，其中分母为归一化因子：&nbsp; 深度理解条件随机场理论上标记序列描述一定条件的独立性，G图结构是任意的，对序列进行建模可形成最简单、最普通的链式结构图。结点对应标记序列X中元素，CRF链式图如21-5所示： ![](https://i.imgur.com/qObpYgD.png) 图12-5 CRF链式结构图 如果上图两种表示是一致的，其中图链式句子标注是图链式2的实例化，那么有的读者可能会问为什么上面图是这种而不是广义的图。其实这是因为观察序列X的元素之间并不存在图结构，没有做独立性假设，这点非常容易理解。诸如图中“我爱中国”，其中b表示反射概率而t是转移概率，线上的数值表示权值即概率值。如图3，我的发射概率0.7，我到爱的转移概率0.5。通俗讲，我和爱两个字是有关联的，而非独立的。 参考文献 数据挖掘十大算法：https://wizardforcel.gitbooks.io/dm-algo-top10/content/apriori.html 中文维基百科：https://zh.wikipedia.org/wiki/%E5%85%88%E9%AA%8C%E7%AE%97%E6%B3%95 GitHub：https://github.com/BaiNingchao/MachineLearning-1 图书：《机器学习实战》 图书：《自然语言处理理论与实战》 完整代码下载 源码请进【机器学习和自然语言QQ群：436303759】文件下载： 作者声明 本文版权归作者所有，旨在技术交流使用。未经作者同意禁止转载，转载后需在文章页面明显位置给出原文连接，否则相关责任自行承担。]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>条件随机场</tag>
        <tag>CRF</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习之模型评估]]></title>
    <url>%2F2019%2F02%2F13%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%2F</url>
    <content type="text"><![CDATA[摘要：本文写作的初衷源于基于HMM模型序列标注的一个实验，在实验完成之后，如果迫切想知道采用的序列标注模型好坏，有哪些指标可以度量。于是，就产生了对这一专题进度的学习总结，这样也便于其他人参考，节约大家的时间。本文依旧旨在简明扼要地梳理出模型评估核心指标，关键以期达到实用的目的。本章首先介绍基于统计角度的模型评估，然后介绍模型评估的方法，最后对模型选择进行介绍。（本文原创，转载必须注明出处.） 统计角度介绍模型概念算法模型 概念简述 李航《统计学习方法》一书：统计学习方法是由模型、策略和算法构成的，即统计学习方法的三要素构成，简化：方法=模型+策略+算法 维基百科对数学模型描述：数学模型是对所描述的对象用数学语言所作出的描述和处理。 百度百科对策略描述：策略式学习是一项复杂的智能活动，学习过程与推理过程是紧密相连的。按照学习中使用推理的多少，机器学习所采用的策略大体上可分为4种——机械学习、通过传授学习、类比学习和通过事例学习。学习中所用的推理越多，系统的能力越强。 单从定义看，不免让人一头雾水，究竟何为模型？还是没有明确的概念。下文将以数学描述以及形式化阐述来解决这个问题。首先解决了什么是模型，咱们才能进行模型好坏指标的评价，进而选择适合的学习模型。 模型：所有学习的条件概率分布或者决策函数。 模型的假设空间：包含所有有可能的条件概率分布或者决策函数。 实例解析 假设决策函数是输入变量的线性函数，模型的假设空间就是这些线性函数构成的函数集合，假设空间中的模型一般为无穷多个。【现实应用：假设解决序列词性标注的的函数模型M，模型假设空间中由不同参数构成的M模型。（不是很严谨，辅助理解。）】 形式化表示：假设空间F表示，假设空间可以为决策函数的集合； x，y在输入空间X和输出空间Y上的变量，这时F通常由一个参数向量决定的函数族表示； 参数向量θ取值于n维欧氏空间 ，称为参数空间。 假设空间也可以定义为条件概率集合： 其中x和y是定义在输入空间X和Y上的随机变量，这时F通常是一个参数向量决定的条件概率分布族； 参数向量θ取值于n维欧氏空间，称为参数空间。 注意：由决策函数表示的模型为非概率模型（如上述F函数），由条件概率表示的模型为概率模型（如上述y=f(x)函数）。 模型评估和模型选择 训练误差和测试误差 好的模型的特征：对已知数据和未知数据都有很好的预测能力。 学习方法评估标准：基于损失函数的模型训练误差和测试误差为指标。 假设学习到的模型&nbsp;,训练误差是模型&nbsp;关于训练数据集的平均损失：&lt;/p&gt;&nbsp;其中N是训练样本容量。测试误差是模型关于测试数据集的平均损失：&nbsp;其中N’是训练样本容量。 实例 若损失函数是0-1损失的时候（0-1损失参考具体相关知识），测试误差就变成了常见的测试数据集上的误差率。，这里的I是指示函数，即&nbsp;时为1，否则为0。相应的，常见的测试数据集上的准确率是：，这里的I是指示函数，即&nbsp;时为1，否则为0。显然：&nbsp;&nbsp; 实例解析 根据误差率和准确率可知，测试误差反映了学习方法对未知测试数据集的预测能力。测试误差小的方法具有很好的预测能力，能更有效地预测。通常将学习方法对未知数据的预测能力称为泛化能力。 由此我们可以应用到现实的NLP模型中。诸如分类模型，当测试误差更小的时候，分类更加准确；聚类模型中，当测试误差较小时候，聚类效果更好等等。因此，我们为了追求较小的测试误差，就要在训练上下功夫。但是，一味苛求训练效果好、训练误差小，以至于所选择的模型复杂度非常高，这样的低训练误差就一定能换回好的预测吗？事实上这就容易出现过拟合现象，如何避免过拟合选择更好的模型？下节继续。 过拟合与欠拟合的模型选择 何时进行模型选择？ 当假设空间含有不同复杂度（如：不同的参数个数）的模型时，就要面临模型选择问题，以期我们表达出的模型与真实的模型（参数个数）相同或相近。 过拟合：一味追求提高对训练数据的预测能力，所选择模型的复杂度往往比真实模型高，此现象就是过拟合。 过拟合指学习时选择的模型包含的参数过多，以至于出现模型对现有数据预测得好，但是对未知数据预测能力较差。模型选择准则是在避免过拟合的前提下尽可能地去提高模型预测的能力。 实例 给出一个训练数据集&nbsp;&nbsp;和一个M次多项式的模型&nbsp;完成下面10个数据点的拟合。 是输入序列集x的观察值是输出序列集y的观察值M 次项式是解决问题的模型W为参数解决如上问题按照经验风险最小化策略求解参数即可，即数学表达：&nbsp;损失函数为平方损失，1/2是便于计算。然后将模型公式和训练数据代入风险最小化公式：，最后采用最小二乘法（过程略）求解。 实例分析 如上图给出M=0,1,3,9的多项式函数的拟合情况。当M=0时，多项式变成了一个常数，数据拟合表现很差；当M=1时，多项式曲线为一条直线，拟合依旧差；当M=9时，多项式通过每一个点，训练误差0。从训练数据拟合角度分析，M=9时效果最好，但是训练数据本身存在很多噪音，对未来数据预测能力差，达不到预期的效果。这就是过拟合，虽然训练数据表现好但是未知数据预测差。当M=3时，多项式曲线对训练数据拟合效果比较好，对未知数据拟合也很好，其模型也简单，可以选择。总结：模型选择时，不仅仅考虑对已知数据的预测能力，还有考虑对未知数据的预测能力。训练误差和测试误差与模型复杂度的关系如下图13-1所示： ![](https://i.imgur.com/ME9FyuT.jpg) 图13-1 训练误差和测试误差与模型复杂度对比图 可知，当模型复杂度增大的时候，训练误差会逐渐减小并趋近于0，而测试误差会先减小到最小值后又反向增大。当选择模型复杂度过大时，过拟合问题就会出现。 补充 以下提供NLP序列句子识别标注实例，用以更好理解本节。（以下实例以帮助读者理解为旨要，假设可能不是特别严谨）训练数据集T={(南 B),(海 I),(是 I), (中 I),(国 I),(领 I), (土 I),(。 O) }未知数据P={(不 B), (容 I),(争I),( 议 I),（。 O）}采用BIO标注，B代表句子开始，I代表中间连续词，O代表句子结束。假设采用模型M识别, M次多项式的模型&nbsp;完成下面句子识别。&nbsp;结果分析：M=0时候，模型一个常数效果很差，识别如下：M=1时候，模型一条直线效果很差，识别如下：M=3时候，模型曲线拟合基本合理，且未知数据预测较好，识别如下：M=9时候，模型一条直线效果很差，识别如下：训练数据集：实验可知：左侧为训练模型的数据，右侧为测试模型的数据。当M=0时，训练误差和测试误差都很大；当M=1时，训练误差和测试误差较大；当M=3时，训练误差比M=9的训练误差大，总体训练误差还好，但是预测误差却小于M=9时的预测误差。综合比较，选择M=3的模型效果会更好。 模型评估与选择模型评估的概念评估准确率的常用技术：保持和随机子抽样、K-折交叉验证、自助方法 统计显著性检验：评估模型准确率 ROC曲线：接收者操作特征曲线比较分类器效果好坏 模型评估的评测指标混淆矩阵：正元组和负元组的合计。详细如表13-1所示： 表13-1 混淆矩阵表 ![](https://i.imgur.com/RuIzvaI.png) 评估度量：（其中P:正样本数； N：负样本数； TP：真正例 ；TN：真负例 ；FP：假正例； FN：假负例）评估信息度量如表13-2所示。 表13-2 评估信息度量表 注意：学习器的准确率最好在检验集上估计，检验集由训练集模型未使用的含有标记的数据构成。各参数描述如下：TP（真正例/真阳性）：是指被学习器正确学习的正元组，令TP为真正例的个数。TN（真负例/真阴性）：是指被学习器正确学习的负元组，令TN为真负例的个数。FP（假正例/假阳性）：是被错误的标记为正元组的负元组，令FP为假正例的个数。FN（假负例/假阴性）：是被错误的标记为负元组的正元组，令FN为假负例的个数。高准确率的学习模型：大部分元组应该在混合矩阵的对角线上，而其他为0或者接近0，即FP和FN为0。其本质上是一个对角矩阵时准确率最高。准确率：正确识别的元组所占的比例。又叫做识别率，公式如下：&nbsp; ## 词性标注为例分析模型评估 词性标注为例分析模型评估详细信息如表13-3所示： 表13-3 词性标注为例分析模型评估表 错误率：错误识别元组所占的比例，又叫误识别率，公式如下：&nbsp;&nbsp;或者1-accuracy(M)检验时，应采用检验集未加入训练集的数据。当采用训练集估计模型时，会再带入误差，这种称为乐观估计。准确率可以度量正确标注的百分比，但是不能正确度量错误率。诸如样本不平衡时，即负样本稀疏的时候。比如：欺诈、癌症等，这种情况下应使用灵敏性特效性度量。灵敏度又叫真正识别率：正确识别的正元组的百分比，公式如下：特效性又叫真负例率：正确识别的正元组的百分比，公式如下：准确率的灵敏度和特效性的函数关系：精度：精确性的度量即标记为正元组实际为正元组的百分比，公式如下：、召回率：完全性的度量即正元组标记为正的百分比，公式如下：精度和召回率之间趋向于逆关系，有可能出现一个指标降低另一个指标提升的情况。此刻两个指标预想达到综合引出了F度量值F度量（又叫F分数）：用精度和召回率的方法把他们组合到一个度量中。公式如下：&nbsp;&nbsp;比较：F度量是精度和召回率的调和均值，赋予精度和召回率相等的权重，&nbsp;度量是精度和召回率加权度量，它赋予召回率权重是精度的β倍。诸如中文词汇中常用词的权重比生僻词的权重大，而这也符合实际应用。注意：当元组属于多个类时，不适合使用准确率。当数据均衡分布即正负元组基本相当时，准确率效果最好，而召回率、特效性、精度、F和&nbsp;更适合于样本分布不均的情况。 ## 模型评估的几种方法 模型评估包括多种方法，常见的评估流程如图13-2所示。 ![](https://i.imgur.com/CRWjZyh.png) 图13-2 模型评估流程图 1）随机二次抽样评估准确率：是保持方法的一种变形，将保持方法重复K次，总准确率估计是每次迭代准确率的平均值2）K-折交叉验证评估准确率：（建议10折）K-折交叉验证：将初始的数据随机分为大小大致相同的K份，训练和检验进行K次，如第1次迭代第一份数据作为检查集，其余K-1份作为训练集，第2次迭代。第二份数据作为检验集，其余K-1份作为训练集，以此类推直到第K份数据作为检验集为止。此方法每份样本用于训练的次数一致且每份样本只作为一次检验集。准确率是K次迭代正确元组总数除以初始数据元组总数。一般建议采用10-折交叉验证估计准确率，因为它的偏移和方差较低。3）自助法评估准确率：自助法有放回的均匀抽样，常用632自助法。即63.2%原数据将出现在自助样本中。而其余38.8%元数据形成检验集。 ROC曲线比较学习器模型成本效益（风险增益）：如错误的预测癌症患者没有患病比将没有患病的病人归类癌症的代价大等等事件，据此给予不同的权重。ROC曲线又叫接受者操作特征曲线，比较两个学习器模型的可视化工具，横坐标参数假正例率，纵坐标参数是真正例率。以此汇聚成的曲线，越靠近对角线（随机猜测线）模型越不好。真正例率（召回率）：假正例率：实例解析：以10个检验元组的概率分类器为例绘制ROC曲线。数据如表13-4所示 表13-4 检验元组数据表 ![](https://i.imgur.com/9TJKhbX.png) 由以上数据绘制ROC曲线如图13-3所示： ![](https://i.imgur.com/JjoM0mI.png) 图13-3 ROC曲线图 由此可知，对角线为随机猜测线，模型的ROC曲线越靠近对角线，模型的准确率越低。如果很好的模型，真正比例比较多，曲线应是陡峭的从0开始上升，后来遇到真正比例越来越少，假正比例元组越来越多，曲线平缓变的更加水平。完全正确的模型面积为1。&lt;/p&gt; 参考文献 GitHub 图书：《自然语言处理理论与实战》 完整代码下载 源码请进【机器学习和自然语言QQ群：436303759】文件下载： 作者声明 本文版权归作者所有，旨在技术交流使用。未经作者同意禁止转载，转载后需在文章页面明显位置给出原文连接，否则相关责任自行承担。]]></content>
      <categories>
        <category>机器学习</category>
        <category>模型评估</category>
      </categories>
      <tags>
        <tag>准确率</tag>
        <tag>召回率</tag>
        <tag>F度量值</tag>
        <tag>模型选择</tag>
        <tag>模型评估</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[命名实体识别技术]]></title>
    <url>%2F2019%2F02%2F13%2F%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%E6%8A%80%E6%9C%AF%2F</url>
    <content type="text"><![CDATA[摘要：命名实体识别在自然语言处理占据着非常重要的地位，也是不可逾越的学术问题。关于命名实体识别的学术理论和研究方法众多，本章侧重整体介绍。首先阐述了命名实体识别的背景知识和研究概况；其次主要介绍中文命名实体识别的特点与难点，加以案例加深理解；然后对命名实体识别当前研究方法和核心技术进行详细介绍；最后，展望其在未来人工智能方面的发展前景。（本文原创，转载必须注明出处.） 命名实体识别概述背景介绍命名实体识别(Named Entity Recognizer,NER)在第六届信息理解会议(MUC-6)被提出后，人们的视野便聚焦在信息抽取 (Information Extraction)问题上(即如何从半结构化、非结构化文本中抽取出结构化信息)。此外，命名实体识别也是信息抽取、本体构建、问答系统等自然语言处理任务的基础工作。 命名实体识别旨在识别文本中的角色实体，可以分解成两个子任务即实体边界确定和实体类别划分。当前命名实体识别研究方法主要有：(1)规则和词典相结合的方法，一般适用于精确度较高的情况。但是其存在系统建设周期长，移植性差等问题。(2)统计机器学习的方法，诸如隐马尔可夫模型、条件随机场、最大熵等。本文采用命名实体识别技术主要解决三个问题：一是如何准确分词，因为中文不同于英文由空格和单词构成，解决中文字词的划分尤为重要；二是分词后如何进行序列标注，标注集合的规约；三是如何准备识别实体边界以及实体名。诸如语料“拖雷与郭靖想起在襄阳城下险些拼个你死我活，都是暗叫惭愧”。这句话可以看成一个由词构成的序列，那么分词效果尤为重要。下面看看部分分词工具的处理结果： (1) StanfordNLP中文分词结果： (2) 结巴分词结果： (3) 中科院分词结果： 上述分词结果表明，(1)与(2)中“暗叫惭愧”的分词结果不一致；(3)与(1)(2)最大区别就是“托雷和郭靖”的分词结果不一致。显然(3)出现的问题更为严重，出现错误的人名识别，由此看出分词的好坏直接影响命名实体的识别结果。此外，歧义词、未登录词都是命名实体识别中亟待解决的问题。本文采用基于角色标注的方法对分词结果进行序列标注。角色标注结果如下： 其中nr为人名，ns为地名。完成实体识别后，采用BMES标注方式进行处理。其中B表示Begin即识别出边界，M表示Middle即识别出实体中间名，E表示End即实体名识别介绍，S表示Single表示独立成词，最后过滤掉独立词即可。其中人名可以细化为译名，日本名等；地名可以细化为国家、省、市县等。综上所述，采用自然语言处理技术手段对文本语料的命名实体识别具有深层次的意义。 国内外研究现状本文采用自然语言处理技术对中文命名实体识别方法进行研究。命名实体识别在自然语言处理中占据很重要的位置，命名实体识别的评测系统也备受国内外会议重视。主要包括如下会议： (1) 信息理解研讨[9]。 (2) 文本检索会议[10]。 (3) 多语种实体评价任务会议[11]。 (4) 国际中文处理评测[12]。 (5) 自动内容抽取评测会议[13]。 (6) ACL会议[14]。 (7) 自然语言学习会议[15]。 (8) 863评测会议[16]。 国内外关于命名实体识别的主要研究机构和相关工作如下：(1) 国外研究机构主要是对英语等语言的实体识别，代表机构包括斯坦福研究所人工智能中心、因特尔研究中心、微软研究院、雅虎研究中心、日本东京大学等。(2) 国内主要解决中文命名实体识别，代表机构包括中科院计算所、微软中国研究院、哈尔滨工业大学自然语言处理实验室、北京语言大学语言信息处理研究所、北京理工大学自然语言处理研究室和复旦大学自然语言处理研究室等。 随着第六届信息理解会议提出的信息抽取相关研究，命名实体识别作为其下的一个子任务而备受关注。名称的自动抽取又称做“命名实体识别”，其任务就是识别出待处理文本中的三大类和七小类命名实体。三大类命名实体包括：实体类(人名、地名机构名)、时间类(日期、时间和持续时间)和数字类(货币、度量衡、百分比和基数)；七小类命名实体包括：人名、地名、机构名、时间、日期、货币和百分比。其中时间、百分比、日期、货币的构成较为规律，识别起来难度不大；而人名、地名、机构名用字较为灵活，识别难度较大。所以，命名实体识别通常指人名、地名、机构名的识别。在命名实体识别中，中文命名实体存在形式不一、语言环境复杂等现象，其研究方法也呈现出多样性的特点。总体分为三类，分别是基于规则模型命名实体识别、基于统计模型的命名实体识别和基于规则结合统计的命名实体识别。其中基于统计模型的学习方式又划分如下四类： (1) 有监督的学习方法：利用人工标注大部分数据集进行模型训练学习。 (2) 半监督的学习方法：利用人工标注很少的数据集(种子数据)自举学习。 (3) 无监督的学习方法：不再进行人工标注，而是通过上下文聚类学习。 (4) 混合方法：几种模型相互结合或利用统计方法和人工总结的知识库。 目前采用自然语言处理工具对命名实体识别具有良好的效果，特别是在结合专业知识领域的情况下。自然语言处理工具主要包括： (1) 精准自然语言解析器(SyntaxNet) (2) 中文自然语言处理工具包(FudanNLP) (3) Java自然语言处理(LingPipe) (4) 自然语言处理工具(OpenNLP) (5) 自然语言工具包(NLTK) (6) 自然语言工具包(CRF++) (7) 单词转换成向量形式(word2vec) (8) 自然语言文本处理库(spaCy) 命名实体识别特点与难点命名实体识别可以分解为两大任务。(1)如何去识别命名实体的边界？(2)如何去判定实体的类别(诸如：人名、地名、机构名)？中文命名实体识别要比英文命名实体识别更为复杂，这一是受中文自身语言特性限制，不同于英语文本中词间有空格界定；二是英文中的实体一般首字母大写容易区分，诸如：‘Jobs was adopted at birth in San Francisco，and raised in a hotbed of counterculture’。人名乔布斯Jobs的首字母大写，地名旧金山San Francisco首字母也是大写。而中文不具备这样的特征，例如：“周总理忙了一日，早已神困眼倦。”人名“周总理”就很难在一串汉字中识别出来。 命名实体语言环境较为复杂。对于同一个汉字某些情况下可以看作实体处理，某些情况又不能看作实体。例如：人名，比如《天龙八部》中“婢子四姊妹一胎孪生，童姥姥给婢子取名为梅剑，这三位妹子是兰剑、竹剑、菊剑。”人物“竹剑”，某些情况下就是指的一种竹子做的剑。地名，比如《射雕英雄传》中“陆庄主知道此人是湖南铁掌帮的帮主”中地点“湖南”，在某种情况下就指代地理方位“湖的那边”。机构名，比如《鹿鼎记》中“这位是莲花堂香主蔡德忠蔡伯伯。”组织机构名(帮派名)“莲花堂”，在某种情况就指代种植莲花的一个地方，变成地点名了。 命名实体内部结构形式多样。例如：人名，人名由姓和名构成。其中姓氏包括单姓和复姓(如：赵、钱、孙、李、慕容、东方、西门等)，名由若干个汉字组成。姓氏的用字范围相对有限，比较容易识别。然而名就比较灵活，既可以用名、字、号表示，也可以使用职务名和用典。比如：“李白、李十二、李翰林 、李供奉、李拾遗、李太白、青莲居士，谪仙人”都是同一个人。地名，一般由若干个字组成地名，可以为作为后缀关键字或者别名都是指代一个地方。比如：“成都、蓉城、锦城、芙蓉城、锦官城、天府之国”，其中“蓉城、锦城、芙蓉城、锦官城、天府之国”为别名。除了全称的名称之外，还有地理位置代表地名的。比如：“河南、河南省、豫”都是指的一个省份，其中“豫”是简称。组织机构名，组织机构命名方式比较复杂，有些是修饰性的命名，有些表示历史典故，有些表示地理方位，有些表示地名，有些表示风俗习惯和关键字等等。例如：组织名“广州恒大淘宝足球俱乐部”中，“广州”表示地名的成分，“恒大”“淘宝”表示公司名称成分，“足球”是一项体育赛事成分，“俱乐部”是关键字的成分。比如：“四川大学附属中学”(四川省成都市第十二中学)中包括另一个机构名“四川大学”。机构名还可以以简称形式表示，比如：“四川大学附属中学”简称“川大附中”，“成都信息工程大学”简称“成信大”。 除开上述问题，由于中文命名实体数量较大，我们还很难构建大而全的名字库、地址库等。还有较长的少数民族人名和译名(比如：扎克伯格、麦当劳、肯德基)，没有统一的构词规范。并且人名、地名和组织机构名之间有着交叉和包含现象，组织名称中也常常包含大量的人名、地名、数字。想要正确标注这些实体类型，需要基于上下文内容。 对命名实体的边界识别和类型确定尚没有统一标准。命名实体识别过程常与中文识别等结合，通常分词、语法分析系统的结果也影响命名实体识别的有效性。根据领域和知识本体的需要实体还可以细分，诸如体育名、汽车名、商标名等。随着电商的发展，品牌名、产品名等商品类实体识别也有需求。针对这些新的实体类型，最大的瓶颈就是缺少标准的训练数据。歧义现象和不同实体内部特征都是亟需解决的问题。 命名实体识别方法基于统计的命名实体识别方法进入21世纪之后，基于海量数据的统计方法逐渐成为自然语言处理的主流，同时自然语言处理的各个方面也得到机器学习方法的支持。基于统计的命名实体识别方法具有可移植性好、语言依赖性小、处理速度快等优点。该方法是利用序列标注实现的，本质上就是命名实体识别问题向序列标注问题的转换，其主要处理步骤如下： (1) 统计学习策略：基于统计方法的命名实体识别，合适的机器学习方法是很重要的。常用的机器学习方法包括：隐马尔可夫模型(HMM)、条件随机场模型(CRF)、最大熵模型(MEM)等 。 (2) 特征选择：特征的选择直接影响命名实体识别的好坏，一般特征包括一些先验知识(如：词性、词典、词后缀等)。序列标注：通过基于机器学习策略的文本序列标注来处理训练集和测试集。 (3) 模型训练：通过对训练集的训练优化算法模型。 (4) 模型评测：训练出来的算法模型，通过测试集进行测试，反复实验以得到理想的模型结果，并将算法模型应用到命名实体识别中。 以上介绍的机器学习模型中，隐马尔可夫模型是非常重要的一种统计模型，其本质上是一种马尔可夫随机过程的概率函数。研究者们将隐马尔可夫模型应用到命名实体识别、语音识别、序列标注等领域中都取得了不错的成绩。最大熵模型的原理就是在学习概率模型时，认为概率分布模型中最大熵的模型是最好的，并常以此作为约束条件来确定概率模型集合。最大熵模型也可表示为在满足约束条件的模型集合中选择最大熵的模型。由于马尔可夫模型具备生成式模型的缺点，而最大熵模型受约束条件限制。结合两种模型的最大熵马尔可夫模型是其一种延伸，这种模型具备以上两种模型的优点，成功克服各自模型的缺陷，但是其存在标记偏置的缺陷。 最大熵模型最大的问题就是标记偏置。针对这一问题，Lafferty 等学者提出了条件随机场(CRF)模型。CRF对给出观测序列的条件下，针对全序列进行联合概率的指数模型，这种方法很好地解决了偏置缺陷的问题。条件随机场不仅适用于命名实体识别，而且在英文POS标注、英文词短语识别等方面都取得了不错效果。 综上所述，可以将基于机器学习的命名实体识别方法划分为：有监督的学习方法、半监督学习的方法、无监督的学习方法和多种模型混合的方法，具体方法归纳如表2-1所示： 基于规则和统计的命名实体识别方法在实际应用中，仅仅基于规则或者统计的方法并不能取得期望的结果。规则和统计结合的方法具备两者的优点，也是实际应用最多的方法。通常我们会选择基于统计的方法对训练语料进行模型构建，从而拟合出一个最佳模型。在这个模型的基础上，再加以人工知识库即基于规则的方法辅助命名实体识别，以防止出现过拟合的现象。这样做的好处一方面降低了命名实体识别模型的语料库规模，提高识别准确率和召回率；另一个方面，可以保证识别模型的算法效率。事实证明这种方法确切可行且取得了不错的效果。 Seon等人提出的基于最大熵模型和神经网络模型以及规则的方法进行命名实体识别取得很好的效果，系统可以对中文人名、地名、机构名进行识别，其效果如下表2-2所示： 张华平等人首先应用隐马尔可夫模型结合词汇表方法进行汉语人名识别，其方法原理是首先对人名进行分类，然后标注语料集进行模型训练，接着采用维特比算法进行人名标注，最后用最大模式匹配进行人名识别。 中文命名实体识别的核心技术命名实体角色标注由于中文实体构成的特点和难点造成实体识别比较困难，针对这些困难本文采用基于角色标注的中文实体识别方法进行处理。分好的词细化为人名内部组成关系、上下文关系、无关词，总结为中国人名构成角色表、地名构成角色表、机构名角色表。中文命名实体的识别是将熟语料中的序列标注转换成实体角色标注的过程。其数学描述为：给定一个文本序列串( ，其中 表示文本特征项)，其目的是构造一个序列标注机器p，使其为文本串x标注合适的标签串 。其中 ，其中y属于人名构成角色表中的标记，然后从所有可能标注的序列中选择最大概率即： (4-1) 根据贝叶斯公式，可知 (4-2) 由于p(x)是一个常数，由(4-1)(4-2)可知： (4-3) 根据第二章第三节HMM相关知识计算P(Y)P(X|Y)可知： (4-4) 式子(4-4)可化简为对数形式，则有： (4-5) 本文训练数据来源于2014年《人民日报》，其采用北京大学计算语言学所的词类标注集得到结果如表4-1所示： 结果表明：分词模型根据训练将姓氏和名字进行分割标注采用nr。针对这种人名的标注不利于姓名的识别，如果姓氏和名字采用不同的标注，利用上下文识别效果会更好。因此，文本采用中科院的词类标注集进行实验，其标注形式如表4-2所示： 分析发现中科院词类标注中对姓和名标注采用nf、nl单独标注，nr标注为人名。这样做的好处是可以通过词位上下文分析，避免出现上文与姓成词或者下文与名成词的现象。 本文针对以上两种情况进行改进，首先对语料采用中科院词类标注集进行处理，再对熟语料进行逐句的读入，判断该句中的词是否为人名相关词即nf、nl、nr词。若非人名相关的词性均标注为独立词即A，若与人名相关则继续判断其上文p与姓nf是否成词，不成词标注为K即上文，成词标注为U；继续判断名与下文是否成词，若不成词标注为L即下文，成词则标注为V。再将姓氏标注为B，姓与单名标注为Y，姓与双名标注为X，双名中间成词标注为Z。诸如此类讨论人名的各种情况，最后过滤掉独立词，将人名项目的标注进行词频统计，以及不同标注直接转化进行概率统计。再用Viterbi算法进行改进后的标注如表4-3所示： 关于中文实体标注的问题实际上就是式子(4-5)的求解问题了，即转化为隐马尔可夫模型的解码问题，采用Viterbi算法便可解决，Viterbi算法具体介绍见第二章第二节内容。以人名实体识别为例，人名识别还存在以下问题，标记为U的词，即“公司现任法人为何三立”，这里的人名上文和姓成词了。标记为V的词，即“白宁超级别过低，不能申请教授”，这里的人名末尾词与下文成词现象。 针对这种情况，解决方式是再次进行细粒度分割，处理成KB(人名上文+姓氏)，DL(双名末尾词+人名下文)或者EL(单名+人名下文)；然后再根据模式集合{BBCD，BBE，BBZ，BCD，BE，BG，BXD，BZ，CD，FB，Y，XD}进行最大模式串的匹配。例如：“白宁超在博客园开了一个博客。”，分词结果是“白/宁/超/在/博客/园/开/了/一个/博客/。”采用本文基于角色标注结果是：“BCDLAAAAAA”可以识别出人名“白宁超”，这对下一步地名、机构名识别减少了很多干扰作用。人名识别算法如表4-4所示： 上述改进算法的人名识别结果如图4-2所示： 标注实体词图生成通过4.4.1节实体角色标注，再利用Viterbi算法求得类别实体名。诸如人名实体识别为“孙中山”，针对南京市的“中山路”该如何识别呢？这就属于包含简单实体的复合实体识别问题。假设包含简单实体“孙中山”为 ，相应的角色标注为 ，由于 不在词典中，所以 就无法求解了。此时，我们引入基于角色是实体的生成模型，该模型与隐马尔可夫模型映射，其目的就是求解复合命名实体的生成概率。采用HMM过程可以得到如4-6(其中w是待识别的命名实体，c是类别)： 经过语料的角色生成模型之后，问题就简化为对统计角色词频和角色转移概率的求解了。其原理是利用中文切分的熟语料，再结合少量代码对熟语料进行标注。诸如：2014年人民日报句子如表4-5所示： 通过少量自动角色标注代码，逐步处理得到如图4-3所示的角色标注结果： 完成如上操作之后的工作就是统计角色词频和转移矩阵了，这个很容易处理。利用HMM-viterbi算法即可得到复合实体的最大生成概率，然后采用模式匹配方法进行实体识别。将识别出来的实体作为词典数据传输到下一层HMM实体中，这不仅可以很好地解决复合实体识别问题，还可以提高层叠隐马尔可夫模型参数的优化和改进。本节值得强调的是，本文基于地名角色标注改进了原有的方法，提出C(中国地名首部)、D(中国地名中间)、E(中国地名未部)，在地名模式识别中(CDE地名+三字后缀)可以将地名识别改进到六个汉字的长度。其后便是对命名实体的自动抽取工作。 命名实体自动抽取层叠隐马尔可夫模型解决的是将中文分词、切分歧义、人名、地名、结构名和词性标注融合在一起的算法模型。由于中文粘稠性的特点，困扰分词结果最大的问题就是未登录词，本质上就是中文命名实体的识别。这主要有人名、地名、机构名、时间、数量、单位等。其中人名、地名、机构名最难识别，也是对中文分词影响最大的因素。因此命名实体一体化抽取整个过程如下： (1) 首先对2014年《人民日报》语料库进行词频统计，构建出核心分词词典，此时核心词典仅仅含有极其少量的高频人名、地名、机构名等实体。 (2) 利用第一步的核心词典，对原始字符串进行粗粒度切分，并利用N元最短路径进行歧义切分。利用角色语料进行训练得到人名角色词典和角色标记间的转移概率，详细参见第四章第二节。 (3) 利用第二步识别出来的人名粗分结果，基于角色的地名进行识别，得到地名词典。为了识别含有人名的复合地名，人名识别的HMM将人名类作为输入参数，进而识别出地名和相应的转移概率。 (4) 跟人名、地名类似，将人名识别HMM和地名识别的HMM作为参数，通过与语料库中标注好的机构名进行训练得到机构名的角色表。 (5) 将识别出的人名、地名、机构名、歧义切分、登录分词融合一体，进行全局最优概率计算。 (6) 对实验结果进行测试和模型性能指标评估。 层叠隐马尔可夫模型框架设计过程如下：(1)构建核心词典本质上就是对词频统计和词之间概率转移的统计；(2)进行N元最短路径歧义切分，本文采用的是N元最短路径的策略，即在初始阶段保全切分概率P(X)最大的N个结果，并以此当做候选集合，再综合运用最少切分和全切分的方法。具体实验过程如下： (1) 对原始语料进行切分标注后如表4-6所示： (2) 词性标注向基于角色标注的转换结果如表4-7所示： (3) 统计词频，在对所有熟语料句子执行自动标注后，即可统计每一个非Z词语的各角色词频。然后统计文本序列的发射矩阵和转移矩阵，并最终带入公式求最大概率即可。其中发射矩阵如图4-4所示： 转移矩阵指的是从一个角色标签转移到另一个角色的频次，利用它和角色词频可以计算出HMM中的初始概率、转移概率、发射概率，进而完成求解。这里对人民日报2014切分语料训练出如下转移矩阵如图4-5所示： 经过角色标注和生成模型后，对于实体发射概率 和转移概率 这两个重要参数进行求解。在大规模语料前提下采用大数定律可知： 其中 是实体角色到下一个角色的次数； 在训练模型中已经得到，由此得到上层实体词典，然后采用N-Best策略将词典传输到下一层HMM实体中，进而优化层叠隐马尔可夫的参数因子。 以“成都张晓明餐饮管理有限公司是由张先生创办的餐饮企业”为例，如表4-8所示： 上例中该公司的各个成分被拆散，无法组成完整的机构名角色标注。将其转化为自定义的角色标注后如表4-9所示： 经过模式串匹配得到如下机构名：[成都张晓明餐饮管理有限公司 GFCC]。 展望中文命名实体识别是文本信息处理中一个重要的研究分支，也是信息抽取、问答系统、中文文摘、机器翻译等自然语言处理技术的基础工作[68]。目前中文单一命名实体识别研究较多，但是对多种中文命名实体一体化识别研究较少。本文经过大量研究工作提出了基于层叠隐马尔可夫模型的中文命名实体一体化识别，主要工作包括： 对命名实体识别工作进行深入研究。主要针对当前命名实体识别研究现状、特点难点、主要研究方法、评测标准和相关模型进行分析讨论。 中文分词未登录词和歧义词切分。本文提出了基于统计和规则结合的改进中文分词方法，将统计方法在未登录词处理优势和词表规则速度优势相结合，很好地解决歧义和未登录词切分问题。 提高了模型的召回率。本文提出了一种细粒度的特征提取方法，首先利用CRF++工具进行特征模板选择。再基于字词不同粒度的特征模型实验对比，从而进行特征模板的优化和改进，形成自定义的特征模板，提高了系统识别的召回率。 多种中文命名实体一体化识别模型设计。本文提出了层叠结构，将系统分为低层的隐马尔可夫模型和高层隐马尔可夫模型。首先采用低层的隐马尔可夫模型对简单的人名、简单地名、简单机构名进行识别。然后将识别的结果提供给更高层的隐马尔可夫模型，再进行对复合地名、机构名的识别。其次，低层识别结果可以作为高层识别决策提供支持；最后，通过对模型参数的改进与优化完成对多种命名实体的一体化识别。 参考文献 GitHub 图书：《自然语言处理理论与实战》 完整代码下载 源码请进【机器学习和自然语言QQ群：436303759】文件下载： 作者声明 本文版权归作者所有，旨在技术交流使用。未经作者同意禁止转载，转载后需在文章页面明显位置给出原文连接，否则相关责任自行承担。]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>命名实体识别</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简洁全面的Scrapy爬虫技术入门]]></title>
    <url>%2F2019%2F01%2F21%2F%E7%AE%80%E6%B4%81%E5%85%A8%E9%9D%A2%E7%9A%84Scrapy%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[摘要：AI时代在我们生活中扮演着愈加重要的角色，其显著特征就是对海量数据的处理。所谓海量数据即大数据，我们首先获取到数据才能够挖掘其信息，达到AI层面的应用。而数据的存在形式，绝大多数是非结构化的，网页存储就是典型的非结构化数据。由此引出了网络爬虫技术，本文主要介绍Scrapy的原理和入门应用，以及本地化存储。（本文原创，转载必须注明出处.） 基础准备IDE：sublime开发环境：win10+mysql5.0+navicat10.0.11编程语言：python3.7+Anaconda4.4技术选型：scrapy+requests爬取目标：http://blog.jobbole.com/all-posts/相关插件：python最近插件均可 建议豆瓣源镜像下载，可以提升下载速度。如：django1pip install -i https://pypi.doubanio.com/simple/ Django 基础知识scrapy 与 requests+beautifulsoup 区别 requests和beautifulsoup都是库，scrapy是框架 scrapy框架可以加入requests和beautifulsoup scrapy基于twisted，性能的最大的优势 scrapy方便扩展，提供丰富功能 scrapy内置css和xpath selector非常方便，beautifulsoup速度慢 爬虫的作用 搜索引擎 百度。google、垂直领域搜索引擎（有目的性的） 推荐引擎 今日头条（用户习惯） 机器学习的数据样本 数据分析、舆情分析等 正则表达式 特殊字符的提取 ^ $ . * ? + {2} {2,} {2,5} ^ 表示开头 . 任意字符 * 任意次数 $ 结尾 ? 非贪婪模式，提取第一个字符 + 至少出现一次 {1} 出现一次 {3,} 出现3次以上 {2,5} 最少2次最多5次 | 或的关系 [] 满足任意一个都可以,[2435]任意 [0-9]区间 1非1 \s 为空格 \S非空格 \w 匹配[A-Za-z0-9_] \W 反匹配[A-Za-z0-9_] [\u4E00-\u9FA5] 汉字的匹配 \d 匹配数字 爬虫去重策略 将访问的url保存到数据库中，效率比较低 将访问过的url保存到set中，只需要o(1)的代价可以查询url1亿2byte50字符/1024/1024/1024=9G。一亿url就有9G内容，占用内存大 url经过md5等方式哈希编码后保存到set中，此时一亿url大约3G左右内容 用bitmap方法，将访问过的url通过hash函数映射到某一位，存在冲突问题 bloomfilter方法对bitmap进行改进，多重hash函数降低冲突 scrapy爬取技术网站Scrapy技术原理(绿线是数据流向) 架构图 Scrapy Engine(引擎): 负责Spider、ItemPipeline、Downloader、Scheduler中间的通讯，信号、数据传递等。 Scheduler(调度器): 它负责接受引擎发送过来的Request请求，并按照一定的方式进行整理排列，入队，当引擎需要时，交还给引擎。 Downloader（下载器）：负责下载Scrapy Engine(引擎)发送的所有Requests请求，并将其获取到的Responses交还给Scrapy Engine(引擎)，由引擎交给Spider来处理， Spider（爬虫）：它负责处理所有Responses,从中分析提取数据，获取Item字段需要的数据，并将需要跟进的URL提交给引擎，再次进入Scheduler(调度器). Item Pipeline(管道)：它负责处理Spider中获取到的Item，并进行进行后期处理（详细分析、过滤、存储等）的地方。 Downloader Middlewares（下载中间件）：你可以当作是一个可以自定义扩展下载功能的组件。 Spider Middlewares（Spider中间件）：你可以理解为是一个可以自定扩展和操作引擎和Spider中间通信的功能组件（比如进入Spider的Responses;和从Spider出去的Requests） 制作 Scrapy 爬虫步骤：1 新建项目 (scrapy startproject xxx)：新建一个新的爬虫项目2 明确目标 （编写items.py）：明确你想要抓取的目标3 制作爬虫 （spiders/xxspider.py）：制作爬虫开始爬取网页4 存储内容 （pipelines.py）：设计管道存储爬取内容 scrapy安装和项目创建1 安装scrapy，pip install scrapy2 进入一个根目录文件夹下，创建Scrapy项目：scrapy startproject mySpider3 其中， mySpider 为项目名称，可以看到将会创建一个 mySpider 文件夹，目录结构大致如下：下面来简单介绍一下各个主要文件的作用： mySpider/ scrapy.cfg mySpider/ __init__.py items.py pipelines.py settings.py spiders/ __init__.py ... 这些文件分别是: scrapy.cfg: 项目的配置文件。 mySpider/: 项目的Python模块，将会从这里引用代码。 mySpider/items.py: 项目的目标文件。 mySpider/pipelines.py: 项目的管道文件。 mySpider/settings.py: 项目的设置文件。 mySpider/spiders/: 存储爬虫代码目录。 项目准备WIN+R调出cmd，并新建项目名为【BoLeSpider】如下： >> scrapy startproject BoLeSpider ![](https://i.imgur.com/iYlbzmx.png) ### 在 BoLeSpider 项目下创建爬虫目录 >> cd BoLeSpider >> Scrapy genspider jobbole http://www.jobbole.com/ ![](https://i.imgur.com/a4cYjJK.png) ### 在 BoLeSpider 目录下创建main.py # -*- coding: utf-8 -*- __author__ = 'BaiNingchao' import sys,os from scrapy.cmdline import execute sys.path.append(os.path.dirname(os.path.abspath(__file__))) execute(["scrapy", "crawl", "jobbole"]) # scrapy crawl jobbole ![](https://i.imgur.com/SoOQfdN.png) main.py中的方法与cmd下执行效果是一致的，这个遍历执行程序创建该主函数。 ## 爬取技术网站内容 ### 打开setting.py修改： ROBOTSTXT_OBEY = False。意思为不符合协议的也继续爬取，如果True很快就会停止爬虫 ITEM_PIPELINES = { 'BoLeSpider.pipelines.BolespiderPipeline': 1, } 分析目标网站设置提取特征 对以上文章内容，我们试图提取【新闻题目、创建时间、URL、点赞数、收藏数、评论数】这些内容 cmd下shell对各个字段调试（xpath或者css方法）：scrapy shell http://blog.jobbole.com/114638/ 打开页面F12,复制对应的xpath路径 对网页特征提取我们一般是shell里面调试（如上图所示），特征抽取有两种方式，一种的基于xpath方法，一种基于css方法，根据大家喜好去使用。 基于xpath方法 title = response.xpath('//*[@id="post-114638"]/div[1]/h1/text()').extract() # 新闻题目 crate_date = response.xpath('//*[@id="post-114638"]/div[2]/p/text()').extract()[0].strip().replace('·','') # 创建时间 url = response.url # url dianzan = self.re_match(response.xpath('//*[@id="post-114638"]/div[3]/div[5]/span[1]/text()').extract()[1]) # 点赞数 soucang = self.re_match(response.xpath('//*[@id="post-114638"]/div[3]/div[5]/span[2]/text()').extract()[0]) # 收藏数 comment = self.re_match(response.xpath('//*[@id="post-114638"]/div[3]/div[5]/a/span/text()').extract()[0]) # 评论数 基于css方法 # css获取内容 title = response.css('.entry-header h1::text').extract() # 新闻题目 crate_date = response.css('p.entry-meta-hide-on-mobile::text').extract()[0].strip().replace('·','') # 创建时间 url = response.url # url dianzan = self.re_match(response.css('.vote-post-up h10::text').extract()[0]) # 点赞数 soucang = self.re_match(response.css('.bookmark-btn::text').extract()[0]) # 收藏数 comment = self.re_match(response.css('a[href="#article-comment"] span::text').extract()[0]) # 评论数 print(title,'\n',crate_date,'\n',url,'\n',dianzan,'\n',soucang,'\n',comment) 编写jobbole.py完整代码： # -*- coding: utf-8 -*- # -*- coding: utf-8 -*- import scrapy,re class JobboleSpider(scrapy.Spider): name = 'jobbole' allowed_domains = ['http://www.jobbole.com/'] start_urls = ['http://blog.jobbole.com/114638'] '''获得单页的信息''' def parse(self, response): # css获取内容 title = response.css('.entry-header h1::text').extract() # 新闻题目 crate_date = response.css('p.entry-meta-hide-on-mobile::text').extract()[0].strip().replace('·','') # 创建时间 url = response.url # url dianzan = self.re_match(response.css('.vote-post-up h10::text').extract()[0]) # 点赞数 soucang = self.re_match(response.css('.bookmark-btn::text').extract()[0]) # 收藏数 comment = self.re_match(response.css('a[href="#article-comment"] span::text').extract()[0]) # 评论数 print('标题:',title,'\n','发布时间:',crate_date,'\n','文章地址:',url,'\n','点赞数：',dianzan,'\n','收藏数',soucang,'\n','评论数',comment) # 对点赞数、收藏数、评论数等进行正则数字提取 def re_match(self,value): match_value = re.match('.*?(\d+).*',value) if match_value: value = int(match_value.group(1)) else: value = 0 return value 运行main.py函数，便提前到所有信息： 获取列表页所有文章 获取列表下所有页的信息，找到列表页F12分析，使其下一页自动爬取.在cmd的项目根目录下 scrapy shell http://blog.jobbole.com/all-posts/ response.css("#archive .floated-thumb .post-thumb a::attr(href)").extract() ![](https://i.imgur.com/uuoJN01.png) > 设置items.py # -*- coding: utf-8 -*- # Define here the models for your scraped items # # See documentation in: # https://doc.scrapy.org/en/latest/topics/items.html import scrapy from scrapy.loader.processors import MapCompose class BolespiderItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() pass # 设置提取字段的实体类 class JobBoleItem(scrapy.Item): title = scrapy.Field() # 文章题目 create_date = scrapy.Field() #发布时间 url = scrapy.Field() #当前文章url路径 dianzan = scrapy.Field() #点赞数 soucang = scrapy.Field() # 收藏数 comment = scrapy.Field() # 评论数 jobbole.py 的代码改为： # -*- coding: utf-8 -*- import scrapy,re,datetime from scrapy.http import Request from urllib import parse from BoLeSpider.items import JobBoleItem class JobboleSpider(scrapy.Spider): name = 'jobbole' allowed_domains = ['http://www.jobbole.com/'] # start_urls = ['http://blog.jobbole.com/114638'] start_urls = ['http://blog.jobbole.com/all-posts/'] # 所有页信息 # 获取列表下所有页信息 def parse(self, response): # 1 获取文章列表中的具体文章url并交给解析函数具体字段解析 post_urls = response.css("#archive .floated-thumb .post-thumb a::attr(href)").extract() for post_url in post_urls: yield Request(url=parse.urljoin(response.url,post_url),callback=self.parses_detail, dont_filter=True) # scrapy下载 # 2 提取下一页并交给scrapy提供下载 next_url = response.css(".next.page-numbers::attr(href)").extract_first("") if next_url: yield Request(url=parse.urljoin(response.url, post_url), callback=self.parse, dont_filter=True) # scrapy shell http://blog.jobbole.com/114638/ def parses_detail(self, response): article_item =JobBoleItem() article_item['title'] = response.css('.entry-header h1::text').extract() article_item['create_date'] = date_convert(response.css("p.entry-meta-hide-on-mobile::text").extract()[0].strip().replace("·","").strip()) article_item['url'] = response.url article_item['dianzan'] = re_match(response.css('.vote-post-up h10::text').extract()[0]) article_item['soucang'] = re_match(response.css('.bookmark-btn::text').extract()[0]) article_item['comment'] = re_match(response.css('a[href="#article-comment"] span::text').extract()[0]) yield article_item # **************************正则对字段格式化处理****************************** # 对点赞数、收藏数、评论数等进行正则数字提取 def re_match(value): match_value = re.match('.*?(\d+).*',value) if match_value: nums = int(match_value.group(1)) else: nums = 0 return nums # 对时间格式化处理 def date_convert(value): try: create_date = datetime.datetime.strptime(value, "%Y/%m/%d").date() except Exception as e: create_date = datetime.datetime.now().date() return create_date 网页提取后的结果 本地化存储爬取的网页内容将结果保存在json文件中 在pipline.py下修改代码如下 # -*- coding: utf-8 -*- # Define your item pipelines here # # Don't forget to add your pipeline to the ITEM_PIPELINES setting # See: https://doc.scrapy.org/en/latest/topics/item-pipeline.html from scrapy.exporters import JsonItemExporter import codecs class BolespiderPipeline(object): def process_item(self, item, spider): return item # 调用scrapy提供的json export导出json文件 class JsonExporterPipleline(object): def __init__(self): self.file = open('articleexport.json', 'wb') self.exporter = JsonItemExporter(self.file, encoding="utf-8", ensure_ascii=False) self.exporter.start_exporting() def close_spider(self, spider): self.exporter.finish_exporting() self.file.close() def process_item(self, item, spider): self.exporter.export_item(item) return item 在setting.py 中修改代码如下： ITEM_PIPELINES = { 'BoLeSpider.pipelines.JsonExporterPipleline': 1, } > 在main.py运行程序，查看articleexport.json结果如下： ![](https://i.imgur.com/OiSTFDk.png) ### 将结果保存在MySql数据库中 > 数据库中表的设计 本地数据库，用户名：root，密码：admin，数据库：test ![](https://i.imgur.com/lRluBiU.png) > pipline.py修改如下： from scrapy.exporters import JsonItemExporter import codecs class BolespiderPipeline(object): def process_item(self, item, spider): return item # 调用scrapy提供的json export导出json文件 class JsonExporterPipleline(object): def __init__(self): self.file = open('articleexport.json', 'wb') self.exporter = JsonItemExporter(self.file, encoding="utf-8", ensure_ascii=False) self.exporter.start_exporting() def close_spider(self, spider): self.exporter.finish_exporting() self.file.close() def process_item(self, item, spider): self.exporter.export_item(item) return item # 将爬取的数据字段存储在mysql数据 import MySQLdb import MySQLdb.cursors # MYSQL数据库存储方法1 class MysqlPipeline(object): #采用同步的机制写入mysql def __init__(self): self.conn = MySQLdb.connect('127.0.0.1', 'root', 'admin', 'test', charset="utf8", use_unicode=True) self.cursor = self.conn.cursor() def process_item(self, item, spider): insert_sql = """ insert into myarticles(title, createdate,url,dianzan,soucang,comment) VALUES(%s,%s,%s,%s,%s,%s) """ self.cursor.execute(insert_sql, (item["title"], item["create_date"], item["url"], item["dianzan"],item["soucang"],item["comment"])) self.conn.commit() 在setting.py 中修改代码如下： ITEM_PIPELINES = { 'BoLeSpider.pipelines.MysqlPipeline': 1, } 在main.py运行程序，查看数据库表结果如下： 对网站图片爬取并本地化存储本地化存储爬取的网页内容 重新进行数据库表的设计 jobbole.py 修改如下： # 获取列表下所有页信息 def parse(self, response): # 1 获取文章列表中的具体文章url并交给解析函数具体字段解析 post_nodes = response.css("#archive .floated-thumb .post-thumb a") for post_node in post_nodes: image_url = post_node.css("img::attr(src)").extract_first("") post_url = post_node.css("::attr(href)").extract_first("") yield Request(url=parse.urljoin(response.url,post_url), meta={"front_image_url":image_url},callback=self.parses_detail, dont_filter=True) # scrapy下载 # 2 提取下一页并交给scrapy提供下载 next_url = response.css(".next.page-numbers::attr(href)").extract_first("") if next_url: yield Request(url=parse.urljoin(response.url, post_url), callback=self.parse, dont_filter=True) # scrapy shell http://blog.jobbole.com/114638/ def parses_detail(self, response): article_item =JobBoleItem() article_item['front_image_url'] = [response.meta.get("front_image_url", "")] # 文章封面图 article_item['title'] = response.css('.entry-header h1::text').extract() article_item['create_date'] = date_convert(response.css("p.entry-meta-hide-on-mobile::text").extract()[0].strip().replace("·","").strip()) article_item['url'] = response.url article_item['dianzan'] = re_match(response.css('.vote-post-up h10::text').extract()[0]) article_item['soucang'] = re_match(response.css('.bookmark-btn::text').extract()[0]) article_item['comment'] = re_match(response.css('a[href="#article-comment"] span::text').extract()[0]) yield article_item items.py 修改如下 设置提取字段的实体类 class JobBoleItem(scrapy.Item): title = scrapy.Field() # 文章题目 create_date = scrapy.Field() #发布时间 url = scrapy.Field() #当前文章url路径 dianzan = scrapy.Field() #点赞数 soucang = scrapy.Field() # 收藏数 comment = scrapy.Field() # 评论数 front_image_url = scrapy.Field() # 原图片文件路径 front_image_path = scrapy.Field() # 下载到本地图片路径 pipline.py设置如下： from scrapy.pipelines.images import ImagesPipeline 获取下载后图片文件的路径 class ArticleImagePipeline(ImagesPipeline): def item_completed(self, results, item, info): if "front_image_url" in item: for ok, value in results: image_file_path = value["path"] item["front_image_path"] = image_file_path return item 将爬取的数据字段存储在mysql数据 import MySQLdb import MySQLdb.cursors '''MYSQL数据库存储方法1''' class MysqlPipeline(object): #采用同步的机制写入mysql def __init__(self): self.conn = MySQLdb.connect('127.0.0.1', 'root', 'admin', 'test', charset="utf8", use_unicode=True) self.cursor = self.conn.cursor() def process_item(self, item, spider): insert_sql = """ insert into myarticles(title, createdate,url,dianzan,soucang,comment,img_url,img_path) VALUES(%s,%s,%s,%s,%s,%s,%s,%s) """ self.cursor.execute(insert_sql, (item["title"], item["create_date"], item["url"], item["dianzan"],item["soucang"],item["comment"],item["front_image_url"],item["front_image_path"])) self.conn.commit() setting.py修改： ITEM_PIPELINES = { \# 'BoLeSpider.pipelines.BolespiderPipeline': 1, \# 'BoLeSpider.pipelines.JsonExporterPipleline': 1, 'BoLeSpider.pipelines.ArticleImagePipeline':1, 'BoLeSpider.pipelines.MysqlPipeline': 2, } import os IMAGES_URLS_FIELD = "front_image_url" # 原图片路径 project_dir = os.path.abspath(os.path.dirname(__file__)) IMAGES_STORE = os.path.join(project_dir, 'images') # 下载后图片保存位置 mian.py运行结果 数据库异步存储当我们爬虫海量网络数据的时候，爬取速度与存储速度便造成了冲突。采用前面交代的数据库存储技术可能会出现数据阻塞的情况。基于此，我们改进数据存储方式，使用异步存储。 pipline.py添加如下 from twisted.enterprise import adbapi '''MYSQL数据库存储方法2：异步操作处理,针对大数据量''' class MysqlTwistedPipline(object): def __init__(self, dbpool): self.dbpool = dbpool @classmethod def from_settings(cls, settings): # cls即MysqlTwistedPipline dbparms = dict( host = settings["MYSQL_HOST"], db = settings["MYSQL_DBNAME"], user = settings["MYSQL_USER"], passwd = settings["MYSQL_PASSWORD"], charset='utf8', cursorclass=MySQLdb.cursors.DictCursor, use_unicode=True ) dbpool = adbapi.ConnectionPool("MySQLdb", **dbparms) return cls(dbpool) def process_item(self, item, spider): #使用twisted将mysql插入变成异步执行 query = self.dbpool.runInteraction(self.do_insert, item) query.addErrback(self.handle_error, item, spider) #处理异常 def handle_error(self, failure, item, spider): #处理异步插入的异常 print (failure) def do_insert(self, cursor,item): insert_sql = """ insert into myarticles(title, createdate,url,dianzan,soucang,comment,img_url,img_path) VALUES(%s,%s,%s,%s,%s,%s,%s,%s) """ cursor.execute(insert_sql, (item["title"], item["create_date"], item["url"], item["dianzan"],item["soucang"],item["comment"],item["front_image_url"],item["front_image_path"])) setting.py添加如下 # 数据库设置 MYSQL_HOST = &quot;127.0.0.1&quot; MYSQL_DBNAME = &quot;test&quot; MYSQL_USER = &quot;root&quot; MYSQL_PASSWORD = &quot;admin&quot; mian.py运行结果 完整代码下载 源码请进【机器学习和自然语言QQ群：436303759】文件下载： 作者声明 本文版权归作者所有，旨在技术交流使用。未经作者同意禁止转载，转载后需在文章页面明显位置给出原文连接，否则相关责任自行承担。]]></content>
      <categories>
        <category>网络爬虫</category>
      </categories>
      <tags>
        <tag>scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python数据预处理：机器学习、人工智能通用技术（1）]]></title>
    <url>%2F2018%2F12%2F24%2FPython%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%81%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E9%80%9A%E7%94%A8%E6%8A%80%E6%9C%AF%EF%BC%881%EF%BC%89%2F</url>
    <content type="text"><![CDATA[摘要：大数据技术与我们日常生活越来越紧密，要做大数据，首要解决数据问题。原始数据存在大量不完整、不一致、有异常的数据，严重影响到数据建模的执行效率，甚至可能导致模型结果的偏差，因此要数据预处。数据预处理主要是将原始数据经过文本抽取、数据清理、数据集成、数据处理、数据变换、数据降维等处理后，不仅提高了数据质量，而且更好的提升算法模型性能。数据预处理在数据挖掘、自然语言处理、机器学习、深度学习算法中起着重要的作用。（本文原创，转载必须注明出处.） 什么是数据预处理数据预处理简而言之就是将原始数据装进一个预处理的黑匣子之后，产生出高质量数据用来适应相关技术或者算法模型。为了大家更明确的了解数据预处理，我们举个新闻分类的例子： 将原始的数据直接进行分类模型训练，分类器准确率和召回率都比较低。因为我们原始数据存在很多干扰项，比如的,是等这些所谓停用词特征对分类起的作用不大，很难达到工程应用。 我们将原始数据放假预处理黑匣子后，会自动过滤掉干扰数据，并且还会按照规约的方法体现每个词特征的重要性，然后将词特征压缩变换在数值型矩阵中，再通过分类器就会取得不错的效果，可以进行工程应用。 总结：数据预处理前的数据存在不完整、偏态、噪声、特征比重、特征维度、缺失值、错误值等问题；数据预处理后的数据存在完整、正态、干净、特征比重合适、特征维度合理、无缺失值等优点。 数据预处理方法： 数据清理：通过填写缺失的值、光滑噪声数据、识别或删除离群点并解决不一致性来清理数据。主要目标：格式标准化，异常数据清除，错误纠正，重复数据的清除。 数据集成：将数据由多个数据源合并成一个一致的数据存储，如数据仓库。 数据变换：通过平滑聚集，数据概化，规范化等方式将数据转换成适用于的形式。如把数据压缩到0.0-1.0区间。 数据归约：往往数据量非常大，在少量数据上进行挖掘分析需要很长的时间，数据归约技术可以用来得到数据集的归约表示，它小得多，但仍然接近于保持原数据的完整性，并结果与归约前结果相同或几乎相同。可以通过如聚集、删除冗余特征或聚类来降低数据的规模。 为什么做这门课程在初期学习阶段，大家精力着重于算法模型和调参上。实际情况是，有时候在算法改进上花费很多功夫，却不如在数据质量上的些许提高来的明显。另外，习惯于数据语料的拿来主义之后，当面对新的任务时候，却不知道如何下手？有的同学在处理英语时候游刃有余，面对中文数据预处理却不知所措。基于以上几个问题，结合作者工程经验，整理出了‘数据预处理’学习资料，本教程主要面对文本信息处理，在图片语音等数据语料处理上是有所区别的。 本课程能学到什么 文本批量抽取：涉及技术点包括pywin32插件安装使用、文档文本提取、PDF文本提取、文本抽取器的封装、方法参数的使用、遍历文件夹、编码问题、批量抽取文本信息。 数据清洗：包括yield生成器、高效读取文件、正则表达式的使用、清洗网页数据、清洗字符串、中文的繁简互相转换、缺失值的处理、噪声数据、异常数据清洗、批量清洗30万条新闻数据。 数据处理：包括结巴分词精讲、HanLP精讲、停用词的处理、NLTK的安装使用、高频词和低频词的处理、词性的选择、特征数据的提取、批量预处理30万条新闻数据。 数据向量化：包括词袋模型、词集模型、词向量的转化、缺失值和数据均衡、语料库技术、TFIDF、特征词比重、主成分分析、主题模型等、批量进行30万条数据向量化。 可视化技术：包括条形图、柱形图、散点图、饼图、热力图等，还有matplotlib、seabom、Axes3D综合使用进行三维可视化。 XGBoost竞赛神器：包括监督学习、文本分类、XGBoost原理、XGBoost算法实现、XGBoost调参、算法性能评估、30万条文档生成词典、30万条文档转化TFIDF、30万条文档转化生成LSI、训练分类器模型、抽样改进模型算法、特征维度改进模型算法、XGBoost实现30万条新闻数据文本分类 综上所述：数据预处理整体包括数据抽取—&gt;数据清洗—&gt;数据处理—&gt;数据向量化—&gt;可视化分析—&gt;模型构建。在整个过程中，我们每个章节相关性很强，首先对整个章节最终实现效果进行演示，然后拆分知识点分别讲解，最后将所有知识点整合起来做小节的实战。每个小节实战数据为下一个章节做铺垫，最后，一个综合实战分类案例串联所有知识点。 开发环境说明 开发语言: Python3.5.3 系统环境：window10操作系统 编程环境：Sublime 软件环境：Anaconda4.4.0 插件版本：均支持最新版本 sublime激活 打开Help &gt;Enter LICENSE ----- BEGIN LICENSE ----- sgbteam Single User License EA7E-1153259 8891CBB9 F1513E4F 1A3405C1 A865D53F 115F202E 7B91AB2D 0D2A40ED 352B269B 76E84F0B CD69BFC7 59F2DFEF E267328F 215652A3 E88F9D8F 4C38E3BA 5B2DAAE4 969624E7 DC9CD4D5 717FB40C 1B9738CF 20B3C4F1 E917B5B3 87C38D9C ACCE7DD8 5F7EF854 86B9743C FADC04AA FB0DA5C0 F913BE58 42FEA319 F954EFDD AE881E0B ------ END LICENSE ------ 解决Package Control报错 Package Control.sublime-settings]修改方法：Preferences &gt; Package Settings &gt; Package Control &gt; Settings - User添加： "channels": [ "http://cst.stu.126.net/u/json/cms/channel_v3.json", //"https://packagecontrol.io/channel_v3.json", //"https://web.archive.org/web/20160103232808/https://packagecontrol.io/channel_v3.json", //"https://gist.githubusercontent.com/nick1m/660ed046a096dae0b0ab/raw/e6e9e23a0bb48b44537f61025fbc359f8d586eb4/channel_v3.json" ] 项目演示原始数据 数据预览 数据清洗 生成词典 生成特征向量 生成LSI XGBoost新闻数据文本分类 目录列表☆ 理论介绍★ 实战演练 第1章 课程介绍 本章介绍课程概要与学习导读 1-1 为什么做这门课—☆ 1-2 课程整体介绍与导学—☆☆ 1-3 学习建议—☆☆ 1-4 课程开发环境介绍—☆ 1-5 文本分类项目演示—☆ 1-6 源码获取说明—☆☆☆ 1-7 总结与扩展—☆ 第2章 Python数据预处理之抽取文本信息 本章介绍常见数据类型，数据采集，文本提取面临的瓶颈，打造自己的文本批量抽取工具。 2.1 数据类型与采集方法—☆☆☆ 2.2 一堆杂乱无章的数据—☆ 2.3 文本抽取问题（3种方法对比）—☆ 2.4 Pywin32实现格式转换—☆☆ 2.3 Word转换TXT算法—★ 2.6 PDF转换TXT算法—★ 2.7 文本抽取工具—★★ 2.8 文本批量编码—★ 2.9 遍历读取文件—★★★ 2.10 实战案例1：遍历文件批量抽取新闻文本内容—★★★ 2.11 总结与扩展—☆☆ 第3章 Python数据预处理之清洗文本信息 本章介绍数据准备，高效读取文件，网络数据、文本数据清洗工作。 3.1 准备30万条新闻数据—☆ 3.2 yield生成器—★ 3.3 高效读取文件—★★ 3.4 数据缺失值—★★ 3.5 脏数据与噪声数据—★★ 3.6 正则清洗数据—★★ 3.7 清洗HTML数据—★★ 3.8 简繁字体转换—★★ 3.9 实战案例2：30万条新闻文本数据清洗—★★★ 3.10 总结与扩展—☆☆ 第4章 Python数据预处理之文本处理 本章介绍常见分词工具、jieba分词的核心操作、自定义规则提取特征词等方法处理文本数据。 4.1 常见分词工具—☆ 4.2 jieba分词（推荐）—★★★ 4.3 HanLP分词（扩展）—★★ 4.4 自定义去停词—★★ 4.5 词频统计—★★ 4.6 自定义去高低词频—★★ 4.7 自定义规则提取特征词—★★ 4.8 实战案例3：6万条新闻文本处理—★★★ 4.9 总结与扩展—☆☆ 第5章 Python数据预处理之文本特征向量化 本章介绍词集模型，词袋模型，具体处理偏态数据、缺少值问题，并进行特征向量化操作。 5.1 解析数据文件—★★ 5.2 词集模型—★★ 5.3 词袋模型—★★ 5.4 特征词转文本向量—★★★ 5.5 不均衡数据归一化处理—★★ 5.6 处理数据缺失值—★★ 5.7 实战案例4：新闻文本特征向量化—★★★ 5.8 总结与扩展—☆☆ 第6章 Python数据预处理之gensim文本向量化 本章介绍gensim进行文本向量化操作 6.1 gensim介绍—☆☆ 6.2 gensim构建语料词典—★ 6.3 gensim统计词频特征—★★ 6.4 gensim计算IF-IDF—★★ 6.5 潜在语义索引—★★★★ 6.6 生成主题模型—★★★★ 6.7 生成随机映射—★★★★ 6.8 分层狄利克雷过程—★★★★ 6.9 实战案例6：gensim实现新闻文本特征向量化—★★★★ 6.10 总结与扩展—☆☆☆ 第7章 Python数据预处理之特征降维 本章介绍最常见的特征降维方法主成分分析PCA及其实现 7.1 什么是降维—☆☆ 7.2 PCA 概述—☆☆☆ 7.3 PCA 应用场景—☆☆ 7.4 PCA 算法原理—★★★ 7.5 PCA 算法实现—★★★ 7.6 高维数据向低纬数据映射—★★ 7.7 前N个主成分特征—★★ 7.8 实战案例5：PCA技术实现新闻文本特征降维—★★★★ 7.9 总结与扩展—☆☆ 第8章 数据可视化分析 本章介绍可视化方法之一的matplotlib操作，以及相关可视化图形应用场景 8.1 matplotlib介绍—☆ 8.2 matplotlib绘制折线图—★★ 8.3 matplotlib绘制散点图—★★ 8.4 matplotlib绘制直方图—★★ 8.5 matplotlib绘制气温图表—★★ 8.6 matplotlib绘制三维图—★★★ 8.7 总结与扩展—☆ 第9章 XGBoost实现30万条新闻数据文本分类 本章介绍整合前面所有知识点，主要实现生成词典、生成tfidf向量、生成lsi向量、分类器参数训练、对新文本进行分类。前4个步骤可以看做是分类器的训练过程，而第五个阶段，则是使用训练得到的参数对新文本进行分类。 9.1 有监督学习—☆☆☆ 9.2 文本分类方法—☆☆☆ 9.3 XGBoost 原理—★★★★ 9.4 XGBoost 算法实现—★★★★ 9.5 准确率与召回率—☆ 9.6 F度量值—☆ 9.7 30万条文档生成词典—★★★ 9.8 30万条文档转化TFIDF—★★★ 9.9 30万条文档转化生成LSI—★★★★ 9.10 训练分类器模型—★★★★ 9.11 测试分类器模型—★★ 9.12 抽样改进模型算法—★★ 9.13 特征维度改进模型算法—★★ 9.14 训练集和测试集比率改进模型算法—★★ 9.15 综合实战：XGBoost实现30万条新闻数据文本分类—★★★★★ 9.11 总结与扩展—★★ 源码获取 源码请进【机器学习和自然语言QQ群：436303759】文件下载： 作者声明 本文版权归作者所有，旨在技术交流使用。未经作者同意禁止转载，转载后需在文章页面明显位置给出原文连接，否则相关责任自行承担。]]></content>
      <categories>
        <category>数据预处理</category>
      </categories>
      <tags>
        <tag>课程导学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python数据预处理之抽取文本信息（2）]]></title>
    <url>%2F2018%2F12%2F21%2F%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E4%B9%8B%E6%8A%BD%E5%8F%96%E6%96%87%E6%9C%AC%E4%BF%A1%E6%81%AF%EF%BC%882%EF%BC%89%2F</url>
    <content type="text"><![CDATA[摘要：大数据技术与我们日常生活越来越紧密，要做大数据，首要解决数据问题。原始数据存在大量不完整、不一致、有异常的数据，严重影响到数据建模的执行效率，甚至可能导致模型结果的偏差，因此要数据预处。数据预处理主要是将原始数据经过文本抽取、数据清理、数据集成、数据处理、数据变换、数据降维等处理后，不仅提高了数据质量，而且更好的提升算法模型性能。数据预处理在数据挖掘、自然语言处理、机器学习、深度学习算法中起着重要的作用。（本文原创，转载必须注明出处.） 数据类型与数据采集通常说的数据指的的数字、图表信息这些。在大数据领域所谓的数据总体包括结构化数据、半结构化数据和非结构化数据。 结构化数据 结构化的数据是指可以使用关系型数据库表示和存储，表现为二维形式的数据。一般特点是：数据以行为单位，一行数据表示一个实体的信息，每一行数据的属性是相同的。比如： id name age gender 1 张三 12 男 2 李花 13 女 3 王五 18 男 数据特点：关系模型数据，关系数据库表示。 常见格式：比如MySQL、Oracle、SQL Server等。 应用场合：数据库、系统网站、数据备份、ERP等。 数据采集：DB导出、SQL等方式。 结构化的数据的存储和排列是很有规律的，这对查询和修改等操作很有帮助。但是，它的扩展性不好。 半结构化数据 半结构化数据是结构化数据的一种形式，它并不符合关系型数据库或其他数据表的形式关联起来的数据模型结构，但包含相关标记，用来分隔语义元素以及对记录和字段进行分层。因此，它也被称为自描述的结构。半结构化数据，属于同一类实体可以有不同的属性，即使他们被组合在一起，这些属性的顺序并不重要。常见的半结构数据有XML如下： 李花 13 女 数据特点：非关系模型数据，还有一定的格式。 常见格式：比如Email、HTML、XML、JSON等。 应用场合：邮件系统、档案系统、新闻网站等。 数据采集：网络爬虫、数据解析等方式。 不同的半结构化数据的属性的个数是不定的。有些人说半结构化数据是以树或者图的数据结构存储的数据，上面的例子中，标签是树的根节点，和标签是子节点。通过这样的数据格式，可以自由地表达很多有用的信息，包括自我描述信息（元数据）。所以，半结构化数据的扩展性是很好的。 非结构化数据 就是没有固定结构的数据。各种文档、图片、视频/音频等都属于非结构化数据。对于这类数据，我们一般直接整体进行存储，而且一般存储为二进制的数据格式。如下所示： 数据特点：没有固定格式的数据 常见格式：Word、PDF、PPT、图片、音视频等。 应用场合：图片识别、人脸识别、医疗影像、文本分析等。 数据采集：网络爬虫、数据存档等方式。 常见的文本抽取方法针对数据不同形式，通过特定方式的数据采集方式（文档下载、数据库导出、网络爬虫、语音收集、图片解析等等）获取数据，无论是结构化的数据库文件、半结构化的网页数据，还是非结构化的图片、音视频。我们最终的目的都是将数据传入到电脑之中，通过算法模型挖掘其潜在的价值，为最终的AI技术做支撑。不同的是，在结构化和半结构化数据数据集成过程中，我们可以提取相关文本信息，做进一步的数据预处理；而非结构化的图片、音视频我们采用一定的技术手段，获取其对应的数据点矩阵。这一点不太容易理解，我们比如说想解析一张图片的数据，我们知道图片是有长宽高组成的，还包括红蓝绿三种基本色。那么我们就找到对应的多维特征，采用数据点占位表示，比如： 图片名 长(bit) 宽(bit) 红 绿 蓝 猫1 12 100 0 0 1 狗2 101 234 1 1 1 猪3 202 24 0 1 0 上面就数据表示猫1这张图片，长宽位点（12,100）处只有蓝色构成；狗2这张图片，长宽位点（101,234）处有红绿蓝3中色构成；猪3这张图片，长宽位点（202,24）处只有绿构成。这就是非结构数据图片转化为数值型数据的原理。完整流程数据挖掘的流程图如下所示： 我根据不同的数据类型，采用对应的数据采集方式获取目标数据。这时候的数据质量很差，存在文本格式不同，数据表示形式不同等诸多问题。这里我们单纯的考虑文本信息的处理，就文本信息而言，你采集的数据可能是网页、数据库文件、pdf文档、word文档等等。我们想去处理这些数据，还需要对数据进行集成即转化为统一的数据格式，这里我们就需要文本信息抽取，常见的抽取方式包括以下几个内容：在线格式转换工具、office内置格式转换、自己开发文本抽取工具。详见下图： 经过实际操作会发现采用在线格式转换工具存在几个弊端，其限制文件转化的数据，要么就是收费的；而采用本地的office自带文档，一个个另存为文本，肯定不现实。基于上述情况，我们对工具抽取的弊端总结如下： 格式转换后，识别乱码较多 不支持或者限制支持批量处理 批量转化收费问题 格式转换后的txt文件存在编码问题 生成文件名一堆数字乱码 操作不够灵活便捷 我们针对以上问题，就去寻求解决方式，那就是自己动手丰衣足食，我们自己去打造批量文本抽取问题，我们期待效果是： 支持PDF/Word等多格式文本抽取 自动过滤不符合指定格式的文件 生成的目标文件与原文件目录一致 生成文档采用统一的编码格式保存（如：UTF-8 ） 支持默认保存路径和自定义保存路径 抽取Word文档文本做word文档抽取工作，我们运行环境是在win10-64bit下，python3.5，Anaconda4.4版本下执行的，所使用的插件是win32com。下载地址：https://pan.baidu.com/s/1-2BsiTs8XjMIe5Gnh_GFjw 密码: 7j3t预装完win32com以后，以下代码便完成抽取word文本信息。 算法思路： 定义文件路径和转存路径：split 修改新的文件名：fnmatch 设置完整的保存路径：join 启动应用程序格式转换：Dispatch 保存文本：SaveAs 算法流程： 代码实现： coding=utf-8 “””Description: Word文件转化TXT文本Author：伏草惟存Prompt: code in Python3 envInstall package： pip install pypiwin32“”” import os,fnmatchfrom win32com import client as wcfrom win32com.client import Dispatch ‘’’功能描述：word文件转存txt文件，默认存储当前路径下；用户可以指定存储文件路径。参数描述：1 filePath：文件路径 2 savePath： 指定保存路径‘’’def Word2Txt(filePath,savePath=’’): # 1 切分文件上级目录和文件名 dirs,filename = os.path.split(filePath) # print(dirs,&#39;\n&#39;,filename) # 2 修改转化后的文件名 new_name = &#39;&#39; if fnmatch.fnmatch(filename,&#39;*.doc&#39;): new_name = filename[:-4]+&#39;.txt&#39; elif fnmatch.fnmatch(filename,&#39;*.docx&#39;): new_name = filename[:-5]+&#39;.txt&#39; else: return print(&#39;-&gt;&#39;,new_name) # 3 文件转化后的保存路径 if savePath==&#39;&#39;: savePath = dirs else: savePath = savePath word_to_txt = os.path.join(savePath,new_name) print(&#39;-&gt;&#39;,word_to_txt) # 4 加载处理应用,word转化txt wordapp = wc.Dispatch(&#39;Word.Application&#39;) mytxt = wordapp.Documents.Open(filePath) mytxt.SaveAs(word_to_txt,4) mytxt.Close() if name==’main‘: filepath = os.path.abspath(r’../dataSet/filename.doc’) # savepath = &#39;&#39; Word2Txt(filepath) &lt;/pre&gt; 抽取PDF文档文本 算法思路： 定义文件路径和转存路径：split 修改新的文件名：fnmatch 设置完整的保存路径：join 启动应用程序格式转换：Dispatch 保存文本：SaveAs 算法流程： 代码实现： # coding=utf-8 """ Description: PDF文件转化TXT文本 Author：伏草惟存 Prompt: code in Python3 env """ import os,fnmatch from win32com import client as wc from win32com.client import Dispatch,gencache ''' 功能描述：pdf文件转化txt文本 参数描述：1 filePath：文件路径 2 savePath： 指定保存路径 ''' def Pdf2Txt(filePath,savePath=''): # 1 切分文件上级目录和文件名 dirs,filename = os.path.split(filePath) # print('目录：',dirs,'\n文件名：',filename) # 2 修改转化后的文件名 new_name = "" if fnmatch.fnmatch(filename,'*.pdf') or fnmatch.fnmatch(filename,'*.PDF'): new_name = filename[:-4]+'.txt' # 截取".pdf"之前的文件名 else: return print('新的文件名：',new_name) # 3 文件转化后的保存路径 if savePath=="": savePath = dirs else: savePath = savePath pdf_to_txt = os.path.join(savePath,new_name) print('保存路径：',pdf_to_txt) # 4 加载处理应用,pdf转化txt wordapp = wc.Dispatch('Word.Application') mytxt = wordapp.Documents.Open(filePath) mytxt.SaveAs(pdf_to_txt,4) mytxt.Close() if __name__=='__main__': # 使用绝对路径 filePath = os.path.abspath(r'../dataSet/Corpus/pdftotxt/2018年世界新闻自由日.pdf') # savePath = r'E:\\' Pdf2Txt(filePath) 文本抽取工具与编码 算法思路： 定义文件夹路径和转存夹路径：split 修改新的文件名：TranType(filename， typename)、fnmatch 设置完整的保存路径：join 启动应用程序格式转换：Dispatch 保存文本：SaveAs 代码实现 coding=utf-8 “””Description: 多文档格式转换工具Author：伏草惟存Prompt: code in Python3 env“”” import os,fnmatchfrom win32com import client as wcfrom win32com.client import Dispatch,gencache ‘’’功能描述：抽取文件文本信息参数描述：1 filePath：文件路径 2 savePath： 指定保存路径‘’’def Files2Txt(filePath,savePath=’’): try: # 1 切分文件上级目录和文件名 dirs,filename = os.path.split(filePath) # print(&#39;目录：&#39;,dirs,&#39;\n文件名：&#39;,filename) # 2 修改转化后的文件名 typename = os.path.splitext(filename)[-1].lower() # 获取后缀 new_name = TranType(filename,typename) # print(&#39;新的文件名：&#39;,new_name) # 3 文件转化后的保存路径 if savePath==&quot;&quot;: savePath = dirs else: savePath = savePath new_save_path = os.path.join(savePath,new_name) print(&#39;保存路径：&#39;,new_save_path) # 4 加载处理应用 wordapp = wc.Dispatch(&#39;Word.Application&#39;) mytxt = wordapp.Documents.Open(filePath) mytxt.SaveAs(new_save_path,4) mytxt.Close() except Exception as e: pass ‘’’功能描述：根据文件后缀修改文件名参数描述：1 filePath：文件路径 2 typename 文件后缀返回数据：new_name 返回修改后的文件名‘’’def TranType(filename,typename): # 新的文件名称 new_name = &quot;&quot; if typename == &#39;.pdf&#39; : # pdf-&gt;txt if fnmatch.fnmatch(filename,&#39;*.pdf&#39;) : new_name = filename[:-4]+&#39;.txt&#39; # 截取&quot;.pdf&quot;之前的文件名 else: return elif typename == &#39;.doc&#39; or typename == &#39;.docx&#39; : # word-&gt;txt if fnmatch.fnmatch(filename, &#39;*.doc&#39;) : new_name = filename[:-4]+&#39;.txt&#39; elif fnmatch.fnmatch(filename, &#39;*.docx&#39;): new_name = filename[:-5]+&#39;.txt&#39; else: return else: print(&#39;警告：\n您输入[&#39;,typename,&#39;]不合法，本工具支持pdf/doc/docx格式,请输入正确格式。&#39;) return return new_name if name == ‘main‘: filePath1 = os.path.abspath(r’../dataSet/Corpus/wordtotxt/一种改进的朴素贝叶斯文本分类方法研究.doc’) filePath2 = os.path.abspath(r’../dataSet/Corpus/pdftotxt/改进朴素贝叶斯文本分类方法研究.pdf’) filePath3 = os.path.abspath(r’../dataSet/Corpus/wordtotxt/科技项目数据挖掘决策架构.docx’) Files2Txt(filePath3)&lt;/pre&gt; 遍历读取文件 遍历文件的类TraversalFun ： TraversalDir、 AllFiles 遍历目录文件TraversalDir ： AllFiles(self.rootDir) 递归遍历文件AllFiles： AllFiles(self,rootDir) 判断是否为文件isfile ：打印出文件名 判断是否是目录isdir ：递归遍历 遍历文件源码实现 # coding=utf-8 """ Description: 遍历读取文件名 Author：伏草惟存 Prompt: code in Python3 env """ import os,time ''' 功能描述：遍历目录处理子文件 参数描述： 1 rootDir 目标文件的根目录 ''' class TraversalFun(): # 1 初始化 def __init__(self,rootDir): self.rootDir = rootDir # 目录路径 # 2 遍历目录文件 def TraversalDir(self): TraversalFun.AllFiles(self,self.rootDir) # 3 递归遍历所有文件，并提供具体文件操作功能 def AllFiles(self,rootDir): # 返回指定目录包含的文件或文件夹的名字的列表 for lists in os.listdir(rootDir): # 待处理文件夹名字集合 path = os.path.join(rootDir, lists) # 核心算法，对文件具体操作 if os.path.isfile(path): print(os.path.abspath(path)) # 递归遍历文件目录 elif os.path.isdir(path): TraversalFun.AllFiles(self,path) if __name__ == '__main__': time_start=time.time() # 根目录文件路径 rootDir = r"../dataSet/Corpus/EnPapers" tra=TraversalFun(rootDir) # 默认方法参数打印所有文件路径 tra.TraversalDir() # 遍历文件并进行相关操作 time_end=time.time() print('totally cost',time_end-time_start,'s') 实战案例：遍历文件批量抽取新闻文本内容 算法思路 引用外部文本抽取模块：import ExtractTxt as ET 参数方法使用：TraversalFun(rootDir,ET.Files2Txt,saveDir) 创建保存根目录：os.path.abspath 递归遍历文件：func(path, save_dir) 源码实现 # coding=utf-8 """ Description: 批量文档格式自动转化txt Author：伏草惟存 Prompt: code in Python3 env """ import ExtractTxt as ET import os,time ''' 功能描述：遍历目录，对子文件单独处理 参数描述：1 rootDir 根目录 2 deffun：方法参数 3 saveDir: 保存路径 ''' class TraversalFun(): # 1 初始化 def __init__(self,rootDir,func=None,saveDir=""): self.rootDir = rootDir # 目录路径 self.func = func # 参数方法 self.saveDir = saveDir # 保存路径 # 2 遍历目录文件 def TraversalDir(self): # 切分文件上级目录和文件名 dirs,latername = os.path.split(self.rootDir) # print(rootDir,'\n',dirs,'\n',latername) # 保存目录 save_dir = "" if self.saveDir=="": # 默认文件保存路径 save_dir = os.path.abspath(os.path.join(dirs,'new_'+latername)) else: save_dir = self.saveDir # 创建目录文件 if not os.path.exists(save_dir): os.makedirs(save_dir) print("保存目录：\n"+save_dir) # 遍历文件并将其转化txt文件 TraversalFun.AllFiles(self,self.rootDir,save_dir) # 3 递归遍历所有文件，并提供具体文件操作功能 def AllFiles(self,rootDir,save_dir=''): # 返回指定目录包含的文件或文件夹的名字的列表 for lists in os.listdir(rootDir): # 待处理文件夹名字集合 path = os.path.join(rootDir, lists) # 核心算法，对文件具体操作 if os.path.isfile(path): self.func(os.path.abspath(path),os.path.abspath(save_dir)) # 递归遍历文件目录 if os.path.isdir(path): newpath = os.path.join(save_dir, lists) if not os.path.exists(newpath): os.mkdir(newpath) TraversalFun.AllFiles(self,path,newpath) if __name__ == '__main__': time_start=time.time() # 根目录文件路径 rootDir = r"../dataSet/Corpus/EnPapers" # saveDir = r"./Corpus/TxtEnPapers" tra=TraversalFun(rootDir,ET.Files2Txt) # 默认方法参数打印所有文件路径 tra.TraversalDir() # 遍历文件并进行相关操作 time_end=time.time() print('totally cost',time_end-time_start,'s') 源码获取 源码请进【机器学习和自然语言QQ群：436303759】文件下载： 作者声明 本文版权归作者所有，旨在技术交流使用。未经作者同意禁止转载，转载后需在文章页面明显位置给出原文连接，否则相关责任自行承担。文版权归作者所有，旨在技术交流使用。未经作者同意禁止转载，转载后需在文章页面明显位置给出原文连接，否则相关责任自行承担。]]></content>
      <categories>
        <category>数据预处理</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>数据预处理</tag>
        <tag>pywin32</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[天府大讲堂：5G时代的物联网发展趋势与产业变革]]></title>
    <url>%2F2018%2F11%2F29%2F%E5%A4%A9%E5%BA%9C%E5%A4%A7%E8%AE%B2%E5%A0%82%EF%BC%9A5G%E6%97%B6%E4%BB%A3%E7%9A%84%E7%89%A9%E8%81%94%E7%BD%91%E5%8F%91%E5%B1%95%E8%B6%8B%E5%8A%BF%E4%B8%8E%E4%BA%A7%E4%B8%9A%E5%8F%98%E9%9D%A9%2F</url>
    <content type="text"><![CDATA[摘要：国家973物联网首席科学家，中科院上海微系统与信息技术研究所副所长，无锡物联网产业研究院院长刘海涛教授讲授的5G时代的物联网发展趋势与产业变革意义深刻。作者根据天府大讲堂听讲内容加工整理所得，旨在进行学术交流使用，严禁商业炒作和不法行为。（本文原创，转载必须注明出处.） 1 特邀嘉宾:物联网首席科学家刘海涛 刘海涛，国家973物联网首席科学家，中科院上海微系统与信息技术研究所副所长，无锡物联网产业研究院院长，国家中长期科技发展规划03专项“新一代宽带无线移动通信网”总体组专家，国家传感网标准化工作组组长，国家物联网产业技术联盟筹备组组长。 2 物联网的发展2.1 物联网与互联网的输入方式物联网大家都耳熟能详，谈及物联网绕不开要互联网，互联网大家并不陌生，互联互通。其数据输入方式主要是人为输入。而物联网的数据输入方式则与之不同，主要是自动采集。两者有何区别呢？大家都知道人为输入数据存在很多问题，比如：手误的输入、数据篡改、数据欺诈等等。典型的案例就是电信网络诈骗问题，导致社会失信问题，这里就凸显出互联网的负面因素。而物联网则是数据自动输入，客观上避免上述情况存在。诸如下面的案例： 我们尝试得到我悬挂在手上苹果手机的质量 互联网方式：数据的输入 物联网数据：根据材料属性、长度、宽度、摇摆角度等等真实数据。最终分析出一个结果 不怀好意的即使篡改，也不可能考虑到每个维度和角度，从而带来数据真实性。 2.2 物联网与互联网的服务方式 互联网其实可以称之为共享系统，主要前期搞互联网的那一拨人通信出身才有了这个名字，本质上还是数据共享的。其宗旨是数据服务或者叫着信息服务。 物联网则是事件服务。 2.3 物联网与互联网的思维方式 互联网的思维是具备清晰逻辑性的，做一件事情先做A再做B后做C都是明确的，即A-&gt;B-&gt;C…具备主观性和确定性。 物联网则是不确定性思维。（举例如下） 偷手机事件 假如某个坏人想偷你的手机，需要先确定好日期，某年某月。再指定好地点，实验室还是车上等等。还有风速多少，阴雨天等等。进房间左脚踢开门还是右脚。等等这些客观环境都是不确定的。从而也解释了上文物联网服务事件。 2.4 物联网与互联网的本质区别物联网和互联网本质上区别在于： 互联网：本质上就是数据信息共享 智能化：本质上就是仿人类思维 物联网：本质上就是仿团队协作思维 智能化只是物联网的一个节点，其将各个节点联合起来就形成了具备有协同、有组织、有纪律的团队协作。 2.5 传感器与物联网有些人认为物联网就是智能化，智能化就具备感知，传感器恰好就是物联网。这种观点是错误的，首先我们类比电脑，识别一个水杯，将水杯转化为电流形式分析处理的，而人的眼睛看到水杯，也是将其特征转换为脑电流、脑电波等复杂形式提供给大脑的。所以这个数据传递的过程本身不具备感知性。这样，也不难理解传感器不是物联网了，传感器只负责信息采集与信号转换，不具有感知性。 2.6 RFID与物联网RFID（射频标签），最早是60年代美国军方用来识别敌人信息的。其机理等同于0,1,2,3,4..一堆数字，等同于条形码，等同于二维码。不同的是数字人眼就可以识别，条形码、二维码手机等可以识别。所以RFID本身还是互联网技术不属于物联网。 RFID与茅台酒溯源可行吗 RFID作为食品溯源方面，早先茅台酒销量好，总会有坏人仿制假冒，干扰市场经济。为此呢，茅台酒的老总找我（刘海涛院长）说茅台酒用了100多种溯源方式，最终还是有假酒。其根本原因在于RFID属于被动标识。而物联网则属于主动感知，可用于实体世界。物联网用来食品溯源将会取得很好的效果。 2.7 物联网与互联网是否对立前面讲了那么多，有些人怀疑是不是物联网和互联网对立的？还有人说“互联网即将消失，物联网无所不能”（某IT巨头大佬是话，编者注）？这些观点都是错误的。 互联网不会消失，物联网也不是无所不能，也不是万物互联。两者是相互融合的关系。提到这里我们联系下5G。 1G时代：通话 2G时代：通话，还可以发短信 3G时代：通话，短信，图片视频 4G时代：移动+互联网 5G时代：将是由高速公路过度到航空 我们互联网时代的使用者是数亿，手机移动时代则是数十亿，而物联网时代会是数千亿。 3 物联网与第四次工业革命3.1 互第四次工业革命 第一次工业革命：蒸汽机的发明 第二次工业革命：电力技术的发明 第三次工业革命：制造业跟IT电子结合 第四次工业革命：物理信息系统（德国叫法，即互联网）支持下的第四次革命，即物联网。 3.2 互联网与物联网计费问题互联网时代的计费是怎么样的形式呢？比如我们熟悉的淘宝、天猫这些。这个是收费的，当然作为购买用户是不收费的，针对商家收费。假如你是一个商家，在淘宝上你交的费，那么马云（严格说是阿里集团）拿到多少呢？首先马云要交通道费（各大电信服务商），剩下的数据费（平台信息费）就是马云的了。 如果在物联网时代，一个商家在淘宝上交的费用就省去了（信息感知费） 3.3 物联网与类比软件我们对软件并不陌生，通常包括ERP软件、游戏软件、安全软件、画图软件、系统软件…分类庞杂。然而这些搞软件最牛的是哪家公司？是微软公司，为什么呢？因为它做的是系统软件。那么什么是系统软件？ 操作系统：所有软件的共性部分 物联网的核心部分就是：共性架构+应用子集 （ps：当前说的物联网技术很多都是夸夸其谈） 4 物联网的产业变革4.1 物联网与智能交通我们现在开着汽车在农村穿行，如果没有红绿灯影响不大。倘若在大城市比如成都这样的城市，没有红绿灯就肯定会混论不堪。于是人们利用智慧发明了红绿灯，指导车俩运行，这里就是智能交通。为什么我说是智能交通而不是智慧交通？因为当凌晨的时候，路上没有多少车俩，你遇到红绿灯还需要等待，显示是不合理的。这里就是红绿灯控制车流量。而物联网应用到智能交通里面将改变这种局面，形成车流量控制红绿灯。 4.2 物联网与安防我们当前采用的智能安防都是事后的，比如某地发生一起刑事案件，我们事后采用智能手段进行追踪、定位、抓捕、问责。而物联网时代的安防将改变这种局面，采用事前预防的方法，从而极大的提高我们安防问题。 4.3 物联网与金融结合提起电子金融，我们首先想起支付宝和微信。互联网金融正面代表阿里、京东等等我们大家所熟知，更多的负面代表同样存在。其根本原因在于信息不对称，骗子们采用信息行骗，挑战社会公信度。传统时代，一个骗子欺骗需要面对面，一天骗2个，一辈子能骗多少？现在互联网诈骗可能达到几十万人、几百万人，涉及金额可以达到几百亿。而到了物联网时代将会改变支付手段和交互方式，让骗子无处行骗。当然物联网在金融领域还可以做很多事情。 5 声明以上内容作者根据刘海涛老师天府大讲堂所做的笔记和回忆加工整理所得，旨在进行学术交流使用，严禁商业炒作和不法行为。若有错误或不妥之处敬请联系作者。转载需作者同意，否则追究相关责任。]]></content>
      <categories>
        <category>物联网</category>
      </categories>
      <tags>
        <tag>物联网</tag>
        <tag>人工智能</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一步步教你轻松学支持向量机SVM算法之案例篇2]]></title>
    <url>%2F2018%2F10%2F16%2F%E4%B8%80%E6%AD%A5%E6%AD%A5%E6%95%99%E4%BD%A0%E8%BD%BB%E6%9D%BE%E5%AD%A6%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BASVM%E7%AE%97%E6%B3%95%E4%B9%8B%E6%A1%88%E4%BE%8B%E7%AF%872%2F</url>
    <content type="text"><![CDATA[摘要：支持向量机即SVM(Support Vector Machine) ，是一种监督学习算法，属于分类的范畴。首先，支持向量机不是一种机器，而是一种机器学习算法。在数据挖掘的应用中，与无监督学习的聚类相对应和区别。广泛应用于机器学习，计算机视觉和数据挖掘当中。（本文原创，转载必须注明出处.） 上节回顾，还记得如何基于svm进行分类？ 寻找最大间隔怎么寻找最大间隔 点到超平面的距离 分隔超平面函数间距: \(y(x)=w^Tx+b\) 分类的结果： \(f(x)=sign(w^Tx+b)\) (sign表示&gt;0为1，&lt;0为-1，=0为0) 点到超平面的几何间距: \(d(x)=(w^Tx+b)/||w||\) （||w||表示w矩阵的二范数=&gt; \(\sqrt{w^T*w}\), 点到超平面的距离也是类似的） 拉格朗日乘子法 类别标签用-1、1，是为了后期方便 \(label*(w^Tx+b)\) 的标识和距离计算；如果 \(label*(w^Tx+b)&gt;0\) 表示预测正确，否则预测错误。 现在目标很明确，就是要找到w和b，因此我们必须要找到最小间隔的数据点，也就是前面所说的支持向量。 让最小的距离取最大.(最小的距离：就是最小间隔的数据点；最大：就是最大间距，为了找出最优超平面—最终就是支持向量) 目标函数：\(arg: max_{w, b} \left( min[label*(w^Tx+b)]*\frac{1}{||w||} \right) \) 如果 \(label*(w^Tx+b)&gt;0\) 表示预测正确，也称函数间隔，\(||w||\) 可以理解为归一化，也称几何间隔。 令 \(label*(w^Tx+b)&gt;=1\)， 因为0～1之间，得到的点是存在误判的可能性，所以要保障 \(min[label*(w^Tx+b)]=1\)，才能更好降低噪音数据影响。 所以本质上是求 \(arg: max_{关于w, b} \frac{1}{||w||} \)；也就说，我们约束(前提)条件是: \(label*(w^Tx+b)=1\) 新的目标函数求解： \(arg: max_{关于w, b} \frac{1}{||w||} \) =&gt; 就是求: \(arg: min_{关于w, b} ||w|| \) (求矩阵会比较麻烦，如果x只是 \(\frac{1}{2}*x^2\) 的偏导数，那么。。同样是求最小值) =&gt; 就是求: \(arg: min_{关于w, b} (\frac{1}{2}*||w||^2)\) (二次函数求导，求极值，平方也方便计算) 本质上就是求线性不等式的二次优化问题(求分隔超平面，等价于求解相应的凸二次规划问题) 通过拉格朗日乘子法，求二次优化问题 假设需要求极值的目标函数 (objective function) 为 f(x,y)，限制条件为 φ(x,y)=M # M=1 设g(x,y)=M-φ(x,y) # 临时φ(x,y)表示下文中 \(label*(w^Tx+b)\) 定义一个新函数: F(x,y,λ)=f(x,y)+λg(x,y) a为λ（a&gt;=0），代表要引入的拉格朗日乘子(Lagrange multiplier) 那么： \(L(w,b,\alpha)=\frac{1}{2} * ||w||^2 + \sum_{i=1}^{n} \alpha_i * [1 - label * (w^Tx+b)]\) 因为：\(label*(w^Tx+b)&gt;=1, \alpha&gt;=0\) , 所以 \(\alpha*[1-label*(w^Tx+b)]&lt;=0\) , \(\sum_{i=1}^{n} \alpha_i * [1-label*(w^Tx+b)]&lt;=0\) 当 \(label*(w^Tx+b)&gt;1\) 则 \(\alpha=0\) ，表示该点为非支持向量 相当于求解： \(max_{关于\alpha} L(w,b,\alpha) = \frac{1}{2} *||w||^2\) 如果求： \( min_{关于w, b}\frac{1}{2} *||w||^2 \) , 也就是要求： \(min_{关于w, b} \left( max_{关于\alpha} L(w,b,\alpha)\right)\) 现在转化到对偶问题的求解 \(min_{关于w, b} \left(max_{关于\alpha} L(w,b,\alpha) \right) \) &gt;= \(max_{关于\alpha} \left(min_{关于w, b}\ L(w,b,\alpha) \right) \) 现在分2步 先求： \(min_{关于w, b} L(w,b,\alpha)=\frac{1}{2} * ||w||^2 + \sum_{i=1}^n \alpha_i * [1 - label * (w^Tx+b)]\) 就是求L(w,b,a)关于[w, b]的偏导数, 得到w和b的值，并化简为：L和a的方程。 参考： 如果公式推导还是不懂，也可以参考《统计学习方法》李航-P103&lt;学习的对偶算法&gt; 终于得到课本上的公式： \(max_{\alpha} [ \sum_{i=1}^m \alpha_i - \frac{1}{2}\sum^m ·\alpha_i·\alpha_j·\langle x^{(i)}, x^{(j)} \rangle ]\) 约束条件： \(a&gt;=0\) 并且 \(\sum_{i=1}^{m} a_i·label_i=0\) 拉格朗日乘子法理解 要求\( f(x,y)\),在\( g(x,y)=c\)时的最大值时，我们可以引入新变量拉格朗日乘数\(\lambda\) ，这时我们只需要下列拉格朗日函数的极值,为了帮助大家更好的理解，请参照下图（绿线标出的是约束g(x,y) = c的点的轨迹。蓝线是f的等高线。箭头表示斜率，和等高线的法线平行。）： {\mathcal {L}}(x,y,\lambda )=f(x,y)+\lambda \cdot {\Big (}g(x,y)-c{\Big )} 拉格朗日乘子证明过程 设函数\( f(x,y)\)在 \( A\)点处有极值\( \kappa\)，且在\( A\)点的邻域内连续。则在\( A\)点处有\( f\left(x,y\right)=\kappa \)另有一常值函数:\( g\left(x,y\right)=c\)二函数在 \(A\)点处的全微分为 \mathrm {d} f={\frac {\partial {f}}{\partial {x}}}\mathrm {d} x+{\frac {\partial {f}}{\partial {y}}}\mathrm {d} y=0 \mathrm {d} g={\frac {\partial {g}}{\partial {x}}}\mathrm {d} x+{\frac {\partial {g}}{\partial {y}}}\mathrm {d} y=0由于\( \mathrm {d} x\)和\(\mathrm{d}y\)是任取的无穷小量，故该线性方程组的系数成比例，有 {\dfrac {\dfrac {\partial {f}}{\partial {x}}}{\dfrac {\partial {g}}{\partial {x}}}}={\dfrac {\dfrac {\partial {f}}{\partial {y}}}{\dfrac {\partial {g}}{\partial {y}}}}=-\lambda即 {\frac {\partial {f}}{\partial {y}}}+\lambda \cdot {\frac {\partial {g}}{\partial {y}}}=0 {\frac {\partial {f}}{\partial {y}}}+\lambda \cdot {\frac {\partial {g}}{\partial {y}}}=0将上二式分别乘以\( \mathrm{d}x \)和\(\mathrm{d}y\)，再相加并积分，得到一新函数 {\mathcal {L}}(x,y,\lambda )=f(x,y)+\lambda \cdot g(x,y)那么，求原函数极值的问题就转化为求该函数极值的问题。类似地，这种求极值的方法也可以推广到多维函数 \(f\left(x_1,\ldots ,x_n\right)\)。 拉格朗日乘子简单例子 求此方程的最小值：\( f(x, y) = x^2 y\),同时未知数满足\(x^2 + y^2 = 1\)因为只有一个未知数的限制条件，我们只需要用一个乘数\(\lambda \). g (x, y) = x^2 +y^2 -1\Phi (x, y, \lambda) = f(x,y) + \lambda g(x, y) = x^2 y + \lambda (x^2 + y^2 - 1)将所有 \(\Phi\) 方程的偏微分设为零，得到一个方程组，最小值是以下方程组的解中的一个： 2 x y + 2 \lambda x = 0x^2 + 2 \lambda y = 0x^2 + y^2 -1 = 0 松弛变量(slack variable) 我们知道几乎所有的数据都不那么干净, 通过引入松弛变量来 允许数据点可以处于分隔面错误的一侧。约束条件： \(C&gt;=a&gt;=0\) 并且 \(\sum_{i=1}^{m} a_i·label_i=0\)总的来说： 常量C是 惩罚因子, 表示离群点的权重（用于控制“最大化间隔”和“保证大部分点的函数间隔小于1.0” ） \(label*(w^Tx+b) &gt; 1\) and alpha = 0 (在边界外，就是非支持向量) \(label*(w^Tx+b) = 1\) and 0&lt; alpha &lt; C (在分割超平面上，就支持向量) \(label*(w^Tx+b) &lt; 1\) and alpha = C (在分割超平面内，是误差点 -&gt; C表示它该受到的惩罚因子程度) C值越大，表示离群点影响越大，就越容易过度拟合；反之有可能欠拟合。 我们看到，目标函数控制了离群点的数目和程度，使大部分样本点仍然遵守限制条件。 例如：正类有10000个样本，而负类只给了100个（C越大表示100个负样本的影响越大，就会出现过度拟合，所以C决定了负样本对模型拟合程度的影响！，C就是一个非常关键的优化点！） 这一结论十分直接，SVM中的主要工作就是要求解 alpha. SMO 高效优化算法SVM有很多种实现，最流行的一种实现是： 序列最小优化(Sequential Minimal Optimization, SMO)算法。下面还会介绍一种称为 核函数(kernel) 的方式将SVM扩展到更多数据集上。 注意：SVM几何含义比较直观，但其算法实现较复杂，牵扯大量数学公式的推导。 序列最小优化(Sequential Minimal Optimization, SMO) John Platt于1996年创建 SMO用途：用于训练 SVM SMO目标：求出一系列 alpha 和 b,一旦求出 alpha，就很容易计算出权重向量 w 并得到分隔超平面。 SMO思想：是将大优化问题分解为多个小优化问题来求解的。 SMO原理：每次循环选择两个 alpha 进行优化处理，一旦找出一对合适的 alpha，那么就增大一个同时减少一个。 这里指的合适必须要符合一定的条件 这两个 alpha 必须要在间隔边界之外 这两个 alpha 还没有进行过区间化处理或者不在边界上。 之所以要同时改变2个 alpha；原因是我们有一个约束条件： \(\sum_{i=1}^{m} a_i·label_i=0\)；如果只是修改一个 alpha，很可能导致约束条件失效。 SMO 伪代码大致如下： 创建一个 alpha 向量并将其初始化为0向量 当迭代次数小于最大迭代次数时(外循环) 对数据集中的每个数据向量(内循环)： 如果该数据向量可以被优化 随机选择另外一个数据向量 同时优化这两个向量 如果两个向量都不能被优化，退出内循环 如果所有向量都没被优化，增加迭代数目，继续下一次循环 SVM 开发流程收集数据：可以使用任意方法。 准备数据：需要数值型数据。 分析数据：有助于可视化分隔超平面。 训练算法：SVM的大部分时间都源自训练，该过程主要实现两个参数的调优。 测试算法：十分简单的计算过程就可以实现。 使用算法：几乎所有分类问题都可以使用SVM，值得一提的是，SVM本身是一个二类分类器，对多类问题应用SVM需要对代码做一些修改。 SVM 算法特点优点：泛化（由具体的、个别的扩大为一般的，就是说：模型训练完后的新样本）错误率低，计算开销不大，结果易理解。 缺点：对参数调节和核函数的选择敏感，原始分类器不加修改仅适合于处理二分类问题。 使用数据类型：数值型和标称型数据。 SVM无核函数分类这里是对小规模数据点进行分类 收集数据文本文件格式： 3.542485 1.977398 -1 3.018896 2.556416 -1 7.551510 -1.580030 1 2.114999 -0.004466 -1 8.127113 1.274372 1 准备数据 '''对文件进行逐行解析，从而得到第行的类标签和整个数据矩阵''' def loadDataSet(fileName): dataMat = [];labelMat = [] fr = open(fileName) for line in fr.readlines(): lineArr = line.strip().split('\t') dataMat.append([float(lineArr[0]), float(lineArr[1])]) labelMat.append(float(lineArr[2])) return dataMat, labelMat ## 训练算法 简化版SMO（序列最小优化）算法 ''' 简化版SMO（序列最小优化）算法 输入参数： C 松弛变量(常量值)，允许有些数据点可以处于分隔面的错误一侧。控制最大化间隔和 保证大部分的函数间隔小于1.0这两个目标权重。通过调节该参数达到不同的结果。 toler 容错率（在某个体系中能减小一些因素或选择对某个系统产生不稳定的概率。） maxIter 退出前最大的循环次数 返回参数： b 模型的常量值 alphas 拉格朗日乘子 ''' def smoSimple(dataMatIn, classLabels, C, toler, maxIter): dataMatrix = mat(dataMatIn) labelMat = mat(classLabels).transpose() m, n = shape(dataMatrix) # 初始化 b和alphas(alpha有点类似权重值。) b = 0;alphas = mat(zeros((m, 1))) # 没有任何alpha改变的情况下遍历数据的次数 iter = 0 while (iter < maxIter): # w = calcWs(alphas, dataMatIn, classLabels) # print("w:", w) # 记录alpha是否已经进行优化，每次循环时设为0，然后再对整个集合顺序遍历 alphaPairsChanged = 0 for i in range(m): # 我们预测的类别 y = w^Tx[i]+b; 其中因为 w = Σ(1~n) a[n]*lable[n]*x[n] fXi = float(multiply(alphas, labelMat).T*(dataMatrix*dataMatrix[i, :].T)) + b # 预测结果与真实结果比对，计算误差Ei Ei = fXi - float(labelMat[i]) # 约束条件 (KKT条件是解决最优化问题的时用到的一种方法。我们这里提到的最优化问题通常是指对于给定的某一函数，求其在指定作用域上的全局最小值) # 0 0)): # 如果满足优化的条件，我们就随机选取非i的一个点，进行优化比较 j = selectJrand(i, m) # 预测j的结果 fXj = float(multiply(alphas, labelMat).T*(dataMatrix*dataMatrix[j, :].T)) + b Ej = fXj - float(labelMat[j]) alphaIold = alphas[i].copy() alphaJold = alphas[j].copy() # L和H用于将alphas[j]调整到0-C之间。如果L==H，就不做任何改变，直接执行continue语句 # labelMat[i] != labelMat[j] 表示异侧，就相减，否则是同侧，就相加。 if (labelMat[i] != labelMat[j]): L = max(0, alphas[j] - alphas[i]) H = min(C, C + alphas[j] - alphas[i]) else: L = max(0, alphas[j] + alphas[i] - C) H = min(C, alphas[j] + alphas[i]) # 如果相同，就没发优化了 if L == H: # print("L==H") continue # eta是alphas[j]的最优修改量，如果eta==0，需要退出for循环的当前迭代过程 # 参考《统计学习方法》李航-P125~P128 eta = 2.0 * dataMatrix[i, :]*dataMatrix[j, :].T - dataMatrix[i, :]*dataMatrix[i, :].T - dataMatrix[j, :]*dataMatrix[j, :].T if eta >= 0: # print("eta>=0") continue # 计算出一个新的alphas[j]值 alphas[j] -= labelMat[j]*(Ei - Ej)/eta # 并使用辅助函数，以及L和H对其进行调整 alphas[j] = clipAlpha(alphas[j], H, L) # 检查alpha[j]是否只是轻微的改变，如果是的话，就退出for循环。 if (abs(alphas[j] - alphaJold) < 0.00001): # print("j not moving enough") continue # 然后alphas[i]和alphas[j]同样进行改变，虽然改变的大小一样，但是改变的方向正好相反 alphas[i] += labelMat[j]*labelMat[i]*(alphaJold - alphas[j]) # 在对alpha[i], alpha[j] 进行优化之后，给这两个alpha值设置一个常数b。 # w= Σ[1~n] ai*yi*xi => b = yj- Σ[1~n] ai*yi(xi*xj) # 所以： b1 - b = (y1-y) - Σ[1~n] yi*(a1-a)*(xi*x1) # 为什么减2遍？ 因为是 减去Σ[1~n]，正好2个变量i和j，所以减2遍 b1 = b - Ei- labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i, :]*dataMatrix[i, :].T - labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[i, :]*dataMatrix[j, :].T b2 = b - Ej- labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i, :]*dataMatrix[j, :].T - labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[j, :]*dataMatrix[j, :].T if (0 < alphas[i]) and (C > alphas[i]): b = b1 elif (0 < alphas[j]) and (C > alphas[j]): b = b2 else: b = (b1 + b2)/2.0 alphaPairsChanged += 1 # print("iter: %d i:%d, pairs changed %d" % (iter, i, alphaPairsChanged)) # 在for循环外，检查alpha值是否做了更新，如果在更新则将iter设为0后继续运行程序 # 知道更新完毕后，iter次循环无变化，才推出循环。 if (alphaPairsChanged == 0): iter += 1 else: iter = 0 # print("iteration number: %d" % iter) return b, alphas]]></content>
      <categories>
        <category>机器学习</category>
        <category>SVM</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>文本分类</tag>
        <tag>支持向量机</tag>
        <tag>SVM</tag>
        <tag>机器学习算法</tag>
        <tag>sklean</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一步步教你轻松学支持向量机SVM算法之理论篇1]]></title>
    <url>%2F2018%2F10%2F16%2F%E4%B8%80%E6%AD%A5%E6%AD%A5%E6%95%99%E4%BD%A0%E8%BD%BB%E6%9D%BE%E5%AD%A6%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BASVM%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[摘要：支持向量机即SVM(Support Vector Machine) ，是一种监督学习算法，属于分类的范畴。首先，支持向量机不是一种机器，而是一种机器学习算法。在数据挖掘的应用中，与无监督学习的聚类相对应和区别。广泛应用于机器学习，计算机视觉和数据挖掘当中。（本文原创，转载必须注明出处.） 相关概念 支持向量就是数据集中的某些点，位置比较特殊。比如 x+y-2=0 这条直线，直线上面区域 x+y-2&gt;0 的全是 A 类，下面的 x+y-2&lt;0 的全是 B 类，我们找这条直线的时候，一般就看聚集在一起的两类数据，他们各自的最边缘 位置的点，也就是最靠近划分直线的那几个点，而其他点对这条直线的最终位置的确定起不了作用，所以我姑且叫这些点叫 “支持点”（意思就是有用的点）. 在最大间隔上的这些点就叫 “支持向量”，因为最后的 classification machine （分类器）的表达式里只含有这些 “支持向量” 的信息，而与其他数据点无关,借助图形理解（支持向量就是图中用紫色框框圈出来的点）如下: 线性可分(linearly separable): 如上图中的两组数据，它们之间已经分的足够开了，因此很容易就可以在图中画出一条直线将两组数据点分开。在这种情况下，这组数据就被称为线性可分数据。 分隔超平面(separating hyperplane): 上述将数据集分隔开来的直线称为分隔超平面。 超平面(hyperplane): 在上面给出的例子中，由于数据点都在二维平面上，所以此时分隔超平面就只是一条直线。但是，如果所给的数据集是三维的，那么此时用来分隔数据的就是一个平面。显而易见，更高纬度的情况可以依此类推。如果数据是 1024 维的，那么就需要一个 1023 维的某某对象来对数据进行分隔。这个 1023 维的某某对象到底应该叫什么呢？ N-1 维呢？该对象被称为超平面，也就是分类的决策边界。分布在超平面一侧的所有数据都属于某个类别，而分布在另一侧的所有数据则属于另一个类别。 间隔(margin): 我们希望能通过上述的方式来构建分类器，即如果数据点离决策边界越远，那么其最后的预测结果也就越可信。既然这样，我们希望找到离分隔超平面最近的点，确保它们离分隔面的距离尽可能远。这里所说的点到分隔面的距离就是间隔。我们希望间隔尽可能地大，这是因为如果我们犯错或者在有限数据上训练分类器的话，我们希望分类器尽可能健壮。 支持向量（support vector） : 就是上面所说的离分隔超平面最近的那些点。 分类器 : 分类器就是给定一个样本的数据，判定这个样本属于哪个类别的算法。例如在股票涨跌预测中，我们认为前一天的交易量和收盘价对于第二天的涨跌是有影响的，那么分类器就是通过样本的交易量和收盘价预测第二天的涨跌情况的算法。 特征 : 在分类问题中，输入到分类器中的数据叫做特征。以上面的股票涨跌预测问题为例，特征就是前一天的交易量和收盘价。 线性分类器 : 线性分类器是分类器中的一种，就是判定分类结果的根据是通过特征的线性组合得到的，不能通过特征的非线性运算结果作为判定根据。还以上面的股票涨跌预测问题为例，判断的依据只能是前一天的交易量和收盘价的线性组合，不能将交易量和收盘价进行开方，平方等运算。 SVM 原理直观理解我们在桌子上放置一些不同颜色的球，让小明用一根棍分开它们？要求：尽量在放更多球之后，仍然适用。 于是小明这样放，看起来还不错？ 然后我们又在桌上放了更多的球，似乎有一个球站错了阵营。 SVM 就是试图把棍放在最佳位置，好让在棍的两边有尽可能大的间隙。 现在即使我们放了更多的球，棍仍然是一个好的分界线。 然后，在 SVM 工具箱中有另一个更加重要的 trick。 我们看到小明已经学会了一个 trick ，于是我们给了小明一个新的挑战。 现在，小明没有棍可以很好帮他分开两种球了，现在怎么办呢？当然像所有武侠片中一样大侠桌子一拍，球飞到空中。然后，凭借大侠的轻功，大侠抓起一张纸，插到了两种球的中间。 现在，从我们的角度看这些球，这些球看起来像是被一条曲线分开了。 我们把这些球叫做 「data」，把棍子叫做「classifier」, 最大间隙 trick 叫做「optimization」， 拍桌子叫做「kernelling」, 那张纸叫做「hyperplane」 。到这里，大家对支持向量机有个更加直观的感受了吧，以上引用网络案例。 支持向量机分类先考虑最简单的情况，豌豆和米粒，用晒子很快可以分开，小颗粒漏下去，大颗粒保留。用一个函数来表示就是当直径 d 大于某个值 D ，就判定为豌豆，小于某个值就是米粒。 d&gt;D, 豌豆 d&lt;D,米粒 在数轴上就是在d左边就是米粒，右边就是绿豆，这是一维的情况。但是实际问题没这么简单，考虑的问题不单单是尺寸，一个花的两个品种，怎么分类？假设决定他们分类的有两个属性，花瓣尺寸和颜色。单独用一个属性来分类，像刚才分米粒那样，就不行了。这个时候我们设置两个值 尺寸 x 和颜色 y.我们把所有的数据都丢到 x-y 平面上作为点，按道理如果只有这两个属性决定了两个品种，数据肯定会按两类聚集在这个二维平面上。我们只要找到一条直线，把这两类划分开来，分类就很容易了，以后遇到一个数据，就丢进这个平面，看在直线的哪一边，就是哪一类。比如 x+y-2=0 这条直线，我们把数据 (x,y) 代入，只要认为 x+y-2&gt;0 的就是 A 类， x+y-2&lt;0 的就是 B 类。以此类推，还有三维的，四维的，N维的 属性的分类，这样构造的也许就不是直线，而是平面，超平面。一个三维的函数分类 ：x+y+z-2=0，这就是个分类的平面了。有时候，分类的那条线不一定是直线，还有可能是曲线，我们通过某些函数来转换，就可以转化成刚才的哪种多维的分类问题，这个就是核函数的思想。例如：分类的函数是个圆形 x^2+y^2-4=0 。这个时候令 x^2=a ; y^2=b ,还不就变成了a+b-4=0 这种直线问题了。这就是支持向量机的思想。 SVM 工作原理 原理 对于上述的苹果和香蕉，我们想象为2种水果类型的炸弹。（保证距离最近的炸弹，距离它们最远） 寻找最大分类间距 转而通过拉格朗日函数求优化的问题 数据可以通过画一条直线就可以将它们完全分开，这组数据叫线性可分(linearly separable)数据，而这条分隔直线称为分隔超平面(separating hyperplane)。 如果数据集上升到1024维呢？那么需要1023维来分隔数据集，也就说需要N-1维的对象来分隔，这个对象叫做超平面(hyperlane)，也就是分类的决策边界。 SVM学习方法的选择 当训练数据线性可分时，通过硬间隔最大化学习一个线性分类器,即线性可分支持向量机又称为硬间隔支持向量机。 当训练数据近似线性可分时，通过软间隔最大化学习一个线性分类器,即线性可分支持向量机又称为软间隔支持向量机。 当训练数据线性不可分时，通过核函数及软间隔最大化学习一个非线性支持向量机。 更多内容参考如下资料： https://www.zhihu.com/question/21094489 http://docs.opencv.org/2.4/doc/tutorials/ml/introduction_to_svm/introduction_to_svm.html https://zhuanlan.zhihu.com/p/26891427?utm_medium=social&amp;utm_source=qq https://zhuanlan.zhihu.com/p/21308801?utm_medium=social&amp;utm_source=qq http://blog.csdn.net/v_july_v/article/details/7624837 理解和应用数据挖掘(DataMining)做数据挖掘应用的一种重要算法，也是效果最好的分类算法之一。举个例子，就是尽量把样本中的从更高纬度看起来在一起的样本合在一起，比如在一维（直线）空间里的样本从二维平面上可以分成不同类别，而在二维平面上分散的样本如果从第三维空间上来看就可以对他们做分类。支持向量机算法目的是找出最优超平面，使分类间隔最大，要求不但正确分开，而且使分类间隔最大，在两类样本中离分类平面最近且位于平行于最优超平面的超平面上的点就是支持向量，为找到最优超平面只要找到所有支持向量即可。对于非线性支持向量机，通常做法是把线性不可分转化成线性可分，通过一个非线性映射将输入到低维空间中的数据特性映射到高维线性特征空间中，在高维空间中求线性最优分类超平面。 scikit(scikit-learn)SVM 的基本原理基本上已经说的差不多了，下面咱们就来看看 SVM 在实际应用该如何使用了。幸运的是，sklearn 提供了一个非常好用的机器学习算法，我们调用相关的包就好啦。 参考文献 scikit中文社区：http://sklearn.apachecn.org/cn/0.19.0/ ApacheCN GitHub：https://github.com/BaiNingchao/MachineLearning-1 图书：《机器学习实战》 图书：《自然语言处理理论与实战》 完整代码下载 源码请进【机器学习和自然语言QQ群：436303759】文件下载： 作者声明 本文版权归作者所有，旨在技术交流使用。未经作者同意禁止转载，转载后需在文章页面明显位置给出原文连接，否则相关责任自行承担。]]></content>
      <categories>
        <category>机器学习</category>
        <category>SVM</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>文本分类</tag>
        <tag>支持向量机</tag>
        <tag>SVM</tag>
        <tag>机器学习算法</tag>
        <tag>sklean</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一步步教你轻松学奇异值分解SVD降维算法]]></title>
    <url>%2F2018%2F10%2F11%2F%E4%B8%80%E6%AD%A5%E6%AD%A5%E6%95%99%E4%BD%A0%E8%BD%BB%E6%9D%BE%E5%AD%A6%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3SVD%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[摘要：奇异值分解（singular value decomposition）是线性代数中一种重要的矩阵分解，在生物信息学、信号处理、金融学、统计学等领域有重要应用，SVD都是提取信息的强度工具。在机器学习领域，很多应用与奇异值都有关系，比如推荐系统、数据压缩（以图像压缩为代表）、搜索引擎语义层次检索的LSI等等。（本文原创，转载必须注明出处.） 奇异值分解原理什么是奇异值分解(SVD） 奇异值分解 假设M是一个m×n阶矩阵，其中的元素全部属于域K，也就是实数域或复数域。如此则存在一个分解使得 M_{m×n}=U_{m×m} \Sigma_{m×n} V^T_{n×n}其中U是m×m阶酉矩阵；Σ是m×n阶非负实数对角矩阵；而\(V^T\)，即V的共轭转置，是n×n阶酉矩阵。这样的分解就称作M的奇异值分解。Σ对角线上的元素\(Σ_i\),i即为M的奇异值。常见的做法是将奇异值由大而小排列。如此Σ便能由M唯一确定了。（虽然U和V仍然不能确定。） V的列组成一套对\(M\)的正交”输入”或”分析”的基向量。这些向量是\(M^*M\)的特征向量。 U的列组成一套对\(M\)的正交”输出”的基向量。这些向量是\(MM^*\)的特征向量。 Σ对角线上的元素是奇异值，可视为是在输入与输出间进行的标量的”膨胀控制”。这些是\( MM^* \)及 \( M^* M \)的特征值的非负平方根，并与U和V的行向量相对应。 矩阵知识 正交与正定矩阵 正交矩阵：若一个方阵其行与列皆为正交的单位向量，则该矩阵为正交矩阵，且该矩阵的转置和其逆相等。两个向量正交的意思是两个向量的内积为 0。 &gt;&gt; 正交矩阵知识扩展 正定矩阵：如果对于所有的非零实系数向量 \(z\)，都有 \(z^T A z &gt; 0\)，则称矩阵 \(A\) 是正定的。正定矩阵的行列式必然大于 0， 所有特征值也必然 &gt; 0。相对应的，半正定矩阵的行列式必然 ≥ 0。&gt;&gt; 正定矩阵知识扩展 转置与共轭转置 矩阵的转置（transpose）是最简单的一种矩阵变换。简单来说，若 $m\times n$ 的矩阵 $\mathbf M$ 的转置记为 $\mathbf M^{\mathsf T}$；则 $\mathbf M^{\mathsf T}$ 是一个 $n\times m$ 的矩阵，并且 \mathbf M_{i,j} = \mathbf M^{\mathsf T}_{j,i}。因此，矩阵的转置相当于将矩阵按照主对角线翻转；同时，我们不难得出 \mathbf M = \bigl(\mathbf M^{\mathsf T}\bigr)^{\mathsf T}。 矩阵的共轭转置（conjugate transpose）可能是倒数第二简单的矩阵变换。共轭转置只需要在转置的基础上，再叠加复数的共轭即可。因此，若以 $\mathbf M^{\mathsf H}$ 记矩阵 $\mathbf M$ 的共轭转置，则有 \mathbf M_{i,j} = \overline{\bigl(\mathbf M^{\mathsf H}\bigr)_{j,i}}。 酉矩阵 酉矩阵（unitary matrix）是一种特殊的方阵，它满足\mathbf U\mathbf U^{\mathsf H} = \mathbf U^{\mathsf H}\mathbf U = I_n. 酉矩阵实际上是推广的正交矩阵（orthogonal matrix）；当酉矩阵中的元素均为实数时，酉矩阵实际就是正交矩阵。另一方面，由于 \mathbf M\mathbf M^{-1} = \mathbf M^{-1}\mathbf M = I_n，所以酉矩阵 $\mathbf U$ 满足 $\mathbf U^{-1} = \mathbf U^{\mathsf H}$；事实上，这是一个矩阵是酉矩阵的充分必要条件。 正规矩阵 同酉矩阵一样，正规矩阵（normal matrix）也是一种特殊的方阵，它要求在矩阵乘法的意义下与它的共轭转置矩阵满足交换律。这也就是说，若矩阵 $\mathbf M$ 满足如下条件，则称其为正规矩阵：\mathbf M\mathbf M^{\mathsf H} = \mathbf M^{\mathsf H}\mathbf M.。显而易见，复系数的酉矩阵和实系数的正交矩阵都是正规矩阵。显而易见，正规矩阵并不只有酉矩阵或正交矩阵。例如说，矩阵 \mathbf M = \begin{pmatrix}1 & 1 & 0 \\ 0 & 1 & 1 \\ 1 & 0 & 1\end{pmatrix} 即是一个正规矩阵，但它显然不是酉矩阵或正交矩阵；因为\mathbf M\mathbf M^{\mathsf H} = \begin{pmatrix}2 & 1 & 1 \\ 1 & 2 & 1 \\ 1 & 1 & 2\end{pmatrix} = \mathbf M^{\mathsf H}\mathbf M. 谱定理和谱分解 矩阵的对角化是线性代数中的一个重要命题。谱定理（spectral theorem）给出了方阵对角化的一个结论：若矩阵 $\mathbf M$ 是一个正规矩阵，则存在酉矩阵 $\mathbf U$，以及对角矩阵 $\mathbf \Lambda$，使得\mathbf M = \mathbf U\mathbf \Lambda\mathbf U^{\mathsf H}.这也就是说，正规矩阵，可经由酉变换，分解为对角矩阵；这种矩阵分解的方式，称为谱分解（spectral decomposition）。 SVD 的计算方法SVD 与特征值现在，假设矩阵 $\mathbf M_{m\times n}$ 的 SVD 分解是\mathbf M = \mathbf U\mathbf\Sigma\mathbf V^{\mathsf H};那么，我们有 \begin{aligned} \mathbf M\mathbf M^{\mathsf H} &{}= \mathbf U\mathbf\Sigma\mathbf V^{\mathsf H}\mathbf V\mathbf\Sigma^{\mathsf H}\mathbf U^{\mathsf H} = \mathbf U(\mathbf\Sigma\mathbf\Sigma^{\mathsf H})\mathbf U^{\mathsf H}\\ \mathbf M^{\mathsf H}\mathbf M &{}= \mathbf V\mathbf\Sigma^{\mathsf H}\mathbf U^{\mathsf H}\mathbf U\mathbf\Sigma\mathbf V^{\mathsf H} = \mathbf V(\mathbf\Sigma^{\mathsf H}\mathbf\Sigma)\mathbf V^{\mathsf H}\\ \end{aligned}这也就是说，$\mathbf U$ 的列向量（左奇异向量），是 $\mathbf M\mathbf M^{\mathsf H}$ 的特征向量；同时，$\mathbf V$ 的列向量（右奇异向量），是 $\mathbf M^{\mathsf H}\mathbf M$ 的特征向量；另一方面，$\mathbf M$ 的奇异值（$\mathbf\Sigma$ 的非零对角元素）则是 $\mathbf M\mathbf M^{\mathsf H}$ 或者 $\mathbf M^{\mathsf H}\mathbf M$ 的非零特征值的平方根。 如何计算 SVD有了这些知识，我们就能手工计算出任意矩阵的 SVD 分解了；具体来说，算法如下 计算 $\mathbf M\mathbf M^{\mathsf H}$ 和 $\mathbf M^{\mathsf H}\mathbf M$； 分别计算 $\mathbf M\mathbf M^{\mathsf H}$ 和 $\mathbf M^{\mathsf H}\mathbf M$ 的特征向量及其特征值； $\mathbf M\mathbf M^{\mathsf H}$ 的特征向量组成 $\mathbf U$；而 $\mathbf M^{\mathsf H}\mathbf M$ 的特征向量组成 $\mathbf V$； 对 $\mathbf M\mathbf M^{\mathsf H}$ 和 $\mathbf M^{\mathsf H}\mathbf M$ 的非零特征值求平方根，对应上述特征向量的位置，填入 $\mathbf\Sigma$ 的对角元。 实际计算看看现在，我们来试着计算 \mathbf M = \begin{bmatrix}2 & 4 \\ 1 & 3 \\ 0 & 0 \\ 0 & 0\end{bmatrix} 的奇异值分解。计算奇异值分解，需要计算 $\mathbf M$ 与其共轭转置的左右积；这里主要以 $\mathbf M\mathbf M^{\mathsf H}$ 为例。首先，我们需要计算 $\mathbf M\mathbf M^{\mathsf H}$， \mathbf W = \mathbf M\mathbf M^{\mathsf H} = \begin{bmatrix}2 & 4 \\ 1 & 3 \\ 0 & 0 \\ 0 & 0\end{bmatrix}\begin{bmatrix}2 & 1 & 0 & 0 \\ 4 & 3 & 0 & 0\end{bmatrix} = \begin{bmatrix}20 & 14 & 0 & 0 \\ 14 & 10 & 0 & 0 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0\end{bmatrix}.现在，我们要求 $\mathbf W$ 的特征值与特征向量。根据定义 $\mathbf W\vec x = \lambda \vec x$；因此 $(\mathbf W - \lambda\mathbf I)\vec x = \vec 0$。这也就是说 \begin{bmatrix} 20 - \lambda & 14 & 0 & 0 \\ 14 & 10 - \lambda & 0 & 0 \\ 0 & 0 & -\lambda & 0 \\ 0 & 0 & 0 & -\lambda \end{bmatrix}\vec x = \vec 0.根据线性方程组的理论，若要该关于 $\vec x$ 的方程有非零解，则要求系数矩阵的行列式为 0；也就是 \begin{vmatrix} 20 - \lambda & 14 & 0 & 0 \\ 14 & 10 - \lambda & 0 & 0 \\ 0 & 0 & -\lambda & 0 \\ 0 & 0 & 0 & -\lambda \end{vmatrix} = \begin{vmatrix} 20 - \lambda & 14 \\ 14 & 10 - \lambda \\ \end{vmatrix}\begin{vmatrix} -\lambda & 0 \\ 0 & -\lambda \\ \end{vmatrix} = 0,这也就是 $\bigl((20 - \lambda)(10 - \lambda) - 196\bigr)\lambda^2 = 0$；解得 \lambda_{1} = \lambda_{2} = 0, \lambda_{3} = 15 + \sqrt{221} \approx 29.866, \lambda_{4} = 15 - \sqrt{221} \approx 0.134。将特征值代入原方程，可解得对应的特征向量；这些特征向量即作为列向量，形成矩阵 \mathbf U = \begin{bmatrix}-0.82 & -0.58 & 0 & 0 \\ -0.58 & 0.82 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1\end{bmatrix}.同理可解得（注意，$\mathbf M\mathbf M^{\mathsf T}$ 和 $\mathbf M^{\mathsf T}\mathbf M$ 的特征值相同） \mathbf V = \begin{bmatrix}-0.40 & -0.91 \\ -0.91 & 0.40\end{bmatrix}.以及 $\mathbf\Sigma$ 上的对角线元素由 $\mathbf W$ 的特征值的算术平方根组成；因此有 \mathbf\Sigma = \begin{bmatrix}5.46 & 0 \\ 0 & 0.37 \\ 0 & 0 \\ 0 & 0\end{bmatrix}.因此我们得到矩阵 $\mathbf M$ 的 SVD 分解（数值上做了近似）： \begin{bmatrix}2 & 4 \\ 1 & 3 \\ 0 & 0 \\ 0 & 0\end{bmatrix} \approx \begin{bmatrix}-0.82 & -0.58 & 0 & 0 \\ -0.58 & 0.82 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1\end{bmatrix}\begin{bmatrix}5.46 & 0 \\ 0 & 0.37 \\ 0 & 0 \\ 0 & 0\end{bmatrix}\begin{bmatrix}-0.40 & -0.91 \\ -0.91 & 0.40\end{bmatrix}几何上的直观解释我们先来看一个例子。假设 $\mathbf M$ 是一个 $m\times n$ 的矩阵，而 $\mathbf x$ 是线性空间 $\mathbb K^n$ 中的向量，则$\mathbf y = \mathbf M\cdot\mathbf x$ 是线性空间 $\mathbb K^m$ 中的向量。这样一来，矩阵 $\mathbb A$ 就对应了一个从 $\mathbb K^n$ 到 $\mathbb K^m$ 的变换 $T: \mathbb K^n \to \mathbb K^m$，具体来说既是 $\mathbf x\mapsto \mathbf M\cdot\mathbf x$。这也就是说，在线性代数中，任意矩阵都能看做是一种变换。这样一来，我们就统一了矩阵和变换。 SVD 场景 隐性语义检索 信息检索-隐性语义检索（Lstent Semantic Indexing, LSI）或 隐形语义分析（Latent Semantic Analysis, LSA）隐性语义索引：矩阵 = 文档 + 词语最早的 SVD 应用之一，我们称利用 SVD 的方法为隐性语义索引（LSI）或隐性语义分析（LSA）。 推荐系统 利用 SVD 从数据中构建一个主题空间。 再在该空间下计算其相似度。(从高维-低维空间的转化，在低维空间来计算相似度，SVD 提升了推荐系统的效率。) 图像压缩 例如：32*32=1024 =&gt; 32*2+2*1+32*2=130(2*1表示去掉了除对角线的0), 几乎获得了10倍的压缩比。 SVD 工作原理矩阵分解 矩阵分解是将数据矩阵分解为多个独立部分的过程。 矩阵分解可以将原始矩阵表示成新的易于处理的形式，这种新形式是两个或多个矩阵的乘积。（类似代数中的因数分解） 举例：如何将12分解成两个数的乘积？（1，12）、（2，6）、（3，4）都是合理的答案。 SVD 是矩阵分解的一种类型，也是矩阵分解最常见的技术 SVD 将原始的数据集矩阵 Data 分解成三个矩阵 U、∑、V 举例：如果原始矩阵 \(Data_{m*n} \) 是m行n列， \(U_{m * k}\) 表示m行k列 \(∑_{k * k}\) 表示k行k列 \(V_{k * n}\) 表示k行n列。 Data_{m×n} = U_{m×k} * ∑_{k×k} * V_{k×n} 具体的案例： \begin{vmatrix} 0 & -1.6 & 0.6 \\ 0 & 1.2 & 0.8 \\ 0 & 0 & 0 \\ 0 & 0 & 0 \\ \end{vmatrix} = \begin{vmatrix} 0.8 & 0.6 & 0 & 0 \\ -0.6 & 0.8 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1 \\ \end{vmatrix} * \begin{vmatrix} 2 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 0 \\ \end{vmatrix} * \begin{vmatrix} 0 & 0 & 1 \\ -1 & 0 & 0 \\ 0 & 1 & 0 \\ \end{vmatrix} 上述分解中会构建出一个矩阵∑，该矩阵只有对角元素，其他元素均为0(近似于0)。另一个惯例就是，∑的对角元素是从大到小排列的。这些对角元素称为奇异值。 奇异值与特征值(PCA 数据中重要特征)是有关系的。这里的奇异值就是矩阵 \(Data * Data^T\) 特征值的平方根。 普遍的事实：在某个奇异值的数目(r 个=&gt;奇异值的平方和累加到总值的90%以上)之后，其他的奇异值都置为0(近似于0)。这意味着数据集中仅有 r 个重要特征，而其余特征则都是噪声或冗余特征。 SVD 算法特点优点：简化数据，去除噪声，优化算法的结果缺点：数据的转换可能难以理解使用的数据类型：数值型数据 推荐系统推荐系统是利用电子商务网站向客户提供商品信息和建议，帮助用户决定应该购买什么产品，模拟销售人员帮助客户完成购买过程。 推荐系统场景 Amazon 会根据顾客的购买历史向他们推荐物品 Netflix 会向其用户推荐电影 新闻网站会对用户推荐新闻频道 推荐系统要点 基于协同过滤(collaborative filtering) 的推荐引擎 利用Python 实现 SVD(Numpy 有一个称为 linalg 的线性代数工具箱) 协同过滤：是通过将用户和其他用户的数据进行对比来实现推荐的。 当知道了两个用户或两个物品之间的相似度，我们就可以利用已有的数据来预测未知用户的喜好。 基于物品的相似度和基于用户的相似度：物品比较少则选择物品相似度，用户比较少则选择用户相似度。【矩阵还是小一点好计算】 基于物品的相似度：计算物品之间的距离。【耗时会随物品数量的增加而增加】 由于物品A和物品C 相似度(相关度)很高，所以给买A的人推荐C。 用户/物品|物品A|物品B|物品C 基于用户的相似度：计算用户之间的距离。【耗时会随用户数量的增加而增加】 由于用户A和用户C 相似度(相关度)很高，所以A和C是兴趣相投的人，对于C买的物品就会推荐给A。 相似度计算 inA, inB 对应的是 列向量 欧氏距离：指在m维空间中两个点之间的真实距离，或者向量的自然长度（即该点到原点的距离）。二维或三维中的欧氏距离就是两点之间的实际距离。 相似度= 1/(1+欧式距离) 相似度= 1.0/(1.0 + la.norm(inA - inB)) 物品对越相似，它们的相似度值就越大。 皮尔逊相关系数：度量的是两个向量之间的相似度。 相似度= 0.5 + 0.5corrcoef() 【皮尔逊相关系数的取值范围从 -1 到 +1，通过函数0.5 + 0.5\corrcoef()这个函数计算，把值归一化到0到1之间】 相似度= 0.5 + 0.5 * corrcoef(inA, inB, rowvar = 0)[0][1] 相对欧氏距离的优势：它对用户评级的量级并不敏感。 余弦相似度：计算的是两个向量夹角的余弦值。 余弦值 = (A·B)/(||A||·||B||) 【余弦值的取值范围也在-1到+1之间】 相似度= 0.5 + 0.5*余弦值 相似度= 0.5 + 0.5*( float(inA.T*inB) / la.norm(inA)*la.norm(inB)) 如果夹角为90度，则相似度为0；如果两个向量的方向相同，则相似度为1.0。 代码实现 '''基于欧氏距离相似度计算，假定inA和inB 都是列向量 相似度=1/(1+距离),相似度介于0-1之间 norm：范式计算，默认是2范数，即:sqrt(a^2+b^2+...) ''' def ecludSim(inA, inB): return 1.0/(1.0 + la.norm(inA - inB)) '''皮尔逊相关系数 范围[-1, 1]，归一化后[0, 1]即0.5 + 0.5 * 相对于欧式距离，对具体量级（五星三星都一样）不敏感皮尔逊相关系数 ''' def pearsSim(inA, inB): # 检查是否存在3个或更多的点不存在，该函数返回1.0，此时两个向量完全相关。 if len(inA) < 3: return 1.0 return 0.5 + 0.5 * corrcoef(inA, inB, rowvar=0)[0][1] '''计算余弦相似度 如果夹角为90度相似度为0；两个向量的方向相同，相似度为1.0 余弦取值-1到1之间，归一化到0与1之间即：相似度=0.5 + 0.5*cosθ 余弦相似度cosθ=(A*B/|A|*|B|) ''' def cosSim(inA, inB): num = float(inA.T*inB) # 矩阵相乘 denom = la.norm(inA)*la.norm(inB) # 默认是2范数 return 0.5 + 0.5*(num/denom) 推荐系统的评价 采用交叉测试的方法。【拆分数据为训练集和测试集】 推荐引擎评价的指标： 最小均方根误差(Root mean squared error, RMSE)，也称标准误差(Standard error)，就是计算均方误差的平均值然后取其平方根。 如果RMSE=1, 表示相差1个星级；如果RMSE=2.5, 表示相差2.5个星级。 推荐系统原理 推荐系统的工作过程：给定一个用户，系统会为此用户返回N个最好的推荐菜。 实现流程大致如下： 寻找用户没有评级的菜肴，即在用户-物品矩阵中的0值。 在用户没有评级的所有物品中，对每个物品预计一个可能的评级分数。这就是说：我们认为用户可能会对物品的打分（这就是相似度计算的初衷）。 对这些物品的评分从高到低进行排序，返回前N个物品。 项目实战: 餐馆菜肴推荐系统假如一个人在家决定外出吃饭，但是他并不知道该到哪儿去吃饭，该点什么菜。推荐系统可以帮他做到这两点。 收集并准备数据 数据准备的代码实现 # 利用SVD提高推荐效果，菜肴矩阵 """ 行：代表人 列：代表菜肴名词 值：代表人对菜肴的评分，0表示未评分 """ def loadExData3(): return[[2, 0, 0, 4, 4, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5], [0, 0, 0, 0, 0, 0, 0, 1, 0, 4, 0], [3, 3, 4, 0, 3, 0, 0, 2, 2, 0, 0], [5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 5, 0, 0, 5, 0], [4, 0, 4, 0, 0, 0, 0, 0, 0, 0, 5], [0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 4], [0, 0, 0, 0, 0, 0, 5, 0, 0, 5, 0], [0, 0, 0, 3, 0, 0, 0, 0, 4, 5, 0], [1, 1, 2, 1, 1, 2, 1, 0, 4, 5, 0]] ``` ## 分析数据 这里不做过多的讨论(当然此处可以对比不同距离之间的差别)，通常保留矩阵 80% ～ 90% 的能量，就可以得到重要的特征并去除噪声。 ```python '''分析 Sigma 的长度取值 根据自己的业务情况，就行处理，设置对应的 Singma 次数 通常保留矩阵 80% ～ 90% 的能量，就可以得到重要的特征并取出噪声。 ''' def analyse_data(Sigma, loopNum=20): # 总方差的集合（总能量值） Sig2 = Sigma**2 SigmaSum = sum(Sig2) for i in range(loopNum): SigmaI = sum(Sig2[:i+1]) print('主成分：%s, 方差占比：%s%%' % (format(i+1, '2.0f'), format(SigmaI/SigmaSum*100, '.2f'))) 训练算法: 通过调用 recommend() 函数进行推荐recommend() 会调用 基于物品相似度 或者是 基于SVD，得到推荐的物品评分。 基于物品相似度 基于物品相似度的推荐引擎代码实现 '''基于物品相似度的推荐引擎 descripte：计算某用户未评分物品中，以对该物品和其他物品评分的用户的物品相似度，然后进行综合评分 dataMat 训练数据集 user 用户编号 simMeas 相似度计算方法 item 未评分的物品编号 Returns: ratSimTotal/simTotal 评分（0～5之间的值） ''' def standEst(dataMat, user, simMeas, item): # 得到数据集中的物品数目 n = shape(dataMat)[1] # 初始化两个评分值 simTotal = 0.0 ; ratSimTotal = 0.0 # 遍历行中的每个物品（对用户评过分的物品遍历，并与其他物品进行比较） for j in range(n): userRating = dataMat[user, j] # 如果某个物品的评分值为0，则跳过这个物品 if userRating == 0: # 终止循环 continue # 寻找两个都评级的物品,变量overLap 给出两个物品中已被评分的元素索引ID # logical_and 计算x1和x2元素的真值。 # print(dataMat[:, item].T.A, ':',dataMat[:, j].T.A ) overLap = nonzero(logical_and(dataMat[:, item].A > 0, dataMat[:, j].A > 0))[0] # 如果相似度为0，则两着没有任何重合元素，终止本次循环 if len(overLap) == 0: similarity = 0 # 如果存在重合的物品，则基于这些重合物重新计算相似度。 else: similarity = simMeas(dataMat[overLap, item], dataMat[overLap, j]) # 相似度会不断累加，每次计算时还考虑相似度和当前用户评分的乘积 # similarity 用户相似度， userRating 用户评分 simTotal += similarity ratSimTotal += similarity * userRating if simTotal == 0: return 0 # 通过除以所有的评分总和，对上述相似度评分的乘积进行归一化，使得最后评分在0~5之间，这些评分用来对预测值进行排序 else: return ratSimTotal/simTotal 基于SVD 基于SVD的代码实现 '''分析 Sigma 的长度取值 根据自己的业务情况，就行处理，设置对应的 Singma 次数 通常保留矩阵 80% ～ 90% 的能量，就可以得到重要的特征并取出噪声。 ''' def analyse_data(Sigma, loopNum=20): # 总方差的集合（总能量值） Sig2 = Sigma**2 SigmaSum = sum(Sig2) for i in range(loopNum): SigmaI = sum(Sig2[:i+1]) print('主成分：%s, 方差占比：%s%%' % (format(i+1, '2.0f'), format(SigmaI/SigmaSum*100, '.2f'))) '''基于SVD的评分估计 Args: dataMat 训练数据集 user 用户编号 simMeas 相似度计算方法 item 未评分的物品编号 Returns: ratSimTotal/simTotal 评分（0～5之间的值） ''' def svdEst(dataMat, user, simMeas, item): # 物品数目 n = shape(dataMat)[1] # 对数据集进行SVD分解 simTotal = 0.0 ; ratSimTotal = 0.0 # 奇异值分解,只利用90%能量值的奇异值，奇异值以NumPy数组形式保存 U, Sigma, VT = la.svd(dataMat) # 分析 Sigma 的长度取值 # analyse_data(Sigma, 20) # 如果要进行矩阵运算，就必须要用这些奇异值构建出一个对角矩阵 Sig4 = mat(eye(4) * Sigma[: 4]) # eye对角矩阵 # 利用U矩阵将物品转换到低维空间中，构建转换后的物品(物品+4个主要的特征) xformedItems = dataMat.T * U[:, :4] * Sig4.I # I 逆矩阵 # print('dataMat', shape(dataMat)) # print('U[:, :4]', shape(U[:, :4])) # print('Sig4.I', shape(Sig4.I)) # print('VT[:4, :]', shape(VT[:4, :])) # print('xformedItems', shape(xformedItems)) # 对于给定的用户，for循环在用户对应行的元素上进行遍历 # 和standEst()函数的for循环一样，这里相似度计算在低维空间下进行的。 for j in range(n): userRating = dataMat[user, j] if userRating == 0 or j == item: continue # 相似度的计算方法也会作为一个参数传递给该函数 similarity = simMeas(xformedItems[item, :].T, xformedItems[j, :].T) # for 循环中加入了一条print语句，以便了解相似度计算的进展情况。如果觉得累赘，可以去掉 # print('the %d and %d similarity is: %f' % (item, j, similarity)) # 对相似度不断累加求和 simTotal += similarity # 对相似度及对应评分值的乘积求和 ratSimTotal += similarity * userRating if simTotal == 0: return 0 else: # 计算估计评分 return ratSimTotal/simTotal 排序获取最后的推荐结果 '''recommend函数推荐引擎，默认调用standEst函数，产生最高的N个推荐结果 Args: dataMat 训练数据集 user 用户编号 simMeas 相似度计算方法 estMethod 使用的推荐算法 Returns: 返回最终 N 个推荐结果 ''' def recommend(dataMat, user, N=3, simMeas=cosSim, estMethod=standEst): # 寻找未评级的物品,对给定的用户建立一个未评分的物品列表 unratedItems = nonzero(dataMat[user, :].A == 0)[1] # .A: 矩阵转数组 # 如果不存在未评分物品，那么就退出函数 if len(unratedItems) == 0: return 'you rated everything' # 物品的编号和评分值 itemScores = [] # 在未评分物品上进行循环 for item in unratedItems: # 获取 item 该物品的评分 estimatedScore = estMethod(dataMat, user, simMeas, item) itemScores.append((item, estimatedScore)) # 按照评分得分 进行逆排序，获取前N个未评级物品进行推荐 return sorted(itemScores, key=lambda jj: jj[1], reverse=True)[: N] 测试和项目调用 测试代码 # 计算相似度的方法 myMat = mat(loadExData3()) # 计算相似度的第一种方式 # print(recommend(myMat, 1, estMethod=svdEst)) # 计算相似度的第二种方式 # print(recommend(myMat, 1, estMethod=svdEst, simMeas=pearsSim)) # 默认推荐（菜馆菜肴推荐示例） print(recommend(myMat, 2)) 运行结果 菜馆菜肴推荐结果： [(3, 4.0), (5, 4.0), (6, 4.0)] ***Repl Closed*** 分析结果，我们不难发现，分别对3烤牛肉，5鲁宾三明治、6印度烤鸡给我4星好评，推荐给我们的用户。 要点补充 基于内容(content-based)的推荐 通过各种标签来标记菜肴 将这些属性作为相似度计算所需要的数据 这就是：基于内容的推荐。 构建推荐引擎面临的挑战 问题 1）在大规模的数据集上，SVD分解会降低程序的速度 2）存在其他很多规模扩展性的挑战性问题，比如矩阵的表示方法和计算相似度得分消耗资源。 3）如何在缺乏数据时给出好的推荐-称为冷启动【简单说：用户不会喜欢一个无效的物品，而用户不喜欢的物品又无效】 建议 1）在大型系统中，SVD分解(可以在程序调入时运行一次)每天运行一次或者其频率更低，并且还要离线运行。 2）在实际中，另一个普遍的做法就是离线计算并保存相似度得分。(物品相似度可能被用户重复的调用) 3）冷启动问题，解决方案就是将推荐看成是搜索问题，通过各种标签／属性特征进行基于内容的推荐。 项目案例: 基于SVD的图像压缩收集并准备数据将文本数据转化为矩阵 '''图像压缩函数''' def imgLoadData(filename): myl = [] for line in open(filename).readlines(): newRow = [] for i in range(32): newRow.append(int(line[i])) myl.append(newRow) # 矩阵调入后，就可以在屏幕上输出该矩阵 myMat = mat(myl) return myMat 分析数据: 分析Sigma的长度个数通常保留矩阵 80% ～ 90% 的能量，就可以得到重要的特征并去除噪声。 '''分析 Sigma 的长度取值 根据自己的业务情况，就行处理，设置对应的 Singma 次数 通常保留矩阵 80% ～ 90% 的能量，就可以得到重要的特征并取出噪声。 ''' def analyse_data(Sigma, loopNum=20): # 总方差的集合（总能量值） Sig2 = Sigma**2 SigmaSum = sum(Sig2) for i in range(loopNum): SigmaI = sum(Sig2[:i+1]) print('主成分：%s, 方差占比：%s%%' % (format(i+1, '2.0f'), format(SigmaI/SigmaSum*100, '.2f'))) 使用算法: 对比使用 SVD 前后的数据差异对比，对于存储大家可以试着写写例如：32*32=1024 =&gt; 32*2+2*1+32*2=130(2*1表示去掉了除对角线的0), 几乎获得了10倍的压缩比。 '''打印矩阵 由于矩阵保护了浮点数，因此定义浅色和深色，遍历所有矩阵元素，当元素大于阀值时打印1，否则打印0 ''' def printMat(inMat, thresh=0.8): for i in range(32): for k in range(32): if float(inMat[i, k]) > thresh: print(1) else: print(0) print('') '''实现图像压缩，允许基于任意给定的奇异值数目来重构图像 Args: numSV Sigma长度 thresh 判断的阈值 ''' def imgCompress(numSV=3, thresh=0.8): # 构建一个列表 myMat = imgLoadData('./0_5.txt') print("****original matrix****") # 对原始图像进行SVD分解并重构图像e printMat(myMat, thresh) # 通过Sigma 重新构成SigRecom来实现 # Sigma是一个对角矩阵，因此需要建立一个全0矩阵，然后将前面的那些奇异值填充到对角线上。 U, Sigma, VT = la.svd(myMat) # SigRecon = mat(zeros((numSV, numSV))) # for k in range(numSV): # SigRecon[k, k] = Sigma[k] # 分析插入的 Sigma 长度 # analyse_data(Sigma, 20) SigRecon = mat(eye(numSV) * Sigma[: numSV]) reconMat = U[:, :numSV] * SigRecon * VT[:numSV, :] print("****reconstructed matrix using %d singular values *****" % numSV) printMat(reconMat, thresh) 参考文献 奇异值分解 中文维基百科 GitHub 图书：《机器学习实战》 图书：《自然语言处理理论与实战》 强大的矩阵奇异值分解(SVD)及其应用 奇异值分解 SVD 的数学解释 完整代码下载 源码请进【机器学习和自然语言QQ群：436303759】文件下载： 作者声明 本文版权归作者所有，旨在技术交流使用。未经作者同意禁止转载，转载后需在文章页面明显位置给出原文连接，否则相关责任自行承担。]]></content>
      <categories>
        <category>数据准备</category>
        <category>SVD</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>数据降维</tag>
        <tag>数据预处理</tag>
        <tag>SVD</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[编程数学之相关与回归]]></title>
    <url>%2F2018%2F10%2F10%2F%E7%BC%96%E7%A8%8B%E6%95%B0%E5%AD%A6%E4%B9%8B%E7%9B%B8%E5%85%B3%E4%B8%8E%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[摘要：在数据科学中，统计地位尤为显著。其在数据分析的基础上，研究如何测定、收集、整理、归纳和分析反映数据规律，以便给出正确消息的科学。通过揭示数据背后的规律和隐藏信息，给相关角色提供参照价值，做出相应的决策。这在数据挖掘、自然语言处理、机器学习都广泛应用。本文主要介绍相关和回归，让读者最短时间掌握基本的统计知识。（本文原创，转载必须注明出处.） 相关定义在概率论和统计学中，相关或称相关系数或关联系数，显示两个随机变量之间线性关系的强度和方向。在统计学中，相关的意义是用来衡量两个变量相对于其相互独立的距离。在这个广义的定义下，有许多根据数据特点而定义的用来衡量数据相关的系数。 相关性相关性即变量之间的数学关系，通过散点图上的点的独特构成模式，可以识别出散点图上的各种相关性。如果散点图上的点几乎呈直线分布，则相关性为线性。 正线性相关：当x轴上的低端值对应y轴上的低端值，同时x轴的高端值对应y轴上的高端值且呈直线分布时，为正线性相关。即随着x增长，y也呈现增长趋势。 负线性相关：当x轴上的低端值对应y轴上的高端值，同时x轴的高端值对应y轴上的低端值且呈直线分布时，为负线性相关。即随着x增长，y呈现下降趋势。 不相关：如果x和y呈现一种随机模式，则我们说二者不相关。 回归最佳拟合线预测 假设最佳拟合线的方程为：\(y=ax+b\) 计算自变量X和因变量Y的均值： 利用最小二乘法回归法求最佳拟合线的斜率： 计算最佳拟合线的切距：\( a=\vec{y}-b\vec{x}\) 由求得的斜率和切距得出最佳拟合线的方程. 计算自变量X和因变量Y的标准差：， 计算相关系数：\( r=\frac{b_{s_x}}{s_y}\) 通过相关系数判断所求最佳拟合线与数据的拟合度，规则如下： 如果相关系数的绝对值越接近1，则所求最佳拟合线的拟合度越高，可用于数据预测。 如果相关系数的绝对值越接近0，则所求最佳拟合线的拟合度越低，不推荐用于进行预测（预测的结果可能不准确）。 应用案例案例定义：有一个二变量数据同时给出预计天晴时数和音乐会听众人数（其中：天晴时数表示自变量，听众人数表示因变量），如下表所示： 如果音乐会当天预计天晴时数可能为4.3小时，请问音乐会听众人数可能会有多少人？ 该场景下算法计算方法 (1) 假设最佳拟合线的方程为：\( y=ax+b\)(2)计算天晴时数和听众人数的均值： (3) 利用最小二乘法回归法求最佳拟合线的斜率： (4) 计算最佳拟合线的切距：\( a=\vec{y}-b\vec{x}=38.875-5.32\times4.3375=15.8\) (5) 由求得的斜率和切距得出最佳拟合线的方程： (6)计算天晴时数和听众人数的标准差： (7) 计算相关系数： (8) 通过相关系数判断所求最佳拟合线与数据的拟合度并得出预测结果： 由于r接近1，说明音乐会听众人数和预计天晴时数之间有很强的正相关。换句话说，根据已有的数据，利用最佳拟合线根据预计天晴时数给出了期望音乐会听总人数的合理的良好估计。当音乐会当天预计天晴时数可能为4.3小时，利用最佳拟合线方程，那么就可估计当天音乐会听众人数大约会是3868人。 线性回归和逻辑回归在线性回归中，结果(因变量)是连续的。它可以具有无限数量的可能值中的任何一个。在逻辑回归中，结果(因变量)只有有限数量的可能值。 例如，如果X包含房屋平方英尺的面积，并且Y包含这些房屋的相应销售价格，则可以使用线性回归来预测售价作为房屋面积的函数。虽然可能的售价可能不是实际的，但是有很多可能的值可以选择线性回归模型。 相反，如果您想根据大小来预测房屋的出售金额是否超过$ 200K，您将使用逻辑回归。可能的输出是是的，房子将卖出超过$ 200K，否则，房子不会。 相关和回归的联系两者区别回归和相关都是研究两个变量相互关系的分析方法。相关分析研究两个变量之间相关的方向和相关的密切程度。但是相关分析不能指出两变量相互关系的具体形式，也无法从一个变量的变化来推测另一个变量的变化关系。回归方程则是通过一定的数学方程来反映变量之间相互关系的具体形式，以便从一个已知量来推测另一个未知量。为估算预测提供一个重要的方法。具体区别有： 相关分析中变量之间处于平等的地位；回归分析中，因变量处在被解释的地位，自变量用于预测因变量的变化； 相关分析中不必确定自变量和因变量，所涉及的变量可以都是随机变量；而回归分析则必须事先确定具有相关关系的变量中，哪个是因变量，哪个是因变量。一般来说，回归分析中因变量是随机变量，而把自变量作为研究时给定的非随机变量； 相关分析研究变量之间相关的方向和相关的程度，但相关分析不能根据一个变量的变化来推测另一个变量的变化情况；回归分析是研究变量之间相互关系的具体表现形式，根据变量之间的联系确定一个相关的数学表达式，从而可以从已知量来推测未知量。 对两个变量来说，相关分析中只能计算出一个相关系数；而回归分析中有时可以根据研究目的的不同建立两个不同的回归方程。 两者联系相关分析与回归分析是广义相关分析的两个阶段,两者有着密切的联系 ： 相关分析是回归分析的基础和前提，回归分析则是相关分析的深入和继续。相关分析需要依靠回归分析来表现变量之间数量相关的具体形式，而回归分析则需要依靠相关分析来表现变量之间数量变化的相关程度。只有当变量之间存在高度相关时，进行回归分析寻求其相关的具体形式才有意义。如果在没有对变量之间是否相关以及相关方向和程度做出正确判断之前，就进行回归分析，很容易造成“虚假回归”。 由于相关分析只研究变量之间相关的方向和程度，不能推断变量之间相互关系的具体形式，也无法从一个变量的变化来推测另一个变量的变化情况，因此，在具体应用过程中，只有把相关分析和回归分析结合起来，才能达到研究和分析的目的。 统计假设检验定义假设检验是推论统计中用于检验统计假设的一种方法。 而“统计假设”是可通过观察一组随机变量的模型进行检验的科学假说。一旦能估计未知参数，就会希望根据结果对未知的真正参数值做出适当的推论。假设检验的种类包括：t检验，Z检验，卡方检验，F检验等等。 假设检验过程假设检验的过程，可以用法庭的审理来说明。先想像现在法庭上有一名被告，假设该被告是清白的，而检察官必须要提出足够的证据去证明被告的确有罪。 在证明被告有罪前，被告是被假设为清白的。 假设被告清白的假设，就相当于零假设。 假设被告有罪的假设，则是备择假设。 而检察官提出的证据，是否足以确定该被告有罪，则要经过检验。 这样子的检验过程就相当于用T检验或Z检验去检视研究者所搜集到的统计资料。 检验过程在统计学的文献中，假设检验发挥了重要作用。假设检验大致有如下步骤： 最初研究假设为真相不明。 第一步是提出相关的零假设和备择假设。这是很重要的，因为错误陈述假设会导致后面的过程变得混乱。 第二步是考虑检验中对样本做出的统计假设；例如，关于独立性的假设或关于观测数据的分布的形式的假设。这个步骤也同样重要，因为无效的假设将意味着试验的结果是无效的。 决定哪个检测是合适的，并确定相关检验统计量 T。 在零假设下推导检验统计量的分布。在标准情况下应该会得出一个熟知的结果。比如检验统计量可能会符合学生t-分布或正态分布。 选择一个显著性水平 (α)，若低于这个概率阈值，就会拒绝零假设。最常用的是 5% 和 1%。 根据在零假设成立时的检验统计量T分布，找到数值最接近备择假设，且概率为显著性水平 (α)的区域，此区域称为“拒绝域”，意思是在零假设成立的前提下，落在拒绝域的概率只有α。 针对检验统计量T，根据样本计算其估计值tobs。 若估计值tobs未落在“拒绝域”，接受零假设。若估计值tobs落在“拒绝域”，拒绝零假设，接受备择假设。 应用实例淑女品茶是一个有关假设检验的著名例子，费雪的一个女同事声称可以判断在奶茶中，是先加入茶还是先加入牛奶。费雪提议给她八杯奶茶，四杯先加茶，四杯先加牛奶，但随机排列，而女同事要说出这八杯奶茶中，哪些先加牛奶，哪些先加茶，检验统计量是确认正确的次数。零假设是女同事无法判断奶茶中的茶先加入还是牛奶先加入，备择假设为女同事有此能力。若单纯以概率考虑（即女同事没有判断的能力）下，八杯都正确的概率为1/70，约1.4%，因此“拒绝域”为八杯的结果都正确。而测试结果为女同事八杯的结果都正确[3]，在统计上是相当显著的的结果。 参考文献 Python官网 中文维基百科 GitHub 图书：《机器学习实战》 图书：《自然语言处理理论与实战》 完整代码下载 源码请进【机器学习和自然语言QQ群：436303759】文件下载： 作者声明 本文版权归作者所有，旨在技术交流使用。未经作者同意禁止转载，转载后需在文章页面明显位置给出原文连接，否则相关责任自行承担。]]></content>
      <categories>
        <category>数学</category>
        <category>统计学</category>
      </categories>
      <tags>
        <tag>数学</tag>
        <tag>统计学</tag>
        <tag>相关</tag>
        <tag>回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[编程数学之概率分布]]></title>
    <url>%2F2018%2F10%2F10%2F%E7%BC%96%E7%A8%8B%E6%95%B0%E5%AD%A6%E4%B9%8B%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83%2F</url>
    <content type="text"><![CDATA[摘要：在数据科学中，统计地位尤为显著。其在数据分析的基础上，研究如何测定、收集、整理、归纳和分析反映数据规律，以便给出正确消息的科学。通过揭示数据背后的规律和隐藏信息，给相关角色提供参照价值，做出相应的决策。这在数据挖掘、自然语言处理、机器学习都广泛应用。本文主要介绍概率分布，让读者最短时间掌握基本的统计知识。（本文原创，转载必须注明出处.） 几何分布定义几何分布是离散型概率分布，（如图所示）。在n次伯努利试验中，试验k次才得到第一次成功的机率。 详细的说，是：前k-1次皆失败，第k次成功的概率。几何分布公式如下：\(P(X=r)=q^{r-1}p\) 计算公式成功概率为p，失败概率为q，试验次数为r，则有： 第r次试验第一次成功：\( P(X=r)=pq^{r-1} \) 需要试验r次以上才第一次成功：\( P(X&gt;r)=q^{r} \) 试验r次或者不到r次才第一次成功：\( P(X&lt;r)=1-q^{r}\) 几何分布的条件 进行一系列相互独立的实验。 每一次实验既有成功，又有失败的可能，且单次实验成功概率相等。 为了取得第一次成功需要进行多少次实验。 几何分布的期望期望特点：随着x变大，累计总数和越来越接近一个特定值。\( E(X)=\frac{1}{p}\) 几何分布的方差方差特点：随着x变大，方差越来越接近特定值 Var(X)=\frac{q}{p^2}应用范围 应用科学：数学以及相关领域 适用领域范围：自然数学，应用数学，高等数学，概率论 射击比赛等 二项分布定义二项分布即重复n次独立的伯努利试验。在每次试验中只有两种可能的结果，而且两种结果发生与否互相对立，并且相互独立，与其它各次试验结果无关，事件发生与否的概率在每一次独立试验中都保持不变。 计算公式在相互独立事件中，每道题答对概率为p，答错概率为q。在n个问题中答对r个问题的概率为二项分布，表达式是\( X\sim B(n,p)\) X表示n次随机变量ξ次成功数，p表示成功的概率。计算公式： 二项分布的条件 正在进行一系列独立试验； 每次试验都可能失败和成功，每一次成功概率相同。 试验次数有限。 二项分布形状特点，如图所示 P0.5时，图形向左偏移。 优缺点 优点：在试验次数一定，求成功次数时，几何分布显示不适合的情况下，给予这类问题二项分布能更好的解决。 缺点：但是面对试验次数不固定，发生事件概率的情况下，显然几何分布与二项分布都不能解决，这里也体现出泊松分布的优势 二项分布的期望 几何分布的方差 应用范围 应用科学：数学以及相关领域 适用领域范围：自然数学，应用数学，高等数学，概率论 射击比赛等 正态分布正态分布描述正态分布又名高斯分布, 以德国数学家卡尔·弗里德里希·高斯的姓冠名，是一个在数学、物理及工程等领域都非常重要的概率分布，由于这个分布函数具有很多非常漂亮的性质，使得其在诸多涉及统计科学离散科学等领域的许多方面都有着重大的影响力。比如图像处理中最常用的正态分布函数，下图是正态分布示意图。 若随机变量\( X \)服从一个位置参数为 \( \mu\)、尺度参数为\(\sigma\) 的概率分布，记为：\( X\sim N(\mu ,\sigma ^{2})\),则其概率密度函数为 正态分布的数学期望值或期望值\( \mu\)等于位置参数，决定了分布的位置；其方差\(\sigma^2\) 的开平方或标准差\(\sigma\) 等于尺度参数，决定了分布的幅度。 定义正态分布概率函数密度曲线可以表示为： 称\(X\)服从正态分布，记为\(X \sim N(m,s_2)\)，其中\(\mu\)为均值，\(\sigma\)为标准差。标准正态分布另正态分布：\( \mu=0\),\( \sigma=1 \)，公式简化为： 正态分布特点 正态分布函数密度曲线在横轴上方均数处最高。 正态分布函数密度曲线以均数为中心，左右对称。 正态分布函数密度曲线有两个参数，即均数（μ）和标准差（s）。μ是位置参数，当s固定不变时，μ越大，曲线沿横轴,越向右移动；反之，μ越小，则曲线沿横轴,越向左移动。是形状参数，当μ固定不变时，s越大，曲线越平阔；s越小，曲线越尖峭。通常用N表示均数为μ，方差为s的正态分布。用N（0，1）表示标准正态分布。 正态分布函数密度曲线下面积的总和为1。 正态分布的期望 几何分布的方差 正态概率计算步骤第一步：首先确定数据是否符合正态分布，确定正态分布的均值和方差。对一些不符和正态分布的数据进行取对数或者样本重新排列称符合正态分布的标准后，在确定均值和方差。 第二步：标准化（平移，收放）：对一般正态分布进行标准化，标准化的过程为先平移，平移过程用公式表达即\( x-\mu \)，再对结果进行收放，收放过程即\(\frac{y}{\sigma}\)，其中\(y=x-\mu \) 。则标准化公式：\(Z=\frac{(x-\mu)}{\sigma} \)；其中Z为标准分，\(X\)为随机变量，\(\mu\)为均值，\(\sigma\)为标准差。 第三步：使用概率表：通过标准分，进行查表（标准正态分布概率表），得到具体的概率。 正态分布的优缺点 优点：对于社会上遇到的大部分问题，其概率分布规律基本都满足正态分布，为了计算某种概率，我们就可以通过数学建模利用正态分布方便解决问题。 缺点：无法近似估算符合几何分布的问题，无法精确解决离散数据概率。 应用场景 不适合应用场景： 数据离散性太大，数据不符合正态分布特点，通过对数据进行取对数或者重新排序亦无法达到正态分布特点，无法得出均数（期望）和标准差。 适用场景：连续型数据或者数据离散性小，数据基本符合正态分布特点，或者对不符合的数据进行取对数或者样本重新排序达到正态分布特点，有具体的均数（期望）和标准差。 中心极限定理正态分布有一个非常重要的性质：在特定条件下，大量统计独立的随机变量的平均值的分布趋于正态分布，这就是中心极限定理。中心极限定理的重要意义在于，根据这一定理的结论，其他概率分布可以用正态分布作为近似。 参数为n和 p的二项分布，在n相当大而且 p接近0.5时近似于正态分布。近似正态分布平均数为\(\mu=np\)，且方差为\( \sigma ^{2}=np(1-p)\) 泊松分布带有参数\(\lambda\) 当取样样本数很大时将近似正态分布\(\lambda\) 。近似正态分布平均数为 \(\mu=\lambda\) 且方差为\(\sigma^2=\lambda\)。 泊松分布定义泊松分布适合于描述单位时间内随机事件发生的次数的概率分布。如某一服务设施在一定时间内受到的服务请求的次数，电话交换机接到呼叫的次数、汽车站台的候客人数、机器出现的故障数、自然灾害发生的次数、DNA序列的变异数、放射性原子核的衰变数、激光的光子数分布等等。泊松分布的概率质量函数为： 泊松分布的参数λ是单位时间（或单位面积）内随机事件的平均发生率。 计算公式X服从参数为\(\lambda\)的泊松分布，记为\( X\sim P(\lambda )\)。单独事件在给定区间随机独立发生，已知事件平均发生数且有限次数，通过以下计算： 泊松分布的条件 单独事件在给定区间内随机独立的发生，给定区别可以是时间或者空间。（一周、一英里） 已知该区间内的事件平均发生次数（发生率），且为有限数值。该事件平均发生次数用λ表示。 泊松分布形状特点 不需要一系列试验，描述事件特定区间发生次数。 两个独立的泊松分布相加也符合泊松分布。（即n&gt;50且p&lt;0.1时或np近似等于npq时）。 特定条件下可以用来近似代替二项分布。 泊松分布形状特点：λ小时，分布向右偏斜；当λ大时，分布逐渐对称。 优缺点不需要一系列试验，描述事件特定区间发生次数，特别适用。另外一定条件下替换二项分布带来简便的运算。 泊松分布与二项分布关系当二项分布X~B(n,p)的n很大而p很小时，泊松分布可作为二项分布的近似，其中λ为np。通常当n≧10,p≦0.1，np&lt;=5时，就可以用泊松公式近似得计算，X可以近似表示X~Po（np）。 问题：为什么n要足够大，p要足够小？ 因为在分时间窗口的时候有个假设：每个时间窗口最多只有一个乘客到达。(时间区间乘客问题) 泊松分布的期望 几何分布的方差 应用范围 用学科：概率论 某一服务设施在一定时间内到达人数，电话交换机接到呼叫的次数，汽车站台的侯客人数，机器出现的故障次数，自然灾害发生次数，一块产品的缺陷，显微镜下单位分区内的细菌分布数等。 在交通工程的应用、非典流行与传播服从泊松分布 自然现象普遍存在泊松分布现象，主要指大量重复实验中稀有事件发生的次数。 参考文献 Python官网 中文维基百科 GitHub 图书：《机器学习实战》 图书：《自然语言处理理论与实战》 完整代码下载 源码请进【机器学习和自然语言QQ群：436303759】文件下载： 作者声明 本文版权归作者所有，旨在技术交流使用。未经作者同意禁止转载，转载后需在文章页面明显位置给出原文连接，否则相关责任自行承担。]]></content>
      <categories>
        <category>数学</category>
        <category>统计学</category>
      </categories>
      <tags>
        <tag>数学</tag>
        <tag>统计学</tag>
        <tag>几何分布</tag>
        <tag>正态分布</tag>
        <tag>泊松分布</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[编程数学之数据度量标准]]></title>
    <url>%2F2018%2F10%2F10%2F%E7%BC%96%E7%A8%8B%E6%95%B0%E5%AD%A6%E4%B9%8B%E6%95%B0%E6%8D%AE%E5%BA%A6%E9%87%8F%E6%A0%87%E5%87%86%2F</url>
    <content type="text"><![CDATA[摘要：在数据科学中，统计地位尤为显著。其在数据分析的基础上，研究如何测定、收集、整理、归纳和分析反映数据规律，以便给出正确消息的科学。通过揭示数据背后的规律和隐藏信息，给相关角色提供参照价值，做出相应的决策。这在数据挖掘、自然语言处理、机器学习都广泛应用。本文主要介绍数据度量标准，让读者最短时间掌握基本的统计知识。（本文原创，转载必须注明出处.） 平均值定义均值是统计中的一个重要概念。为集中趋势的最常用测度值，目的是确定一组数据的均衡点。 均值的计算将所有的数字加起来，然后除以数字的个数 。可用记为：\( \mu =\frac{ \sum_{i}^{n} x_i }{n}\) 应用实例寒假英语兴趣班的总人数为28，总共有7个小组。知道了总的人数和总共有多少个小组，即可求出每组人数的均值。 均值的优缺点优点： 可以用它来反映一组数据的一般情况，也可以用它进行不同组数据的比较，以看出组与组之间的差别。 缺点： 只能应用于数值型数据，不能用于分类数据和顺序数据。 中位数定义中位数又称中值，统计学中的专有名词，代表一个样本、种群或概率分布中的一个数值，其可将数值集合划分为相等的上下两部分。在 n 个数据由大到小排序后，位在中间的数字。 中位数的计算 按顺序排列数字：从最小值排列到最大值 如果有奇数n个数值，则中位数为位于中间的数值。中间数的位置为\(frac{n+1}{2}\) 如果有偶数n个数值，则将两个中间数相加，然后除以2。中间位置的算法是：\(\frac{n}{2}\)。 应用实例有一组数据如表所示，，求出该组数据的中位数。 (1) 按顺序排列数字：从最小值排列到最大值： 19 19 19 20 20 20 20 20 20 21 21 21 145 147 (2) 统计总的个数为：3+6+3+1+1=14个 (3) 因为偶数选择算法：\( frac{n}{2}=\frac{14}{2}=7\) (4) 找到第七个位置，则中位数是：20。（注意，计算机中下标从0开始，即寻找(7-1)的位置） 中位数的特点一个数集中最多有一半的数值小于中位数，也最多有一半的数值大于中位数。如果大于和小于中位数的数值个数均少于一半，那么数集中必有若干值等同于中位数。 众数定义众数指一组数据中出现次数最多的数据值。例如{2,3,3,3}中，出现最多的是3，因此众数是3，众数可能是一个数，但也可能是多个数。众数主要用于分类数据，也可用于顺序数据和数值型数据。 众数的计算 把数据中的不同类别或数值全部找出来。 写出每个数值或类别的频数。 挑出具有最高频数的一个或几个数值，得出众数。 应用实例有一组数据：19 19 19 20 20 20 20 20 20 21 21 21 147 145，求出该组数据的中位数。 (1) 把数据中的不同类别或数值全部找出来：19,20,21,147,145。 (2) 写出每个数值或类别的频数，如表所示。 (3) 挑出具有最高频数的一个或几个数值，得出众数。观察易知是：20。总共出现6次。 扩展：如果将上面一组数字在多加3个19和3个21。则众数就变成三个即19、20和21。 众数的特点 在离散概率分布中，众数是指概率质量函数有最大值的数据，也就是最容易取様到的数据。在连续概率分布中，众数是指概率密度函数有最大值的数据，也就是概率密度函数的峰值。 在高斯分布（正态分布）中，众数位于峰值，和平均数、中位数相同。但若分布是高度偏斜分布，众数可能会和平均数、中位数有很大的差异。 用众数代表一组数据，适合于数据量较多时使用，且众数不受极端数据的影响，并且求法简便。在一组数据中，如果个别数据有很大的变动，选择中位数表示这组数据的“集中趋势”就比较适合。 期望定义在概率论和统计学中，期望为期望值的简称，是指在一个离散型随机变量试验中每次可能结果的概率乘以其结果的总和。随机变量X的期望通常写作E(X)，但有时也会写作μ，也就是均值的符号。下面是E(X)的计算式：\(E(X) =\sum{sP(X=x)}\) 应用案例假设下表为游戏机的概率分布： 游戏机收益的期望： 游戏机收益的方差： 游戏机收益的期望： 方差定义 形式化描述 方差：在概率论和统计学中，一个随机变量的方差描述的是它的离散程度，也就是该变量离其期望值的距离。量度数据分散性的一种方法，是数据与均值的距离的平方数的平值。 数学化描述 设X为服从分布F的随机变量， 如果E[X]是随机变数X的期望值（平均数μ=E[X]）随机变量X或者分布F的方差为： 计算方法 连续随机变数 如果随机变数X是连续分布，并对应至概率密度函数f(x)，则其方差为： 此处\(\mu\)是一期望值，\( \mu =\int xf(x)dx\) 离散随机变数： 如果随机变数X是具有概率质量函数的离散概率分布x1 ↦ p1, …, xn ↦ pn，则： 此处 \(\mu\) 是其期望值：\( \mu =\sum _{i=1}^np_i\cdot x_i\) 特点 方差不会是负的，因为次方计算为正的或为零。 标准差定义形式化描述：标准差，是描述典型值与均值距离的一种方法，标准差越小，数值离均值越近 。 数学化描述：标准差 σ=方差开方 注：标准差也有可能为0，如果每个数值与均值的距离都是为0，则标准差将为0。 计算方法 计算出该组数据的均值u 再统计该组数据的个数n 利用方差的公式计算出方差 利用标准差的公式计算出标准差 标准分定义标准分数也叫z分数，是一种具有相等单位的量数。它是将原始分数与团体的平均数之差除以标准差所得的商数，是以标准差为单位度量原始分数离开其平均数的分数之上多少个标准差，或是在平均数之下多少个标准差。它是一个抽象值，不受原始测量单位的影响，并可接受进一步的统计处理。 计算公式用公式表示为：z=(x-μ)/σ；其中z为标准分数；x为某一具体分数，μ为平均数，σ为标准差。 Z值的量代表着原始分数和母体平均值之间的距离，是以标准差为单位计算。在原始分数低于平均值时Z则为负数，反之则为正数。 标准分的计算公式：z=(x-μ)/σ 特点标准分数是一种不受原始测量单位影响的数值。其作用除了能够表明原数据在其分布中的位置外，还能对未来不能直接比较的各种不同单位的数据进行比较。如比较各个学生的成绩在班级成绩中的位置或比较某个学生在两种或多种测验中所得分数的优劣。 应用实例例如，有两名考生的高考入学考试成绩，根据原始分数乙考生的总分是400分，而甲只有382分，按总录取则取乙生，若按标准分数录取则应录取甲，因为甲的所有成绩都不低于平均分数，而乙却在数学、外语二门学科上低于平均分数，可见把分数标准化(转换为标准分数)是有好处的。 其应用在数据预处理的数据归一化中，比如数据集中，列向量显示英语、数学等成绩。其中一个列向量可能是绩点比如3.2吧。这个3.2 与成绩 90分。根本不在一个数据量级上，这种情况下会用到标准分进行数据归一化处理。 参考文献 Python官网 中文维基百科 GitHub 图书：《机器学习实战》 图书：《自然语言处理理论与实战》 完整代码下载 源码请进【机器学习和自然语言QQ群：436303759】文件下载： 作者声明 本文版权归作者所有，旨在技术交流使用。未经作者同意禁止转载，转载后需在文章页面明显位置给出原文连接，否则相关责任自行承担。]]></content>
      <categories>
        <category>数学</category>
        <category>统计学</category>
      </categories>
      <tags>
        <tag>数学</tag>
        <tag>统计学</tag>
        <tag>平均值</tag>
        <tag>众数</tag>
        <tag>期望</tag>
        <tag>方差</tag>
        <tag>标准差</tag>
        <tag>标准分</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[编程数学之图形可视化分析]]></title>
    <url>%2F2018%2F10%2F10%2F%E7%BC%96%E7%A8%8B%E6%95%B0%E5%AD%A6%E4%B9%8B%E5%9B%BE%E5%BD%A2%E5%8F%AF%E8%A7%86%E5%8C%96%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[摘要：在数据科学中，统计地位尤为显著。其在数据分析的基础上，研究如何测定、收集、整理、归纳和分析反映数据规律，以便给出正确消息的科学。通过揭示数据背后的规律和隐藏信息，给相关角色提供参照价值，做出相应的决策。这在数据挖掘、自然语言处理、机器学习都广泛应用。本文主要介绍常见的图形可视化的概念和使用，使读者最短时间掌握基本的统计知识。（本文原创，转载必须注明出处.） 饼图定义饼图广泛得应用在各个领域，用于表示不同分类的占比情况，通过弧度大小来对比各种分类。饼图通过将一个圆饼按照分类的占比划分成多个区块，整个圆饼代表数据的总量，每个区块（圆弧）表示该分类占总体的比例大小，所有区块（圆弧）的加和等于 100%。 饼图的优缺点 优点：饼图可以很好地帮助用户快速了解数据的占比分配 缺点： 饼图不适用于多分类的数据，原则上一张饼图不可多于 9 个分类，因为随着分类的增多，每个切片就会变小，最后导致大小区分不明显，每个切片看上去都差不多大小，这样对于数据的对比是没有什么意义的。所以饼图不适合用于数据量大且分类很多的场景。 相比于具备同样功能的其他图表（比如百分比柱状图、环图），饼图需要占据更大的画布空间。 很难进行多个饼图之间的数值比较。 应用场景 适合的场景 展示2个分类的占比情况。比如一个班级的男女生的占比情况。 多个但不超过 9 个分类的占比情况。比如一个游戏公司的销售情况。 不适合的场景 分类过多的场景。比如各个省的人口的占比情况，很难清晰对比各个省份的人口数据占比情况，所以这种情况下，我们推荐使用横向柱状图。 分类占比差别不明显的场景，比如游戏公司的不同种类的游戏的销售量相近，所以不太适合使用饼图，此时可以使用柱状图来呈现。 饼图与其他图表的对比 饼图和柱状图 饼图主要是展示分类之间的占比情况。 而柱状图主要展示各个分类数量、大小的对比。 饼图示意图某站点用户访问来源饼状图统计， 如下图所示： 条形图定义典型的条形图（又名柱状图），使用垂直或水平的柱子显示类别之间的数值比较。其中一个轴表示需要对比的分类维度，另一个轴代表相应的数值。 柱状图有别于直方图，柱状图无法显示数据在一个区间内的连续变化趋势。柱状图描述的是分类数据，回答的是每一个分类中“有多少？”这个问题。 需要注意的是，当柱状图显示的分类很多时会导致分类名层叠等显示问题，下面我们会举例说明。 条形图的优缺点 优点: 可以很清晰的看出每个类的总和和各个属性的比例。 缺点: 不容易看出各个属性的频数。 应用场景 适合的场景 适合应用到分类数据对比。比如一个游戏销量的图表，展示不同游戏类型的销量对比 不适合的场景 分类太多不适合使用纵向柱状图。如对比不同省份的人口数量。分类情况过多时，柱状图的文本为了排布合理，需要进行旋转，不利于阅读，相比于纵向柱状图，横向柱状图更适用于此类分类较多的场景。 不适合表示趋势 柱状图使用矩形的长度（宽度）来对比分类数据的大小，非常方便临近的数据进行大小的对比，不适合展示连续数据的趋势。比如展示 ACME 这只股票在 2015 年 9 月份整个月的每日的价格走势，但是效果不尽人意。 条形图与其他图表的对比 柱状图和折线图、饼图 柱状图主要用于多个分类间的数据（大小、数值）的对比 折线图主要用于展示连续数值（例如时间）或者有序分类的变化趋势 饼图主要是展示分类之间的占比情况 条形图示意图 水平条形图：世界人口总量条形图统计，如下所示： 垂直条形图：某地区蒸发量和降水量统计，如下图所示： 堆叠条形图：某站点用户访问统计，如下图所示： 热力图定义热力图(Heat Map)，“热力图”一词最初是由软件设计师Cormac Kinney于1991年提出并创造的，用来描述一个2D显示实时金融市场信息。最开始的热力图，是矩形色块加上颜色编码。经过多年的演化，习语上的热力图，如今更规范，更被大多数人理解的是这种经过平滑模糊过的热力图谱。 热力图的特点 热力图尤其关注分布。 热力图可以不需要坐标轴，其背景常常是图片或地图。 热力图一般情况用其专有的色系彩虹色系(rainbow) 热力图能告诉你，页面的哪些部分吸引了大多数访客的注意。 热力图可以直观清楚地看到页面上每一个区域的访客兴趣焦点。 热力图应用场景 适合的场景 连续数值数据分布。城市房租热力图，用于显示城市房租价格分布。 数据的统计预测。钻石克拉数和价格的关系。通过已有的钻石数据，对未知区域的钻石数据进行预测。 热力图示意图笛卡尔坐标系上的热力图，如下所示： 折线图定义折线图用于显示数据在一个连续的时间间隔或者时间跨度上的变化，它的特点是反映事物随时间或有序类别而变化的趋势。在折线图中，数据是递增还是递减、增减的速率、增减的规律（周期性、螺旋性等）、峰值等特征都可以清晰地反映出来。所以，折线图常用来分析数据随时间的变化趋势，也可用来分析多组数据随时间变化的相互作用和相互影响。例如可用来分析某类商品或是某几类相关的商品随时间变化的销售情况，从而进一步预测未来的销售情况。在折线图中，一般水平轴（X轴）用来表示时间的推移，并且间隔相同；而垂直轴（Y轴）代表不同时刻的数据的大小。 折线图应用场景如果分类标签是文本并且代表均匀分布的数值（如月、季度或财政年度），则应该使用折线图。当有多个系列时，尤其适合使用折线图 — 对于一个系列，应该考虑使用类别图。如果有几个均匀分布的数值标签（尤其是年），也应该使用折线图。如果拥有的数值标签多于十个，请改用散点图。另外，折线图是支持多数据进行对比的。 适合的场景 有序的因变量，比如：时间。某监控系统的折线图表，显示了请求次数和响应时间随时间的变化趋势。 不同月份的温度 不适合的场景 当水平轴的数据类型为无序的分类或者垂直轴的数据类型为连续时间时，不适合使用折线图。 当折线的条数过多时不建议将多条线绘制在一张图上,下图展示了多台机器（实例）的资源占用情况 折线图与其他图表的对比 折线图和柱状图 柱状图主要用于多个分类间的数据（大小、数量）的对比，折线图主要用于时间或者连续数据上的趋势。 分类间的数据比较，如果分类不存在顺序，那么不要使用折线图。 折线图和面积图 折线图和面积图都可以表示一段时间（或者有序分类）的趋势，相比之下面积图的表现力更强一些。 面积图还可以表示数据的上下限，例如可以表示温度的最小值、最大值。 折线图示意图未来一周气温变化折线图，如下图所示： 箱线图定义箱形图又称盒须图、盒式图或箱线图，是一种用作显示一组数据分布情况的统计图。 如果一个数据集中包含了一个分类变量和一个或者多个连续变量，那么你可能会想知道连续变量会如何随着分类变量水平的变化而变化，而箱形图就可以提供这种方法，它只用了5个数字对分布进行概括，即一组数据的最大值、最小值、中位数、下四分位数及上四分位数。对于数据集中的异常值，通常会以单独的点的形式绘制。箱形图可以水平或者垂直绘制。 箱形图多用于数值统计，虽然相比于直方图和密度曲线较原始简单，但是它不需要占据过多的画布空间，空间利用率高，非常适用于比较多组数据的分布情况。 从箱形图中我们可以观察到： 一组数据的关键值：中位数、最大值、最小值等。 数据集中是否存在异常值，以及异常值的具体数值。 数据是否是对称的。 这组数据的分布是否密集、集中。 数据是否扭曲，即是否有偏向性。 箱线图特点分析以下是箱形图的具体例子： 这组数据显示出： 最小值(minimum)=5 下四分位数(Q1)=7 中位数(Med —也就是Q2)=8.5 上四分位数(Q3)=9 最大值(maximum )=10 平均值=8 四分位间距(interquartile range)= Q3-Q1=2 (即ΔQ)在区间 Q3+3ΔQ, Q1-3ΔQ 之外的值被视为应忽略(farout)。 farout: 在图上不予显示，仅标注一个符号∇。 最大值区间： Q3+1.5ΔQ 最小值区间： Q1-1.5ΔQ最大值与最小值产生于这个区间。区间外的值被视为outlier显示在图上. mild outlier = 3.5 extreme outlier = 0.5 箱线图应用场景 关注于一组数据的分布情况。比如经典的鸢尾花数据为例。我们用箱形图将不同种类的鸢尾花的花萼和花瓣的长度、宽度数据展示出来，同时我们还可以比较不同品种间花瓣和萼片数据是如何变化的。 分组箱形图。 为了更清晰得比较不同品种间相同属性数值的区别，可以将上图变化为如下二维多个箱形图形式。 一维箱形图。 箱形图有多种变换，这里介绍下一维箱形图，比如还以鸢尾花数据为例，展示的是所有品种的花萼和花瓣长度、宽度的情况，虽然是在一维坐标系中，但是通过添加颜色属性，可以为该一维箱形图再增加一个展示维度，即分类。 箱线图示意图两种性别的三种基因含量表箱线图，如下图所示： 散点图定义散点图也叫 X-Y 图，它将所有的数据以点的形式展现在直角坐标系上，以显示变量之间的相互影响程度，点的位置由变量的数值决定。 通过观察散点图上数据点的分布情况，我们可以推断出变量间的相关性。如果变量之间不存在相互关系，那么在散点图上就会表现为随机分布的离散的点，如果存在某种相关性，那么大部分的数据点就会相对密集并以某种趋势呈现。数据的相关关系主要分为：正相关（两个变量值同时增长）、负相关（一个变量值增加另一个变量值下降）、不相关、线性相关、指数相关等，表现在散点图上的大致分布如下图所示。那些离点集群较远的点我们称为离群点或者异常点。 散点图经常与回归线（就是最准确地贯穿所有点的线）结合使用，归纳分析现有数据以进行预测分析。 对于那些变量之间存在密切关系，但是这些关系又不像数学公式和物理公式那样能够精确表达的，散点图是一种很好的图形工具。但是在分析过程中需要注意，这两个变量之间的相关性并不等同于确定的因果关系，也可能需要考虑其他的影响因素。 散点图的特点散点图通常用于显示和比较数值，不光可以显示趋势，还能显示数据集群的形状，以及在数据云团中各数据点的关系。 散点图应用场景 男女身高和体重的例子来展示上述所描述的散点图的功能。 散点图与其他图表的对比 散点图和折线图 折线图可以显示随单位（如：单位时间）而变化的连续数据，因此非常适用于显示在相等时间间隔下数据的趋势。 散点图显示若干数据系列中各数值之间的关系，或者将两组数绘制为 xy 坐标的一个系列。 在折线图中，类别数据沿水平轴均匀分布，所有值数据沿垂直轴均匀分布，即折线图只有一个数据轴（即垂直轴）。 散点图有两个数值轴，沿水平轴（x 轴）方向显示一组数值数据，沿垂直轴（y 轴）方向显示另一组数值数据。散点图将这些数值合并到单一数据点并以不均匀间隔或簇显示它们。散点图通常用于显示和比较数值，例如科学数据、统计数据和工程数据。 散点图和气泡图 散点图和气泡图都是将两个字段映射到x,y轴的位置上。散点图侧重于展示点之间的分布规律，而气泡图将数值映射到气泡的大小上，增加了一个维度的数据展示。 散点图可以展示成千上万个点的数据，而气泡图为了防止气泡的互相遮挡，需要根据画布的大小控制数据的规模。 散点图示意图男性女性身高体重分布散点图，如下图所示： 地图定义地图是一种在地图分区上使用视觉符号（通常是颜色、阴影或者不同疏密的晕线）来表示一个范围值的分布情况的地图。在整个制图区域的若干个小的区划单元内（行政区划或者其他区划单位），根据各分区的数量（相对）指标进行分级，并用相应色级或不同疏密的晕线，反映各区现象的集中程度或发展水平的分布差别，最常见于选举和人口普查数据的可视化，这些数据以省、市登地理区域为单位。 地图的特点 一个颜色到另一个颜色混合渐变； 单一的色调渐变； 透明到不透明； 明到暗； 用一个完整的色谱变化。 地图应用场景 适合的场景 某年度国家各个省州的人口情况。 分级统计地图较多的是反映呈面状但属分散分布的现象，如反映人口密度、某农作物播种面积的比、人均收入等。 不适合的场景 2008 年美国总统大选结果。 民主党候选人奥巴马和共和党候选人麦凯恩胜出的州分别用蓝色和红色表示。这个例子的选举可视化很容易给用户造成简介中提到的错觉：数据分布和地理区域大小的不对称。共和党比民主党获得了更多的投票，因为红色的区域所占的面积更大。但是在美国总统大选中，最后的结果是看候选人获得的选举人票数，每个州拥有的选举人票数是不一样的，在一个州获胜的选举人将得到该州所有的选举人票数。纽约州虽然面积很小，却拥有33张选举人票，而蒙大拿州虽然面积很大，却只有3票。 地图示意图iphone销量地图，如下图所示： 雷达图定义雷达又叫戴布拉图、蜘蛛网图。传统的雷达图被认为是一种表现多维（4维以上）数据的图表。它将多个维度的数据量映射到坐标轴上，这些坐标轴起始于同一个圆心点，通常结束于圆周边缘，将同一组的点使用线连接起来就称为了雷达图。它可以将多维数据进行展示，但是点的相对位置和坐标轴之间的夹角是没有任何信息量的。在坐标轴设置恰当的情况下雷达图所围面积能表现出一些信息量。 每一个维度的数据都分别对应一个坐标轴，这些坐标轴具有相同的圆心，以相同的间距沿着径向排列，并且各个坐标轴的刻度相同。连接各个坐标轴的网格线通常只作为辅助元素。将各个坐标轴上的数据点用线连接起来就形成了一个多边形。坐标轴、点、线、多边形共同组成了雷达图。 雷达图的优缺点 优点：雷达图还可以展示出数据集中各个变量的权重高低情况，非常适用于展示性能数据。 缺点： 如果雷达图上多边形过多会使可读性下降，使整体图形过于混乱。特别是有颜色填充的多边形的情况，上层会遮挡覆盖下层多边形。 如果变量过多，也会造成可读性下降，因为一个变量对应一个坐标轴，这样会使坐标轴过于密集，使图表给人感觉很复杂。所以最佳实践就是尽可能控制变量的数量使雷达图保持简单清晰。 雷达图应用场景 世界经济论坛不久前还发布了全球竞争力指数报告，通过基本要求、效率增强器、创新与成熟因素等三个大方面对全球国家和地区进行竞争力评估。中国排名第28，得分4.89。通过雷达图，我们可以清晰看出中国在各个因素下的得分情况，进而进行分析。 常常表示由多个维度组成的能力衡量。比如展示了华为 Mate 和 中兴 Grand Memo 两款手机的综合表现雷达图，分别从易用性、功能、拍照、跑分、续航这五个维度进行考核，可以看出两款手机在这个维度方面的性能都比较平衡，同时也可逐项对比。虚构数据 雷达图示意图浏览器占比变化雷达图，如下图所示： 仪表盘定义仪表盘(Gauge)是一种拟物化的图表，刻度表示度量，指针表示维度，指针角度表示数值。仪表盘图表就像汽车的速度表一样，有一个圆形的表盘及相应的刻度，有一个指针指向当前数值。目前很多的管理报表或报告上都是用这种图表，以直观的表现出某个指标的进度或实际情况。 仪表盘的优缺点-优点： - 仪表盘的好处在于它能跟人们的常识结合，使大家马上能理解看什么、怎么看。拟物化的方式使图标变得更友好更人性化，正确使用可以提升用户体验。 - 仪表盘的圆形结构，可以更有效的利用空间。 仪表盘应用场景适合仪表盘的场景 时钟&amp;表 投资收益率 仪表盘示意图业务指标完成度仪表盘，如下图所示： 可视化图表用法可视化比较图可视化的方法显示值与值之间的不同和相似之处。 使用图形的长度、宽度、位置、面积、角度和颜色来比较数值的大小， 通常用于展示不同分类间的数值对比，不同时间点的数据对比。包括： 柱状图 气泡图 子弹图 色块图 漏斗图 直方图 K 线图 马赛克图 雷达图 玉玦图 螺旋图 层叠面积图 层叠柱状图 矩形树图 词云 可视化分布图可视化的方法显示频率，数据分散在一个区间或分组。 使用图形的位置、大小、颜色的渐变程度来表现数据的分布， 通常用于展示连续数据上数值的分布情况。包括： 箱形图 气泡图 色块图 等高线 分布曲线图 点描法地图 热力图 直方图 散点图 茎叶图 可视化流程图可视化的方法显示流程流转和流程流量。 一般流程都会呈现出多个环节，每个环节之间会有相应的流量关系，这类图形可以很好的表示这些关系。包括： 漏斗图 桑基图 可视化占比图可视化的方法显示同一维度上占比关系。包括： 环图 马赛克图 饼图 层叠面积图 层叠柱状图 矩形树图 可视化区间图可视化的方法显示同一维度上值的上限和下限之间的差异。 使用图形的大小和位置表示数值的上限和下限，通常用于表示数据在某一个分类（时间点）上的最大值和最小值。包括： 仪表盘 层叠面积图 可视化关联图可视化的方法显示数据之间相互关系。 使用图形的嵌套和位置表示数据之间的关系，通常用于表示数据之间的前后顺序、父子关系以及相关性。包括： 弧长链接图 和弦图 桑基图 矩形树图 韦恩图 可视化趋势图可视化的方法分析数据的变化趋势。 使用图形的位置表现出数据在连续区域上的分布，通常展示数据在连续区域上的大小变化的规律。包括： 面积图 K 线图 卡吉图 折线图 回归曲线图 层叠面积图 可视化时间图可视化的方法显示以时间为特定维度的数据。 使用图形的位置表现出数据在时间上的分布，通常用于表现数据在时间维度上的趋势和变化。包括： 面积图 K 线图 卡吉图 折线图 螺旋图 层叠面积图 可视化地理图可视化的方法显示地理区域上的数据。 使用地图作为背景，通过图形的位置来表现数据的地理位置， 通常来展示数据在不同地理区域上的分布情况。包括： 带气泡的地图 分级统计地图 点描法地图 参考文献 Echarts扩展学习：http://echarts.baidu.com/index.html AntV扩展学习：https://antv.alipay.com/index.html GitHub 图书：《机器学习实战》 图书：《自然语言处理理论与实战》 完整代码下载 源码请进【机器学习和自然语言QQ群：436303759】文件下载： 作者声明 本文版权归作者所有，旨在技术交流使用。未经作者同意禁止转载，转载后需在文章页面明显位置给出原文连接，否则相关责任自行承担。]]></content>
      <categories>
        <category>数学</category>
        <category>统计学</category>
      </categories>
      <tags>
        <tag>数学</tag>
        <tag>统计学</tag>
        <tag>可视化</tag>
        <tag>条形图</tag>
        <tag>柱形图</tag>
        <tag>折线图</tag>
        <tag>热力图</tag>
        <tag>饼图</tag>
        <tag>箱线图</tag>
        <tag>地图</tag>
        <tag>散点图</tag>
        <tag>雷达图</tag>
        <tag>仪盘表</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[编程数学之信息论]]></title>
    <url>%2F2018%2F10%2F10%2F%E7%BC%96%E7%A8%8B%E6%95%B0%E5%AD%A6%E4%B9%8B%E4%BF%A1%E6%81%AF%E8%AE%BA%2F</url>
    <content type="text"><![CDATA[摘要：信息论（英语：information theory）是应用数学、电机工程学和计算机科学的一个分支，涉及信息的量化、存储和通信等。信息论是由香农发展，用来找出信号处理与通信操作的基本限制，如数据压缩、可靠的存储和数据传输等。自创立以来，它已拓展应用到许多其他领域，包括统计推断、自然语言处理、密码学、神经生物学、进化论和分子编码的功能、生态学的模式选择、热物理、量子计算、语言学、剽窃检测、模式识别、异常检测和其他形式的数据分析。（本文原创，转载必须注明出处.） 信息熵熵是信息的一个关键度量，通常用一条消息中需要存储或传输一个符号的平均比特数来表示。熵衡量了预测随机变量的值时涉及到的不确定度的量。例如，指定掷硬币的结果（两个等可能的结果）比指定掷骰子的结果（六个等可能的结果）所提供的信息量更少（熵更少）。 1948年，香农引入信息熵，将其定义为离散随机事件的出现概率。一个系统越是有序，信息熵就越低；反之，一个系统越是混乱，信息熵就越高。所以说，信息熵可以被认为是系统有序化程度的一个度量。 信息度量信息熵如果一个随机变量X的可能取值为\( X={ x_1,x_2 ,…..,x_n }\) ，其概率分布为\( P\left( X=x_i \right) =p_i ,i=1,2,…..,n\) ，则随机变量X的熵定义为H(X)： 其中 x是定义在 X上的随机变量。信息熵是随机事件不确定性的度量。 例子：若S为一个三个面的骰子， P(面一)=1/5 P(面二)=2/5 P(面三)=2/5计算其信息熵为： 联合熵两个随机变量X和Y的联合分布可以形成联合熵，定义为联合自信息的数学期望，它是二维随机变量XY的不确定性的度量，用H(X,Y)表示： 条件熵在随机变量X发生的前提下，随机变量Y发生新带来的熵，定义为Y的条件熵，用H(Y|X)表示： 条件熵用来衡量在已知随机变量X的条件下，随机变量Y的不确定性。 由贝氏定理，我们有\( p(x,y)=p(y|x)p(x)\) ，代入联合熵的定义，可以分离出条件熵，于是得到联合熵与条件熵的关系式：\(H\left( Y|X \right) =H\left( X,Y\right) -H\left( X \right)\) 推导过程如下： 上式中： 第二行推到第三行的依据是边缘分布P(x)等于联合分布P(x,y)的和； 第三行推到第四行的依据是把公因子logP(x)乘进去，然后把x,y写在一起； 第四行推到第五行的依据是：因为两个sigma都有P(x,y)，故提取公因子P(x,y)放到外边，然后把里边的-（log P(x,y) log P(x)）写成log (P(x,y) / P(x) ) ； 第五行推到第六行的依据是：P(x,y) = P(x) * P(y|x)，故P(x,y) / P(x) = P(y|x)。 相对熵：信息增益相对熵又称互熵、交叉熵、KL散度、信息增益，是描述两个概率分布P和Q差异的一种方法，记为D(P||Q)。在信息论中，D(P||Q)表示当用概率分布Q来拟合真实分布P时，产生的信息损耗，其中P表示真实分布，Q表示P的拟合分布。 对于一个离散随机变量的两个概率分布P和Q来说，它们的相对熵定义为： 注意：D(P||Q) ≠ D(Q||P) 互信息两个随机变量X，Y的互信息定义为X，Y的联合分布和各自独立分布乘积的相对熵称为互信息，用I(X,Y)表示。互信息是信息论里一种有用的信息度量方式，它可以看成是一个随机变量中包含的关于另一个随机变量的信息量，或者说是一个随机变量由于已知另一个随机变量而减少的不肯定性。互信息是两个事件集合之间的相关性。 互信息、熵和条件熵之间存在以下关系：\( H\left( Y|X \right) =H\left( Y \right) -I\left( X,Y \right)\) 推导过程如下： 通过上面的计算过程发现有：H(Y|X) = H(Y) - I(X,Y)，又由前面条件熵的定义有：H(Y|X) = H(X,Y) - H(X)，于是有I(X,Y)= H(X) + H(Y) - H(X,Y)，此结论被多数文献作为互信息的定义。 最大熵最大熵原理是概率模型学习的一个准则，它认为：学习概率模型时，在所有可能的概率分布中，熵最大的模型是最好的模型。通常用约束条件来确定模型的集合，所以，最大熵模型原理也可以表述为：在满足约束条件的模型集合中选取熵最大的模型。 前面我们知道，若随机变量X的概率分布是\( P\left( x_i \right)\) ，则其熵定义如下： 熵满足下列不等式： 式中，|X|是X的取值个数，当且仅当X的分布是均匀分布时右边的等号成立。也就是说，当X服从均匀分布时，熵最大。 直观地看，最大熵原理认为：要选择概率模型，首先必须满足已有的事实，即约束条件；在没有更多信息的情况下，那些不确定的部分都是“等可能的”。最大熵原理通过熵的最大化来表示等可能性；“等可能”不易操作，而熵则是一个可优化的指标。 参考文献 Python官网 中文维基百科 GitHub 图书：《机器学习实战》 图书：《自然语言处理理论与实战》 完整代码下载 源码请进【机器学习和自然语言QQ群：436303759】文件下载： 作者声明 本文版权归作者所有，旨在技术交流使用。未经作者同意禁止转载，转载后需在文章页面明显位置给出原文连接，否则相关责任自行承担。]]></content>
      <categories>
        <category>数学</category>
        <category>信息论</category>
      </categories>
      <tags>
        <tag>数学</tag>
        <tag>信息论</tag>
        <tag>熵</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[编程数学之事件与概率]]></title>
    <url>%2F2018%2F10%2F10%2F%E7%BC%96%E7%A8%8B%E6%95%B0%E5%AD%A6%E4%B9%8B%E4%BA%8B%E4%BB%B6%E4%B8%8E%E6%A6%82%E7%8E%87%2F</url>
    <content type="text"><![CDATA[摘要：由于基于规则方法向基于统计方法的转型，概率就显得尤为重要，诸如一些随机事件、独立假设、条件概率、完全概率等等。然后对贝叶斯模型进行案例式介绍，旨在读者深度理解。（本文原创，转载必须注明出处.） 概率论介绍概率论概述概率论（英语：Probability theory）是集中研究概率及随机现象的数学分支，是研究随机性或不确定性等现象的数学。概率论主要研究对象为随机事件、随机变量以及随机过程。对于随机事件是不可能准确预测其结果的，然而对于一系列的独立随机事件——例如掷骰子、扔硬币、抽扑克牌以及轮盘等，会呈现出一定的、可以被用于研究及预测的规律，两个用来描述这些规律的最具代表性的数学结论分别是大数定律和中心极限定理。 概率的生活案例之六合彩买5, 17, 19, 24, 33, 49中奖概率高还是买1,2,3,4,5,6的中奖概率高? 古典概率论说：一样。但实际上机械或彩球制造上都有些微小的差异，所以每组概率不一定完全相同，但必须累积多期开奖结果后才看得出来。 概率的生活案例之生日悖论在一个足球场上有23个人（2×11个运动员和1个裁判员），不可思议的是，在这23人当中至少有两个人的生日是在同一天的概率要大于50％。 如果这23人都没有相同的生日也不违反概率，只是小于50％。 概率的生活案例之轮盘游戏在游戏中玩家可能认为，在连续出现多次红色后，出现黑色的概率会越来越大。这种判断也是错误的，即出现黑色的概率每次是相等的，因为球本身并没有“记忆”， 它不会意识到以前都发生了什么，其概率始终是 18/37。但轮盘的前后期开奖数字形成时间序列（可能存在自回归模型）。 概率的生活案例之赢取名车赢取电视节目里的名车：在参赛者面前有三扇关闭的门，其中只有一扇后面有名车，而其余的后面是山羊。 游戏规则是，参赛者先选取一扇门，但在他打开之前，主持人在其余两扇门中打开了一扇有山羊的门， 并询问参赛者是否改变主意选择另一扇门，以使赢得名车的概率变大。 正确的分析结果是，假如不管开始哪一扇门被选，主持人都打开其余两扇门中有山羊的那一扇并询问参赛者是否改变主意， 则改变主意会使赢得汽车的概率增加一倍；（“标准”的三门问题情况。） 假如主持人只在有名车那扇门被选中时劝诱参赛者打开其它门，则改变主意必输。(资讯不对称) 事件随机试验 随机试验的定义 我们将对自然现象的一次观察或进行一次科学试验称为试验。 随机试验的例子 举例1:硬币试验 E1: 抛一枚硬币，观察正(H)反(T) 面的情况。 E2: 将一枚硬币抛三次,观察正反面出现的情况。 E3: 将一枚硬币抛三次，观察出现正面的情况。 E4: 电话交换台一分钟内接到的呼唤次数。 E5: 在一批灯泡中任取一只, 测试它的寿命。 举例2:数学家去赌场 新闻：数学家3年赌赢156亿人民币，数学家在赌场里有什么优势？ 令19名数学家惊喜的是，虽然他们所掌握的那些高深数学知识在现实生活中似乎派不上多大用场，但竟然出人意料地在赌场上显现出了巨大的威力！据悉，19名数学家参与的大多是赛马、赛狗以及21点之类的赌博项目。而每次下注之前，他们会利用自己所精通的专业数学方法对各种中奖的概率进行推理演算，从而研究出某种“逢赌必赢”的秘笈！因为它的形态看起来合乎理想。在现实生活中，遇到测量之类的大量连续数据时，你“正常情况下”会期望看到这种形态。 随机事件和样本空间 基本事件或单位事件 定义：在一次随机试验中可能发生的不能再细分的结果被称为基本事件，或者称为单位事件，用\( E\) 表示。 样本空间 定义：在随机试验中可能发生的所有单位事件的集合称为事件空间，用\( S\) 来表示。 例如：在一次掷骰子的随机试验中，如果用获得的点数来表示单位事件，那么一共可能出现 6 个单位事件，则事件空间可以表示为\( S={1,2,3,4,5,6} \)。 上面的事件空间是由可数有限单位事件组成，事实上还存在着由可数无限以及不可数单位事件组成的事件空间，比如在一次获得正面朝上就停止的随机掷硬币试验中，其事件空间由可数无限单位事件组成，表示为： S={正，反正，反反正，反反反正，反反反反正，···}，注意到在这个例子中”反反反正”是单位事件。将两根筷子随意扔向桌面，其静止后所形成的交角假设为\(\alpha \)，这个随机试验的事件空间的组成可以表示为 \( S={\alpha |0^{\circ }\leq \alpha &lt;180^{\circ }}\) 随机事件 随机事件是事件空间\( S \)的子集，它由事件空间 \( S \)中的单位元素构成，用大写字母\( A,B,C\cdots \) 表示。例如在掷两个骰子的随机试验中，设随机事件 \( A \)= “获得的点数和大于10”，则 \( A \)可以由下面 3 个单位事件组成：\( A={(5,6),(6,5),(6,6)}}\)。 必然事件和不可能事件 如果在随机试验中事件空间中的所有可能的单位事件都发生，这个事件被称为 必然事件；相应的如果事件空间里不包含任何一个单位事件，则称为不可能事件。 事件的计算因为事件在一定程度上是以集合的含义定义的，因此可以把集合计算方法直接应用于事件的计算，也就是说，在计算过程中，可以把事件当作集合来对待，如图所示。 在轮盘游戏中假设 \( A \)代表事件“球落在红色区域”，\( B \)代表事件”球落在黑色区域”，因为事件 \( A \)和\( B \)没有共同的单位事件，因此可表示为\( A\cap B=\varnothing \)。注意到事件\( A \) 和 \( B \) 并不是互补的关系，因为在整个事件空间 \( S \)中还有一个单位事件“零”，其即不是红色也不是黑色，而是绿色。 概率古典概率古典概率又叫传统概率或拉普拉斯概率，古典概率的定义是由法国数学家拉普拉斯 ( Laplace ) 提出的。如果一个随机试验所包含的单位事件是有限的，且每个单位事件发生的可能性均相等，则这个随机试验叫做拉普拉斯试验。在拉普拉斯试验中，事件\( A \)在事件空间 \( S \)中的概率\( P(A)\) 为： P(A)=构成事件A的元素数目/构成事件空间S的所有元素数目例如，在一次同时掷一个硬币和一个骰子的随机试验中，假设事件 A 为获得国徽面且点数大于 4 ，那么事件 A 的概率应该有如下计算方法： S= { ( 国徽，1 点 )，( 数字，1 点 )，( 国徽，2 点 )，( 数字，2 点 )，( 国徽，3 点 )，( 数字，3 点 )，( 国徽，4 点 )，( 数字，4 点 )，( 国徽，5 点 )，( 数字，5 点 )，( 国徽，6 点 )，( 数字，6 点 ) }， A＝{( 国徽，5 点 )，( 国徽，6 点 )}，按照拉普拉斯定义， A 的概率为， P(A)=\frac {2}{12}=\frac {1}{6}注意到在拉普拉斯试验中存在着若干的疑问，在现实中是否存在着其单位事件的概率具有精确相同的概率值的试验? 因为我们不知道，硬币以及骰子是否完美，即骰子制造的是否均匀，其重心是否位于正中心，以及轮盘是否倾向于某一个数字。 尽管如此，传统概率在实践中被广泛应用于确定事件的概率值，其理论根据是： 如果没有足够的论据来证明一个事件的概率大于另一个事件的概率，那么可以认为这两个事件的概率值相等。 如果仔细观察这个定义会发现拉普拉斯用概率解释了概率，定义中用了相同的可能性 ( 原文是 également possible )一词，其实指的就是”相同的概率”。这个定义也并没有说出，到底什么是概率，以及如何用数字来确定概率。在现实生活中也有一系列问题，无论如何不能用传统概率定义来解释，比如，人寿保险公司无法确定一个 50 岁的人在下一年将死去的概率。 古典概率的两个特点 样本空间的元素只有有限个。 实验中每个基本事件发生的可能性相同。 统计概率：大数定律 继传统概率论之后，英国逻辑学家约翰·维恩和奥地利数学家理查德提出建立在频率理论基础上的统计概率。他们认为，获得一个事件的概率值的唯一方法是通过对该事件进行 100 次，1000 次或者甚至 10000 次的前后相互独立的 n 次随机试验，针对每次试验均记录下绝对频率值 \(h_n(A)\)和相对频率值\( f_n\)，随着试验次数 n 的增加，会出现如下事实，即相对频率值会趋于稳定，它在一个特定的值上下浮动，也即是说存在着一个极限值 P(A)，相对频率值趋向于这个极限值。这个极限值被称为统计概率，表示为： 例如，若想知道在一次掷骰子的随机试验中获得 6 点的概率值可以对其进行 3000 次前后独立的扔掷试验（如表4-1所示），在每一次试验后记录下出现 6 点的次数，然后通过计算相对频率值可以得到趋向于某一个数的统计概率值。 上面提到的这个有关相对频率的经验规律是大数定律在现实生活中的反映，大数定律是初等概率论的基础。统计概率在今天的实践中依然具有重要意义，特别是在初等概率论及数理统计等学科中。 概率公理 公理 1：事件 A 的概率 P(A) 是一个0与1之间（包含0与1）的非负实数。\( 0\leq P(A)\leq 1\ (A\in S) \) 公理 2： 事件空间的概率值为 1 。\( P(S)=1\) 公理 3： ， 互斥事件的加法法则。这里需注意：公理3可以推广到可数个互斥事件的联集。 定理 1 (互补法则)：与 A 互补事件的概率始终是\( P({\bar {A}})=1-P(A),\in S \) 定理 2：不可能事件的概率为零：\( P(\varnothing )=0\) 定理 3：如果若干事件 \( A{1},A{2},\cdots A_{n}\in S\)每两两之间是空集关系，那么这些所有事件集合的概率等于单个事件的概率的和。 注意针对这一定理有效性的决定因素是\( A_{1}\cdots A_{n}\) 事件不能同时发生。例如，在一次掷骰子中，得到 5 点或者 6 点的概率是： 定理 4：如果事件 A，B 是差集关系，则有，\( P(A\setminus B)=P(A)-P(A\cap B)\) 定理 5 (任意事件加法法则)：对于事件空间S 中的任意两个事件 A 和 B，有如下定理：\( P(A\cup B)=P(A)+P(B)-P(A\cap B)\) 例如，在由一共 32 张牌构成的斯卡特扑克牌中随机抽出一张，其或者是”方片”或者是” \(\mathcal {A}\) “的概率是多少？ 事件 A， B 是或者的关系，且可同时发生，就是说抽出的这张牌即可以是”方片”，又可以是” \(\mathcal {A}\)”， A ∩ B ( 既发生 A 又发生 B ) 的值是 1 / 32，因此有如下结果：\( P(A\cup B)={\frac {8}{32}}+{\frac {4}{32}}-{\frac {1}{32}}={\frac {11}{32}}\) 定理 6 (乘法法则)：事件\(A\) ，\(B\) 同时发生的概率是：\(P(A\cap B)=P(A)\cdot P(B\vert A)=P(B)\cdot P(A\vert B)\) 公式中的\( P(A|B) \)是指在 \(B\) 条件下 \(A\) 发生的概率，又称作条件概率。回到上面的斯卡特游戏中，在 32 张牌中随机抽出一张，即是方片又是 \(\mathcal {A}\)的概率是多少呢？现用 P(A) 代表抽出方片的概率，用P(B) 代表抽出 \(\mathcal {A}\)的概率，很明显， A， B 之间有一定联系，即 A 里包含有 B， B 里又包含有 A，在 A 的条件下发生 B 的概率是 P(B | A)=1/8，则有：\( P(A\cap B)=P(A)\cdot P(B\vert A)={\frac {8}{32}}\cdot {\frac {1}{8}}={\frac {1}{32}}\) 或者，\( P(A\cap B)=P(B)\cdot P(A\vert B)={\frac {4}{32}}\cdot {\frac {1}{4}}={\frac {1}{32}}\) 从上面的图中也可以看出，符合条件的只有一张牌，即方片 \(\mathcal {A}\) 。另一个例子，在 32 张斯卡特牌里连续抽两张 ( 第一次抽出的牌不放回去 )，连续得到两个 \(\mathcal {A}\)的概率是多少呢？ 设 A， B 分别为连续发生的这两次事件，我们看到， A， B 之间有一定联系，即 B 的概率由于 A 发生了变化，属于条件概率，按照公式有： 定理 7 (无关事件乘法法则)：两个不相关联的事件 A， B 同时发生的概率是： 注意到这个定理实际上是定理 6 (乘法法则) 的特殊情况，如果事件 A， B 没有联系，则有 P(A|B)=P(A)，以及 P(B|A)=P(B)。现在观察一下轮盘游戏中两次连续的旋转过程， P(A) 代表第一次出现红色的概率，P(B) 代表第二次出现红色的概率，可以看出， A 与 B 没有关联，利用上面提到的公式，连续两次出现红色的概率为： 条件概率和全概率条件概率 条件概率的描述 设试验E的样本空间为S, 事件包括A,B, 要考虑在A已经发生的条件下B发生的概率, 这就是条件概率问题。 条件概率的定义 设A，B是两个事件，且P(A)&gt;0，称：\( P(A|B)=\frac{P(AB)}{P(A)}\)（AB不独立） 设A，B是两个事件，且P(A)&gt;0，称：\( P(A|B)=P(A)\)（AB独立） 条件概率的性质 性质1：对于每一个事件B，有：\( 0 \leqslant P(B|A)\leqslant 1\) 性质2：\( P(S|A)=1\) 性质3. 设\( B_1,B_2,…,B_n\)两两互不相容，则 \( P(UB_i|A)=\sum P(B_i|A)\) 条件概率的计算方法 公式法： 先计算\( P(A)\) ，\( P(AB)\) ，然后按公式计算\( P(B|A)=P(AB)/P(A)\) 图解法：利用概率树求解 例如： 图圈饼店正在调查客户购买圈饼和咖啡的概率，下面是一些线索，画出概率树并求解相应概率（如图所示）。以下是已知条件： P(圈饼) = 3/4 P(咖啡|圈饼’) = 1/3 P(圈饼∩咖啡) = 9/20 计算过程： P(咖啡|圈饼) = P(圈饼∩咖啡) / P(圈饼) = 3/5 P(咖啡|圈饼) = P(圈饼∩咖啡) / P(圈饼) = 1/3 P(咖啡|圈饼) = P(圈饼∩咖啡) / P(圈饼) = 2/5 P(咖啡|圈饼) = P(圈饼∩咖啡) / P(圈饼) = 2/3 使用概率树求解问题的优缺点： 优点： 能够以图形体现条件概率，同时帮助计算概率，利用分支结构，条理清楚，不易算错。 不足： 画概率树很浪费时间。 完全概率 概念介绍 n 个事件\( H_1,H_2,…H_n\) 互相间独立，且共同组成整个事件空间 S，即\( H_i\cap H_j=\varnothing , (i\neq j)\)以及\( H_1\cup H_2\cup …\cup H_n=S\),这时 A 的概率可以表示为， 举例解析 例如，一个随机试验工具由一个骰子和一个柜子中的三个抽屉组成，抽屉 1 里有 14 个白球和 6 个黑球，抽屉 2 里有 2 个白球和 8 个黑球，抽屉 3 里有 3 个白球和 7 个黑球，试验规则是首先掷骰子，如果获得小于 4 点，则抽屉 1 被选择，如果获得 4 点或者 5 点，则抽屉 2 被选择，其他情况选择抽屉 3 。然后在选择的抽屉里随机抽出一个球，最后抽出的这个球是白球的概率是： P(白)=P(白|抽1)·P(抽1)+P(白|抽2)·P(抽2)＋P(白|抽3)·P(抽3) \\\ =(14/20)·(3/6)+(2/10)·(2/6)+(3/10)·(1/6) \\\ =28/60=0.4667 \\\从例子中可看出，完全概率特别适合于分析具有多层结构的随机试验的情况。 贝叶斯定理贝叶斯公式贝叶斯定理由英国数学家托马斯·贝叶斯 ( Thomas Bayes 1702-1761 ) 发展，用来描述两个条件概率之间的关系，比如 P(A|B) 和 P(B|A)。按照定理 6 的乘法法则，P(A∩B)=P(A)·P(B|A)=P(B)·P(A|B)，可以立刻导出贝叶斯定理： 案例1：狗叫抓贼例如：一座别墅在过去的 20 年里一共发生过 2 次被盗，别墅的主人有一条狗，狗平均每周晚上叫 3 次，在盗贼入侵时狗叫的概率被估计为 0.9，问题是：在狗叫的时候发生入侵的概率是多少？ 我们假设 A 事件为狗在晚上叫， B 为盗贼入侵，则\( P(A)=3/7,P(B)=2/(20·365.25)=2/7305,P(A | B) = 0.9\)按照公式很容易得出结果：\( P(B\vert A)=0.9\cdot {\frac {2}{7305}}\cdot {\frac {7}{3}}=0.0005749486653…\) 案例2：追踪红球现分别有 A， B 两个容器，在容器 A 里分别有 7 个红球和 3 个白球，在容器 B 里有 1 个红球和 9 个白球，现已知从这两个容器里任意抽出了一个球，且是红球，问这个红球是来自容器 A 的概率是多少? 假设已经抽出红球为事件 B，从容器 A 里抽出球为事件 A，则有：\( P(B) = 8 / 20, P(A) = 1 / 2, P(B|A) = 7 / 10\)按照公式，则有：\( P(A\vert B)={\frac {7}{10}}\cdot {\frac {1}{2}}\cdot {\frac {20}{8}}={\frac {7}{8}}\) 参考文献 Python官网 中文维基百科 GitHub 图书：《机器学习实战》 图书：《自然语言处理理论与实战》 完整代码下载 源码请进【机器学习和自然语言QQ群：436303759】文件下载： 作者声明 本文版权归作者所有，旨在技术交流使用。未经作者同意禁止转载，转载后需在文章页面明显位置给出原文连接，否则相关责任自行承担。]]></content>
      <categories>
        <category>数学</category>
        <category>概率论</category>
      </categories>
      <tags>
        <tag>事件</tag>
        <tag>概率</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[语言学中的语音词汇和语法问题]]></title>
    <url>%2F2018%2F10%2F10%2F%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%B8%AD%E7%9A%84%E8%AF%AD%E8%A8%80%E5%AD%A6%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[摘要：语音、词汇、语法三个角度对现代汉语进行了一个简单概要的勾勒，在以往传统的语言学教材中一般还有“文字”、“修辞”两节内容。需要注意的是，语言学本身是一门十分庞杂的学科，知识体系与研究方法或因语言不同而有区别，或因派别主义不同而有区别。但无论是何种语言，亦或是何门何派，在进行自然语言处理时我们要面临的永远是一个个真实的语料和具体的语言现象。理论是用来指导实践，拓宽我们研究思路的，究竟最后采用何种理论，这只是一个“白猫黑猫”的问题。（本文原创，转载必须注明出处.） 什么是语音语音是通过人类发音器官发出的、有意义内容并用于社会交际的声音。它既是语言的物质外壳，又是语义的重要载体。因此，自然界中的虫鸣鸟叫不是语音，甚至我们日常发出的诸如咳嗽、哭笑、打鼾、轻哼等声响，由于其本身并不能传达确定的语义而应用于交际，也不能算作是真正的语音。有一个简单的判断标准，在普通话中有字可以表示的音就是语音。“他呼呼大睡”中的“呼呼”模拟了“他”睡觉时发出的声音，是语音（注意：“呼呼”并不就是人类睡觉时实际发出的声音，它是抽象的、符号化后的拟声词）。另外，孩童们在玩闹时用手作枪模拟子弹发射的[biubiu]声不是语音，虽然它的表意在这里似乎很明确，但缺乏字形的现状表明，这个音目前还未被大众所认可，能用于社会交际。 语音的三大属性 物理属性 语音的本质是音波，是发声体振动而产生。因此语音同自然界其他声音一样拥有音高、音强、音长、音色四种要素。 音高 指的是声音的高低，它是由发声体振动的频率（单位Hz）决定的。单位时间内发声体振动的次数越多，其声音就越高，反之亦然。一般说来，大的、粗的、厚的、长的、松的物体振动慢；小的、细的、薄的、短的、紧的物体振动快。反映到人声上，一般来说成年男性声带长且厚，所以音高低；而儿童与妇女之所以常被认为“嗓子尖”则是缘于他们的声带常短而薄。音高的区别，在汉语里就构成了声调与语调的区别。 音强 指的是声音的强弱，它是由发声体的振幅（单位dB）决定的。发声体振动时摆动幅度越大，其声音就越强，反之亦然。反映到人声上，一般来说用力越大，肺部呼出气流越强，声带振动越剧烈，声音就越响亮；反之如窃窃私语时，音强就较弱。音强的区别，在汉语里就构成了轻重音的区别。 音长 指的是声音的长短，它是由发声体的振动时长决定的。发声体振动时间越长，其声音就越长，反之亦然。音强的区别，在汉语普通话里虽未用来区别意义，但在一些外语和方言中其区别词义的作用却较为明显。如：英语：[][]粤语：[ban]奔[baan]班。 音色（音质、音品） 指的是声音的特色，它是由发声体振动时产生的不同音波波纹的曲折形式决定的。它的影响因素有发音体材质、发音方法以及共鸣器的形状。反映到人声上，千人千语就是音色影响声音的最好例子。 以上物理四要素在自然语音中是难以分割的，它们在不同语言或方言中分别起着不同的区别作用。其中，音色是任何语言中最基础也是最重要的区别因素，如“足球”与“篮球”两个词的第一个字“足”[zú]与“篮”[lán]就是音色截然不同的两个语音。另外在汉语普通话中，音高带来的所谓“阴平、阳平、上声、去声”四调与音强所带来的轻重音（如“东西”的两个音[dōng xī]和[dōng xi]）也起着十分重要的区别作用。 生理属性 语音是由人的发音器官发出的，后者根据参与发音过程的先后顺序可分为三个部分：肺、气管、胸腔、横膈膜等呼吸器官，它们主要通过呼出气流为发音提供原动力；喉头、声带等发声器官，粘附在喉头的软骨上由两片薄膜构成的声带在气流冲击下发生振动，形成音波；咽腔、鼻腔、口腔处的调音器官，它们将把从声带传出的音波进行加工，从而产生我们人耳听到的各种语音。 社会属性 语言是一套符号系统，语音作为其重要组成部分也是如此。语音的社会性质主要有以下三个方面： 语音与相应语义之间的关系是非必然的，约定俗成的。汉语普通话中的[dà lù]可以用来指代陆地，也可以用来指代宽广的道路。不同语言间的音义规定差别则更为明显，同样是苹果，英国人就会说成是“apple”。 语音具有地域性、民族性。上面说到音义之间的对应关系是人为规定的，但是这种规定如果不能得到一个区域内特定群体的广泛认同，人与人之间就无法进行有效的交流。如果进一步放任音义关系的随意扩展，那么就有可能产生《圣经》中“巴别塔”般的悲剧。为了避免上述现象的发生，不同民族和地区的人们在地理文化的差异下便产生了成百上千种各具特色的语言。 语音具有系统性。这就意味着不同语言或方言拥有着不同的语音元素以及不同的内部关系。同样是说“牛奶”和“男人”这两个词，北京人和四川人就会有两种不同的发音结果（四川方言存在[n][l]不分的情况）。 语音单位 音素 音素是音色角度划分而出的最小语音单位，分为元音、辅音两大类。例如，“看”[kàn]就可以划分出“k、a、n”三个各有特色的音素。元音音素发音时气流振动声带，在口腔、咽腔等处不受阻碍，辅音相反。元音又叫母音，在现代汉语普通话中共有10个，分别是7个舌面元音：α[Α]、o[]、e[ɣ]、ê[ε]、i[i]、u[u]、ü[y]；2个舌尖元音：-i[]、-i[]；以及一个卷舌元音：er[]。辅音又叫子音，在现代汉语普通话中共有22个：b[]p[]m[]f[]d[]t[]n[]l[]g[]k[]h[]j[]q[]x[]zh[]ch[]sh[]r[]z[]c[]s[]ng[],其中前21个都可以作为声母，而ng[]只可做韵尾。 音节 音节是音素构成的，在交谈时自然感到的语音单位。一般情况下一个汉字就表示着一个音节，儿化音节诸如“花儿”[huār]等则是用两个汉字代表一个音节的特例。 声母、韵母、声调 声母 普通话中共分为21个，是主要由辅音构成的音节前段。例如，在“表”[biǎo]这个音节里，辅音b就是它的声母。特别注意，有些音节不以辅音开头，我们习惯上将元音前头那部分看做是零，叫做“零声母”。例如“乌”[wū]开头没有辅音，就算是零声母音节。而ū前的w在拼音书写时既是一种隔音符号，也提示了ū前面实际发音中半元音[w]的存在。因此，“零声母”未必就是零，但其不以辅音开头的特点却是肯定的。 韵母 普通话中共分为39个，是由元音或元音加辅音构成的音节后段，可分为韵头、韵腹、韵尾三个轻重长短不一的部分。韵头只有i、u、ü三个；韵腹是韵母的主干，是其必不可少的部分且声音在三部分中最为清晰响亮；韵尾只能由元音i、u和鼻辅音n、ng四个充当。来看一例，在“壮”[zhuàng]这个音节里，辅音“zh”是它的声母，“u”是它的韵头，“a”是它最为响亮的韵腹，“ng”是它的后鼻韵尾。另外请注意，并不是所有音节都有声母、韵头、韵尾，如上文所举“乌”[wū]的例子，它就只有韵腹u。 声调 贯穿于一个音节、具有区别意义作用的音高变化。调类是声调的种类，普通话中共有四种：阴平、阳平、上声和去声，它们的调值分别为55、35、214、51（此处采用赵元任的“五度标记法”，见下图）。 音位 音位是一个具体的语言系统中能够区别意义的最小语音单位，也就是按语音的辨义作用归纳出的音类。在具体的某种语言或方言里人们可以发出的音素很多，但音位的数量却是有限的。例如“妈”[m]与“怕”[p]的声母[m][p]就是两个音位，它们区别意义；而如“巴”[b]“班”[bn]“帮”[bng]中的α实际上分别读作[A]、[a]、[α]（称作“音位变体”），是三个不同的元音，只不过因为三者的音色不会因读错而导致人们的误解，因此我们常把它们归纳到一个音位里去。 记音符号历史上汉语的记音方法很多，诸如反切法、“注音符号”（至今台湾等部分地区还在使用）标记等。现在国内通行的是1958年审议通过的《汉语拼音方案》，相信绝大多数读者们在小学语文课堂上就学习了相应的知识，在此就不再赘述。另外需要重点介绍的，也是目前国际上通用的记音符号就是国际音标（International Phonetic Alphabet，简称IPA）。它采用一符一音原则，大部分符号采用拉丁小写字母及其变体（如倒写、反写、合写、添加附加符号等）。现特将国际音标简表、三种记音符号对照表罗列如下，至于国际音标里部分普通话中没有的符号发音敬请有兴趣的读者自行上网检索，这方面的语音资料网上很多，尤以赵元任和瞿霭堂两位先生的发音最为经典，可供参阅。 共时语流音变 变调 轻声 四声在一定条件下念得比原调短而弱的特殊音变现象。具有区别词义、区别词性等作用，如曾举过的“东西”例（见P），又如“对头”[][]。 上声的变调 两个上声相连，前一个调值从214变成35（上上相连变阳平），例如“蒙古”[]；上声后接其余三调或轻声，这个上声变为半上21，例如“好的”[]；三个及以上上声音节相连的变读还请读者自行尝试总结规律，当然亦可参阅相应的现代汉语教材，尤以黄伯荣、廖序东主编的《现代汉语》（上下册）最为经典。 “一、不的变调” “一、不”在单念、词句末以及“一”用在序数中时读作原调；去声前二者一律变为上声35，例如“一个”“不去”；在非去声前，“一、不”皆读去声51，例如“一天”“不开心”；“一、不”嵌在相同的动词、形容词间皆读作轻声，例如“瞅一瞅”“行不行”；“不”处于可能补语中变读轻声bu，例如“起不来”。 “七、八的变调” “七、八”在去声前可变做阳平35，亦可不变。例如“七岁”“八路”。 其他变调 除上述三类外，还有一些变调可供有兴趣的读者下来自行总结，此处举上几例，如“远远儿的”（“AA儿的”式，第二个“远”常独阴平55）、“乱蓬蓬”（“ABB”式，“蓬蓬”常独阴平55）。 儿化 一个音节中韵母带上卷舌音色的特殊音变现象。与轻声一样，儿化也具有区别词义、区别词性的作用，如“头”“头儿”，“盖”“盖儿”；此外，儿化后的字词常带有细小、轻松、喜爱、亲切的感情色彩，如“小猫儿”“脸蛋儿”。（三）“啊”的音变 语气词“啊”受前字末尾音素的影响往往产生音变，共有6种，举几例如下：“吃呀喝呀”“困难哪”“还爬楼哇”（此处记“啊”音变后的字）。此外几种读者可自行参阅相关教材。 词汇什么是词汇词汇又叫语汇，是某种特定语言里所有词和固定短语的总和。 词汇单位 语素 构词的单位，语言中最小的音义结合体。例如“笔”就是一个语素，它的语音形式是[sh]，它的词汇意义是“写字、画图的工具”语法意义是“名词、量词”及相关的语法作用。 从音节角度看，语素有单复音节之分。现代汉语中绝大部分是由一个音节构成的语素，如“天、地、人、山、海、的、啥、啊”等；而复音节语素是指由两个及其以上音节构成的语素，如“蝴蝶、鸳鸯、玻璃、抠门、布宜诺斯艾利斯”等；另外还有小于一个音节的特殊语素，如“鸟儿”的“儿”字因是儿化标记，只算做半个语素。 有一个简单的判别语素的方法——“替代法”，即用一个新语素替代尚未确定为语素的语言单位，其间还要注意替代前后另外一个未被替代的语素，其意义不能发生变化。举上文“抠门”例，该词虽可被替换为“铁门”“抠鼻子”等，然而“抠门”合在一起表示的“小气、不大方”义，并非拆开后各自的“用手指或细小的东西挖”“建筑物的出入口”等义简单组合就可得到。对于复音节语素，其语义常常是确定的，其组成元素也往往是不容随意更改的。从表意虚实看，语素还分为实语素和虚语素。前者有具体的词汇意义，如“天、人、喜”等；后者则只表示抽象的语法意义，如“吗、的、老（公）”等。从构词能力看，还有成词语素与不成词语素之分。前者又叫自由语素，指既能单独成词又可以与其他语素组合成词的语素，如“马跑了。”这句话中的成词语素“马”既可以单用，也可以组合成“白马、骏马、老马”等入句。而不成词语素则是指不能独立成词，必须和别的语素组合在一起的语素，也称黏着语素。不成词语素又可以分为两类，一类位置自由、承担了所成词部分乃至全部的基本意义，如“卉、健、民、丽”等，它与成词语素合在一起被称作词根，是词义的承担者；另一类位置固定只表示一些附加语义，被称作词缀，依所在位置被进一步分为前缀、中缀、后缀，如“者、第、化（白热化）、里（稀里哗啦）”等。 词 词由语素构成，是语言中最小的、能够独立运用的音义结合体。例如“打水”可被拓展成“打热水”因此是短语，不是最小的词。再如上文的不成词语素“卉”，生活中我们从不单说这个字，相反语义相近的“花”就可以，因此也不能算作能够独立运用的词。 固定短语 词语之间的固定搭配，一般不能随意增减或改换其中的元素；与之相对应的是自由短语，它可以依据表达的需要临时组合词语。我们常说的“短语”其实是后一种自由短语，它十分能产，例如“看报、看电视、看球赛”等。固定短语又可分为专名与熟语两类。前者以企事业单位名居多，如“四川大学”、“全国计算语言学学术会议”、“国营长虹机器厂”等；后者包括成语（“七上八下、东倒西歪”等）、惯用语（“炒鱿鱼、穿小鞋”等）、歇后语（“哑巴吃黄连──有苦说不出”等）。 缩略语 语言中被压缩或省略的词语，主要有简称、数词略语两种形式。前者如“彩色电视机──彩电”、“工人联合会──工会”、“马萨诸塞州──麻省” 等；后者如“三个代表”、“三好学生”、“五讲四美”等。 词的构造 单纯词:只由一个语素构成的词。最简单的单纯词主要是单音节词，如“花、山、人、好、吗”等，另有一部分是多音节的，主要有以下几类。 联绵词 由两个不同音节连缀而成的，不能拆分为两个语素解释其义的单纯词，又具体分为三种。 (1) 双声 指两个音节声母相同的联绵词，如“琵琶、参差、蹊跷、仓促”等。 (2) 叠韵 指两个音节韵母（或只是韵腹韵尾）相同的联绵词，如“骆驼、琢磨、叮咛”等。 (3) 其他 指两个音节声韵母皆不同的联绵词，如“鸳鸯、峥嵘、马虎”等。 叠音词:指两个相同音节重叠构成的词，其中任何一个音节都只有音而没有义（或没有组合后的新义），它们只有合在一起构成一个复音节语素才有意义。如“孜孜、娘娘（单说表母亲，合说后意义发生变化）、草草、狒狒”等。 拟声词:指用来模拟声音的词。如“叮叮当、扑哧、叽里咕噜”等。 音译外来词 指对音翻译外民族语言而得的词，其中任何一个音节都只表音而不表原义。如“海参崴、麦克风、朱古力”等。 合成词 由两个及以上语素构成的词，主要由以下几类。 复合词:指由两个及以上不同词根构成的合成词，从词根间关系看，又可细分为五类： (1) 联合式 也称并列式，词根语素之间地位平等，意义相同、相近、相关或相反，例如“建造、车马、迟早、忘记”等。其中“忘记”一词，实际承担语义的只有“忘”这一词根语素，另一词根并不表意，我们把这类词也称作“偏义词”。 (2) 偏正式 前一词根修饰、限制后一词根，例如“莲子、南瓜、嫩绿、风行”等。其中前两个例子中心词根是名词性语素，也称为定中式；后两个例子中心词根是动词、形容词性语素，也称状中式。 (3) 述宾式 前一词根表示动作行为，后一词根表示前者所支配关涉的对象，例如“司机、主席、化石”等。 (4) 主谓式 前一词根表示被陈述主体，后一词根陈述前一词根，也称“陈述式”。例如“月食、自强、胆小”等。 (5) 补充式 后一词根补充说明前一词根的动作结果、事物单位等，例如“打倒、课本、改正”等。 此外还有介宾式（如“从前，当天”等）、连动式（如“走访、病故”等）、兼语式（如“讨（人）厌，请（人）教”）等，这三者因数量较少，在有的论著中前者被归入述宾式，而后二者则被归入联合式中。 重叠词 由相同词根语素重叠构成的合成词，例如“妹妹、弟弟、常常”等。 附加词 由词根词缀构成的合成词，“前缀+词根”的叫前加式，“词根+后缀”的叫后加式，另有加中缀的情况。例如“老虎、老公、石头、馒头、稀里糊涂、古里古怪”等。 另外，“词汇单位”一节所讲之“缩略语”中有部分词汇诸如“北大、社科”等因组成元素间关系密切，社会认可度高、使用广泛，有时我们也将其看成是一种特殊的合成词。现代汉语中双语素合成词是占绝大多数的，当然还有更多数量的语素构成的词，篇幅有限难以展开介绍，但还请读者留意在分析这些词时一定要留意它们内部的多层性。如“老虎机”中外层关系“老虎”修饰“机”，属偏正式；内层关系“老”又做“虎”的前缀，属附加式。 词义及其分类 什么是词义 词义是词的意义，广义上包括词汇意义和语法意，本章中所讲“词义”通常指词汇意义。 词义的分类 理性义 也称概念义，是词义中反应事物概念的意义部分。例如“货币：政府法律规定强制使用,可充当交易的媒介、价值的标准、记帐的单位及延期支付的工具”、“飞船：运送东西的飞行器”等。一般来说，词典里对各条词目的解释就是该词的理性义。另外，理性义还可以分为通用义与专门义。同样是“水”，生活中人们只会把它解释为“一种无色、无臭、透明的液体”，而在化学实验室里“水”就变成了“氢、氧两种元素组成的无机物”。 色彩义 也称附属义，是附着在理性义之上旨在表达评价、形象等内容的词义部分。可细分为以下几类。 (1) 感情色彩 有褒义、贬义、中性词之分，例如“英雄、伟人、汉奸、抠门、大海、学习”等。 (2) 语体色彩 有书面语和口语之分，例如“进食──吃饭，交谈──唠嗑”等。 (3) 形象色彩 能使人产生关于事物形态、颜色、声音等具体形象联想的词义部分。例如“蝴蝶兰、迎客松、鹅黄、啦啦队”等。 (4) 文化义 一些词出自经典、诗文、民间传说等，本身就具有丰富的历史文化内涵，引人联想。例如“松龟──长寿，莲花──洁净，乌鸦──噩讯”等。 义项与义素 义项 词的理性意义的分项说明。例如“经过”就有两个义项：一是“通过”，二是“经历的过程”。一个词若同时拥有几个义项，那其中必有一个义项是最为基本、常用的，我们把它称作基本义；其他的一些义项一般都是由它直接或间接转变而来的，我们称之为转义。根据转义产生的方式，又可以细分为两种：引申义与比喻义。前者是基本义经过推演而形成的意义，如“锤”基本义是“ 敲打物件的器具”，后来也引申为“用锤敲打”这一动作；后者是基本义被用来比喻另一事物后逐渐固定下来的新意，如“手足”原指手脚，后也用它来比喻“兄弟”。 根据词义项的多少我们可以将其分为单义词和多义词。值得注意的是，日常生活中我们很容易把多义词与一些特殊的同音词混淆在一起。多义词顾名思义就是有两个及以上义项的词，而同音词则是语音相同而意义毫无联系的一组词。试看一对例子：“广”是一个多义词，既可以在“宽广”中指“ 面积、范围宽阔”；又能在“广开言路”中作动词，表“扩大、扩充”义。而“花”则是由一组同音词构成的，一组以“花朵”为基本义，另一组如“花钱、花销”等，则是词义与前者毫不相干的另一组词。 义素 和语音学部分所讲的“音素”、“音位”类似，词义部分也有义素与义位两组概念。前者是最小的不能独立运用的意义单位，也称词的语义特征；后者则是最小的能独立运用的意义单位，略等于之前所讲的义项。有了这样一组概念，接下来我们就可以使用一种名为“义素分析法”的手段来深入分析词的内部意义构成，举例如下。 父亲──[+男性+长辈+血亲] 伯父──[+男性+长辈-血亲] 妹妹──[+女性 -长辈+血亲] 姨妈──[+女性+长辈-血亲] 要注意，只有相关的词才可以进行义素分析，反之如“大海”与“狗”，因二者词义相差过大而很难总结出较为短小的几条区别特征。另外义素的选取也尤为重要，要仔细分析出一组可以区别目标词的特征，然后用“[]”将其标注出来，在前面用“+”“-”号进行分析。 语义场物理学上把物理量在空间中某个特定区域内的分布称为场，有温度场、引力场、电场、磁场等。语义学中也引入了“场”的概念，从而形成了语义场，它是根据词义的共同特点或关系划分出来的类。同场词有着共同的语义区别特征，同时其各个义素前正负号的差异又将它们在同一个语义场中做出区别。 例如： 词 共同义素 区别义素 运动项目 陆上项目 评分 计时 游泳 + - - + 跳水 + - + - 体操 + + + - 跨栏 + + - + 有些读者在阅读到上文“只有相关的词才可以进行义素分析”时可能会有些疑惑，不知具体什么样的联系才可以叫作“相关”。在语义场概念引入后，我们就可以对这个问题作出一定程度上的解答——只要词在同一语义场内，就可以认为其具有相关性，从而进行义素分析。那么语义场到底有哪些呢？根据场中各成员间的关系不同，语义场可以粗分为以下几类。 (1)类属义场 场内成员同属于一个更高层的类，如“诗歌──散文──小说”都属于文学体裁类，其中“诗歌”、“散文”等我们称其为下位词，“文学体裁”我们称其为上位词。上下位的概念是相对的、不断转换的，如“诗歌”就可以进一步细分为“古诗、近体诗、现代诗歌”等。 (2)顺序义场 场内成员之间存在着某种顺序排列，如“状元──榜眼──探花”、“立春──雨水──惊蛰”等。 (3)同义义场 场内成员意义相同或相近，如“高兴──开心──喜悦”、“集合──汇合──聚集”等。 (4)反义义场 与同义义场相反，场内成员意义相反或相对，如“高贵──卑贱”、“干净──邋遢”等。 (5)其他关系义场 除上述义场外，仍有部分语义关系无法得以充分归纳，所以将其单列一项，一齐算入“其他关系义场”。例如：“父母──子女”、“前边──后边”等。 词汇的构成现代汉语词汇分为基本词汇与一般词汇两类。前者具有稳固性、能产性、全民通用性等特征，是机器翻译训练集的理想词源；后者包括古语词、方言词、外来词、行业语、熟语以及新兴的网络用语等，这些词数量多、适用范围窄、更新速度快，是专语语料库的主要面向对象，但并不适合作为给机器自动学习的词汇语法教科书。 语法什么是语法 语法 语法是语言三要素之一（另有语音、语义），专指一种语言中语素、词、短语和句子等有意义的语言单位由小到大组合所依据的规则。在术语层面，“语法”除了指代上述的语法规律外，还可以指代研究这些语法规律及其系统的科学——语法学。与语音和词汇不同的是，语法一般比较抽象（抽象性）、变化相对缓慢（稳固性）、因地区民族不同而存在着明显差异（民族地域性）。语法单位主要有四级，从小到大依次是：语素、词、短语、句子。前两级我们已在之前的篇章中有所涉及，它们在语法中的具体组合规则以及新出现的后两级我们将在在随后的几章中一一予以系统介绍。 句法成分 即句法结构的组成成分，根据成分间存在的陈述、支配等关系可细分为五组：主语和谓语、述语和宾语、定语和中心语、状语和中心语、中心语和补语。主语是被陈述对象；而谓语则是用来陈述主语的，二者是陈述关系。例如：“她的新发型非常漂亮”，大主语是“她的新发型”，大谓语是“非常漂亮”。述语又叫动语，表示发生的动作行为；而宾语表客观事物，二者之间是支配、涉及关系。例如：“小明吃完了一大碗饭”，大动语是“吃完了”，大宾语是“一大碗饭”。修饰语位于中心语之前，用来描写或限制中心语。根据后接中心语的性质又分为定语和状语两种。定语修饰名词性短语里的中心语，而状语则修饰谓词（动词、形容词）性短语里的中心语。在“她的新发型非常漂亮”中，“她的”和“新”作定语修饰中心语“发型”；“非常”作状语修饰中心语“漂亮”。补语是跟在谓词性短语里的中心语后面的补充成分，起补充说明的作用。例如：“今天超市里的苹果便宜得很”中“很”就作补语来补充说明中心语“便宜”的程度。此外还有一种独立于八大配对成分之外的特殊句子成分——独立语。它不与句中其他成分产生结构关系，只是出于语用或表达的需要在句中起特定的表意功能。可分为插入语、称呼语、感叹语、拟声语四类，例如：事情已然如此，你说，我还能怎么办？“、老王，你去哪儿？”、“啊，我马上就来。”、“咚咚咚，响起了敲门声。”。 词类词类是词在语法性质方面划分出的类别，主要依据三个标准：语法功能、形态和意义。其中尤以语法功能标准最为重要，它主要指词的组合能力以及充当句法成分的能力。在三条标准的指导下，词可先粗分为两类：实词与虚词。前者可以单独充当句法成分，意义较为具体；而后者不能充当句法成分，只能伴随实词发挥语法意义。二者还可进一步细分，具体如下。 实词 名词 表示人、事、物、时、地等的名称，又分： (1) 专有名词 巴金、美国、社会主义。 (2) 普通名词 母亲（个体名词）、大众（集合名词）、经济（抽象名词）、烟花（物质名词。 ① 时间名词 盛夏、清晨、曾经、北魏。 ② 处所名词 周围、城郊、厨房（欧洲、美国、成都等地名既是专有名词又是处所名词。 ③ 方位名词 上、左、东、后面、以上。 语法特征 ：常作主语、宾语和定语，一般不作状语（时间名词例外）；一般可被数量短语修饰，却不能加副词“不”；大多能跟在介词后头构成介词短语；一般不能重叠；单复数同形。 动词 表示动作行为、心理活动以及存现等，又分： (1) 动作动词 听、说、读、写、打击、革新。 (2) 心理动词 爱、恨、害怕、焦虑、希冀。 (3) 存（变）现动词 有、在、灭亡、变化、发展。 (4) 使令动词 叫、让、给、请、令、要求。 (5) 判断动词 是、称、叫、等于。 (6) 能愿动词 能、会、敢、要、可以、应该。 (7) 趋向动词 去、来、上、过、出、进去、起开。 (8) 形式动词 进行、给予、加以。 语法特征：大多可作谓语（中心语）、动语；能被否定副词“不、没、没有”修饰；除少数心理动词和部分能愿动词外，一般前面不能加程度副词；多数可以后接“着、了、过”；有些动作动词可以重叠，且表意发生一定变化（表短促、尝试、轻松等义）。 形容词 表示事物的形状、性质和状态等，又分： (1) 性质形容词 好、英勇、聪慧、大、高、瘦弱。 (2) 状态形容词 冷清、乌黑、干巴巴、滑不溜秋。 语法特征：常作定语和谓语（中心语）；不带宾语（部分兼类词除外，如“严肃纪律”中的“严肃”既可作形容词又可作动词）；多数可被程度副词“很、太”等修饰；部分可重叠，且重叠后意义常趋向加深或适中，不能再被“很”修饰；单音形容词可附加叠音词缀或其他词缀（如“硬邦邦”，亦不能再加“很”修饰）。 区别词 一种特殊的形容词（非谓形容词），表示事物的区别性特征，往往成对出现。例如：“男：女、土：洋、国营：民营、小号：中号：大号”等。 语法特征：单用只能作定语，多数可以扩充为“的”字短语（如“假的、坏的”等）；组成联合短语（如“的”字短语）后可充当主谓宾（如“我要中杯”）；单用时否定需用“非”而不能用“不”。 数词 分基数词和序数词以分别表示数目和次序。前者包括“零”至“九”共10个系数词以及“十”到“兆”共6个位数词；后者一般则是在基数前加“第、初”等词构成，当然也有用天干地支、拉丁字母等表次序的情况。 语法特征：常需和量词组合成数量短语入句，充当定状补；“俩”、“仨”可以看作是“两个”、“三个”的合体词，后不能再加“个”，其意义与功能和数量短语同。 量词 又称“单位词”，表示计算单位，又分： (1) 名量词 用以计算人或事物的数量，有个体量词（“根、条、把”等）、集体量词（“群、批、副”等）、度量词（“寸、升、吨”等）之分。这三种也叫“专用名量词”，另有从名词、动词处借用来的名量词，如“一瓶水、一抔土”等。 (2) 动量词 用以计算动作次数多少以及持续时间长短，如“下、次、场、番”等。另有借用的动量词，如“嗷一嗓子、打一拳、算了一算”等。 语法特征：常位于名词和数词之间，数量短语可作定状补宾；许多单音节量词可以重叠，且重叠后意义常趋向每一、逐一、多等义，不能再作状语、宾语；有时也可单独入句，但常常是对数词“一”的省略（如“我有个朋友”等）；量词与名词的搭配关系并不固定，随方言习惯而各有差异。 副词 表示范围、程度、时间等义，常用以修饰动词、形容词性词语，又分： (1) 表示范围 全、都、皆、均、总、共、齐、就、只、单、光。 (2) 表示程度 很、非常、极、最、十分、太、越发。 (3) 表示时频 立即、曾经、刚刚、一向、再次、偶尔。 (4) 表示肯否 必须、一定、别、未、莫、没有、勿。 (5) 表示语气 竟然、岂非、偏、难怪、未免、只好。 (6) 表示处所 到处、处处、随处、四处。 (7) 表示关联 遂、就、再、又、仍。 (8) 表示情态方式：匆匆、一味、亲自、暗自、随意。 语法特征：皆可作状语，例外如“极”、“很”等还可作补语；一般不单独成句，只有“别、不、当然、何苦”等可以单用于省略句；部分副词可以起关联作用（如“越跑越快、又蹦又跳”等）。 代词 起代替、指示作用的词，语法功能与所代指语言单位大体相当，又分： (1) 代名词 有一般代名词（包括人称、疑问、指示）、处所代词、时间代词、数量代共4种，如“我、什么、这、哪里、这会儿、多少”等。 (2) 代谓词 怎么、这样。 (3) 代副词 多么、那么。 (4) 其他指示代词 各、每、旁的、其他。 语法特征：一般单用，不被别的词所修饰；使用灵活，有任指、虚指等用法（如“明天吃什么呢”、“多少钱他都不卖”等）。 拟声词 也称“象声词”，用以模拟事物的声音，如“轰、哞、叮咚、呼啦啦”等。 语法特征：常在句中作状语，有时后加“地”、“一声”等（有人把能加上“一声”的称作“单纯拟声词”，不能的称作“合成拟声词”）；还可作定谓补，亦能单独成句。 叹词 表示感叹、应答或呼唤的词，如“唉、哎、啊、喂、哦、嗯”等。 语法特征：常作句子独立语，亦可单独成句。 虚词 介词 也称“前置词”，常在实词、短语前组成介词短语，用以修饰或补充谓词性词语。可分为： (1) 表示施受关系 让、被、叫、把、给。 (2) 表示原因、目的：因为、为了、由于。 (3) 表示方式：据、按、照、依、靠、以。 (4) 表示关涉对象：和、跟、同、比、对、关于。 (5) 表示时间、处所、方向 自、从、往、到、至、趁着、当着。 语法特征：主要作状语，少数也能作补语、定语；多由动词虚化而来（如“比、给、叫”等），有些介词还处于过渡阶段，判断标准是介词不可重叠、不能单独作谓语（中心语）、不能加动态助词“着了过”。 连词 也称“连接词”，用以连接词、短语、分句和句子等成分并表示其间逻辑语义关系的虚词，又分： (1) 主要连接词和短语 和、跟、同、与、及、或。 (2) 主要连接词语或分句 而、并、且、或者。 (3) 主要连接复句中的分句 虽然、然而、与其、因此、只要。 语法特征：不能单独充当句子成分，需连接词语才可表达语法意义；常与副词配合成关联词语；有些特殊兼类词（如“和、同、跟”等）既是介词也是连词，需加以区分。 助词 附着在实词、短语或句子前后表示一定的结构关系或语法意义，又分： (1) 结构助词 的、地、得、之（定语后接“的”、状语后接“地”、补语前加“得”） (2) 动态助词 着、了、过。 (3) 尝试助词 看（轻声。 (4) 时间助词 来着、的（如“饭后是我洗的碗”等。 (5) 约数助词 把、来、多、左右。 (6) 比况助词 似的、一般、一样 。 (7) 其他助词：所、连、给、等、们。 语法特征：必须跟在别的词语前后，且后接的都读轻声，前加的都读原调。 语气词 也称“语（气）助词”，主要附在句末或句中停顿处表示语气，念作轻声。可分为： (1) 陈述语气词 的、了、吧、啊、呢、嘛、嘞。 (2) 疑问语气词 吗、吧、么、呢。 (3) 祈使语气词 吧、了、啊。 (4) 感叹语气词 啊。 语法特征：语气词常跟语调一起表达语气，因此同一个语气词可能会因语调不同而出现在好几种不同的语气中（如“啊”）；语气词可以连用，并具有一定的层次性（第一层为“的”，第二层为“了”，第三层为“啊、吗、吧、呢”），一般全局的基本语气由最后一个语气词决定。 短语 什么是短语 短语也叫“词组”，是词和词按照一定的语法规则和语义搭配关系组合起来的，没有句调的语言单位。词组成短语的主要语法手段有语序和虚词两类，前者主要体现在词组的直接组合中，后者主要体现在间接组合里，例如“努力工作/工作努力、老人与海/老人的狗”等。如何区别词和短语呢？一个简单的方法就是看这个语言单位是否可以插入其他成分进行扩展而自身意义还不发生较大的变化。例如“黑板”原指一种教具，若是强行拓展为“黑色的板”，就会变成一种泛指概念；而网络热词“心累”中间加入程度副词拓展为“心好累”，词义本身并无改变，只是程度加深了。 短语的类型 组合分类(1) 主谓短语 前主后谓 ，说明主语是什么或怎么样，二者间是陈述说明关系。例如：时光||飞逝（名||动）、空气||清新（名||形）、明天||周二（名||名）、他的话||我不相信（名||主|谓） (2) 动宾短语 前动后宾，表示做什么、有什么、是什么，二者间是支配、涉及关系。例如：提高|免疫力（动||名）、喜欢|热闹（动||形）、开始|学习（动||动）、喜欢|你（动||代）、买|两袋（动||数量短语） (3) 偏正短语 由修饰语和中心语组成，二者间是修饰与被修饰的关系，又分为定中（有时以结构助词“的”为标志）、状中两类（有时以结构助词“地”为标志）。例如：海上钢琴师、雨果的秘密、爱乐之城、海边的曼彻斯特（定中）、不停地说、电话联系、这么做、一米长（状中） (4) 中补短语 也称补充短语，由两个谓词组成，前为中心语后为补语，二者间是补充说明关系，有时补语前有结构助词“得”作为标志。例如：讲得&lt;好&gt;（动·形）、学&lt;会&gt;（动·动）、跑了&lt;一圈&gt;（动·数量短语）、高兴&lt;极&gt;了（动·副） (5) 联合短语 由语法地位平等的两个及其以上部分组成，其间是联合关系（可细分为并列、选择、递进等关系），有时用“与、和、或”等词连接。例如：罗密欧与朱丽叶（并列）、生存还是毁灭（选择）、讨论并通过（递进） (6) 其他短语 除开以上五种基本短语，另有连谓（多个谓词性成分连用，如“外出见面”）、兼语（动宾和主谓短语套用，如“祝你幸福”）、同位（前后两项不同词语指向同一事物，如“美国总统特朗普”）、方位（主要表示处所、范围或时间，如“放学后”）、量词（分数量和指量，如“一根、哪款”）、介词（如“[为中华崛起]而读书”）、助词（分“的”字、比况、“所”字等，如“好吃的、傻瓜似的、所了解”）等短语，在此不再一一赘述，感兴趣的读者可以自行查阅相关书籍。 功能分类,依照短语的句法功能相当于那类词而划分为： (1) 名词性短语 包括主谓、偏正、联合、同位、量词、方位、“的”字短语 (2) 谓词性短语 分为动词性短语和形容词性短语，包括主谓、偏正、中补、联合等 (3) 加词性短语 包括偏正短语（如“高质量”）和介词短语（如“向北走”） 短语的歧义 我们把意义单一的短语称作“单义短语”，把含有两个及其以上意义的短语称作“多义短语”。对于后者，人们在日常对话或篇章理解中常可通过背景信息或上下文进行语义排歧，然而当计算机处理这些字面上完全相同的短语时事情就没那么简单了。理清歧义产生背后的原因，有助于我们对目前的机器理解机制进行改进。 结构关系不同产生的歧义 A. 共享-单车,动宾,动宾短语（相当于“共享什么”） B. 共享-单车,定中,偏正短语（相当于“什么单车”） 语义关系不同产生的歧义 A. 鸡-不-吃-了,主-谓-状-中,主谓短语（相当于“不吃鸡了”） B. 鸡-不吃了,主-谓状中,主谓短语（相当于“鸡不吃食了”） 结构、语义关系皆不同产生的歧义 A. 咬-死-了-猎人-的-狗,动-谓-中-补-定-中,动宾短语（相当于“猎人的狗被咬死了”） B. 咬-死-了-猎人-的-狗,定-中-动-宾,偏正短语（相当于“狗咬死了猎人”） 以上分析短语时用到的框式图解方法称作“层次分析法”，它要求分析到词，层层二分（兼语、联合与连动例外），是国内传统现代汉语研究常用来分析各级语法单位的切分工具。层次分析法先是切分，再是定性；既可以由小到大（组合法），也可以由大到小（切分法）。 此外，西方语言学家乔姆斯基还提出了一种“X-bar”理论用来分析句法结构，其形式正是我们常在外文自然语言处理教材中看到的二叉树。应用这套理论，上文提到的三种例外也全部可以进行二分表示，具体内容请有兴趣的读者自行参阅原著《Lecture on Government Binding》（1981）或外文语言学教材如《An Introduction to Language》（Eighth Edition）等进行学习。 单句 什么是单句 句子是用以实际交际的基本语言单位，带有一定语调并表达相对完整的意思。而单句则是由短语或词构成的句子，在句子结构上与“复句”相对。 单句的分类 单句大体有两种分类角度：根据句法成分的搭配格局分出的结构类，也叫句型；根据整个句子的语气语调分出的语气类，也叫句类；具体如下。 句型 (1) 主谓句 由“主语+谓语”构成的单句，根据谓语核心的词性还可分为三小类：动词性谓语句、形容词性谓语句和名词性谓语句。例如：“故乡的月光不知让多少人魂牵梦绕。”（动谓）、“这块石头硬邦邦的。”（形谓）、“明天礼拜天。”（名谓）。 (2) 非主谓句 主谓句的补集，常由单个词或短语组成，分不出主谓语。主要分为：名词性非主谓句、动词性非主谓句、形容词性非主谓句、叹词句和拟声词句几类。例如：“妖怪啊！”（名）、“加油！”（动）、“妙极了！”（形）、“哎哟！”（叹）、“砰！”（拟）。 句类 (1) 陈述句 给出一定事实信息并带有陈述语气、语调的句子。常用平调、平而略降调，根据陈述内容的不同还可分为叙述句、描写句、判断句三类。例如“我下班回家了。”（叙述）、“同志们的工作热情都十分高涨。”（描写）、“你就是我失散多年的亲人呐！”（判断）。 (2) 疑问句 用提问索取相关信息并带有疑问语气、语调的句子。多数用升调，根据疑问标记方式还可分为是非问、选择问、正反问和特指问四类。例如：“你饿了吗？”（是非）、“我们是去公园还是去逛街？”（选择）、“你来不来？”（正反）、“谁动了我的奶酪？”（特指）。 (3) 祈使句 要求受话人实施某种行为的句子，常用短促的降调。例如：“放下武器！”。 (4) 感叹句 表达强烈情感的句子，常用舒缓绵长的降调。例如：“人心不古哇！”。 几类特殊句式 句式是根据句子的局部结构特征划分出来的句子类型，体现了语言在形式与表意等方面的特色。现代汉语中常见的、有结构特点的句式主要有主谓谓语句、双宾句、连谓句、兼语句、存现句、“把”“被”诸字句等。因篇幅有限，在此就不一一赘述这些特殊句式的构成与特点，请有兴趣的读者自行参阅相关教材，本书但举几例于下供大家思考品味。例如：“什么大风大浪他没见识过？”（主谓谓）、“老师告诉小华明天比赛。”（双宾）、“这大饼吃起来咯牙。”（连谓）、“你妈妈喊你回家吃饭！”（兼语）、“天墙上挂着一副世界地图。”（存现）、“你可把我气坏了！”（“把”字句）、“小明被人打了。”（“被”字句）。 析句方法 一个单句可以从句法、语义、语用等角度进行分析，其中单句的句法分析仍可以沿用我们之前介绍的层次分析法，在此就不再赘述了。而语义分析最常见的有三种方法，分别是：语义特征、语义角色和语义指向。其中语义特征的表示方法有点类似于之前讲过的义素分析法，都是用[+/-A]来进行标注；语义角色则是根据词语间的相互关系将其划分为“时间/处所”、“施/受/与事”、“工具”、“动作”、“结果”等成分；至于语义指向则是利用箭头将句中指向不明的两个成分关联起来。 最后，若是要对句子进行语用分析，有几组概念必须先做了解。在实际交流过程中，人们常常会出于各种各样的需要故意对句子成分进行简省或调换，这些变化后的句型统一叫做“变式句”。其中简省了成分的叫“省略句”，调换了句子成分的叫“倒装句”。面对以上变式句的语用分析，我们第一步要做的就是通过扩展、移位转换等辅助手段将其变为正常结构及语序的句子来进行分析。 复句 什么是复句 上文已经提到“复句”是结构上与“单句”相对的句子类别。它特指由两个及以上意义相关而结构上互不包含的分句，加上贯通前后的句调后所构成的句子。 复句的意义分类 根据复句分句间的语义关系，我们可以把复句相应地分为联合复句、偏正复句两大类。各分句间意义地位平等且无主从之分的复句叫联合复句（等立复句），反之句类有主从、正偏之分的就叫偏正复句（主从复句）。二者之下又可以进一步细分，具体如下。 联合复句 (1) 并列复句 前后分句分别对有关联的几件事情或同一事物的不同方面进行叙述，常用意合法或“一方面……一方面”“既……又”等关联词语连接。例如：“读诗使人灵秀,数学使人周密。”（意合法）、“低碳不仅是一种生活方式，也是一种生活态度。”（关联词）。 (2) 承接复句 前后分句按时空、逻辑等顺序说明相关的动作或情况，常用分句顺序或关联词“然后”“接着”“便”等连接。例如：“湖水滋润着湖边的青草，青草喂肥了羊群，羊奶哺育着少女的后代子孙。”（顺序）、“他说家里还有事，就提前走了。”（关联词）。 (3) 解说复句 用后一分句解释说明或归纳概括前面的分句，有总分和解释两种关系，一般不用关联词，以分句间语义关系连接。例如：“我从国外给你带回来了一盒巧克力，榛子味的。”（解释）、“世上有两样东西不可直视,一是太阳,二是人心。”（总分）。 (4) 选择复句 提出两种或多种可能的情况进行选择，常用“是……还是”“要么……要么”“宁可……也不”等合用关联词连接。例如：“如果我不在家,就是在咖啡馆。如果不是在咖啡馆,就是在往咖啡馆的路上。” (5) 递进复句 用后面分句进一步解释说明前句的意思，从而构成由浅入深的表达格局，反之亦可。常用关联词“不但……而且”“尚且……何况”“甚至”等连接。例如：“木犹如此，人何以堪。” 偏正复句 (1) 条件复句 偏句提出条件，正句说明这一条件推出的结果。有充足条件、必要条件、无条件三类，常用关联词“只要……就”“只有……才”“任凭”等连接。例如：“只要人人都献出一点爱，世界将变成美好的人间。”（充足）、“只有社会主义才能救中国，只有中国特色社会主义才能发展中国。”（必要）、“无论是谁，都会在不经意间失去什么。”（无条件）。 (2) 假设复句 偏句先提出假设，正句再说明将产生的结果。有一致和相背两类关系，常用关联词“如果……那么”“即使……也”等连接。例如：“如果你不说出去，就没有人会知道。”（一致）、“你要是不方便的话，也可以明天来。”（相背）。 (3) 因果复句 偏句说原因，正句表结果。有说明和推论两种关系，常用关键词“因为……所以”“既然”等连接。例如：“因为一个小小的失误，这次的行动彻底失败。”（说明）、“小明这题都算错了，可见他上课没有认真学。”（推论）。 (4) 目的复句 偏句说行为，正句表明目的。有得到和避免两类，常用关联词“为了”“以免”等连接。例如：“你把这些零食都带上，路上才好充饥。”（得到）、“一块儿上，省得我一个个收拾你们！”（避免）。 (5) 转折复句 前后分句语义上相对或相反。依转折强度不同，有重转、轻转、弱转之分。常用关联词“虽然……但是”（重）、“然而”“却”（轻）、“只是”“不过”（弱）等连接。例如：“虽然已是家财万贯，他却仍郁郁寡欢。” 多重复句与紧缩句 根据结构层次的数量，复句可以划分成“一重复句”和“多重复句”两类。我们之前所举的例句大多都是一重复句，而多重复句则是由两个及以上结构层次嵌套使用得到的句子。分析时常常根据各分句间的关系，第一层复句用|，第二层复句用||，以此类推将句中组合层次进行切分，并在竖线旁标明结构关系。例如：“老家没有高楼大厦，||递进甚至没有电灯电话，|转折可那儿的人们关系单纯亲切，||并列生活自在悠闲。” 紧缩句是取消了分句间语音停顿，压缩了某些关联词语，结构上更为紧凑的复句。它与单句中的连谓句颇为相像，分析时要仔细辨认，区别主要在于结构上有无关联词，语义上有无各类关系。例如：“他俩见面交谈起来。”（连谓）、“他俩一见面就交谈起来。”（紧缩）。 参考文献 GitHub 图书：《自然语言处理理论与实战》 完整代码下载 源码请进【机器学习和自然语言QQ群：436303759】文件下载： 作者声明 本文版权归作者所有，旨在技术交流使用。未经作者同意禁止转载，转载后需在文章页面明显位置给出原文连接，否则相关责任自行承担。]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>语言学</tag>
        <tag>语音</tag>
        <tag>语法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[编程数学之距离计算]]></title>
    <url>%2F2018%2F10%2F10%2F%E7%BC%96%E7%A8%8B%E6%95%B0%E5%AD%A6%E4%B9%8B%E8%B7%9D%E7%A6%BB%E8%AE%A1%E7%AE%97%2F</url>
    <content type="text"><![CDATA[摘要：数学在计算机科学、机器学习、深度学习、自然语言处理等多个领域占据比较重要的位置。比如：特征值和特征向量在PCA降维中会使用；均值在回归算法中估计公式参数中使用；期望和均值在回归方程应用；余弦定理求相似度中运用；拉格朗日定理在支持向量机中使用；求导在解决梯度下降中使用；矩阵和向量在数据格式转换和预处理中运用等等。还有在算法建模、参数设置、验证策略、识别欠拟合和过拟合等方面依然应用广泛。本系列数学文章旨在带领大家快速回顾常用知识，重在理解。（本文原创，转载必须注明出处.） 余弦距离形式化描述余弦夹角也可以叫余弦相似度。 几何中夹角余弦可用来衡量两个向量方向的差异，机器学习中借用这一概念来衡量样本向量之间的差异。余弦取值范围为[-1,1]。求得两个向量的夹角，并得出夹角对应的余弦值，此余弦值就可以用来表征这两个向量的相似性。夹角越小，趋近于0度，余弦值越接近于1，它们的方向更加吻合，则越相似。当两个向量的方向完全相反夹角余弦取最小值-1。当余弦值为0时，两向量正交，夹角为90度。因此可以看出，余弦相似度与向量的幅值无关，只与向量的方向相关。 公式化描述 在二维空间中向量A(x1,y1)与向量B(x2,y2)的夹角余弦公式 Python实现12345678910111213import numpy as npvec1 = [1,2,3,4]vec2 = [5,6,7,8]#方法一：根据公式求解dist1=np.dot(vec1,vec2)/(np.linalg.norm(vec1)*np.linalg.norm(vec2))print("余弦距离测试结果是：\t"+str(dist1))#方法二：根据scipy库求解from scipy.spatial.distance import pdistVec=np.vstack([vec1,vec2])dist2=1-pdist(Vec,'cosine')print("余弦距离测试结果是：\t"+str(dist2)) 欧氏距离形式化描述在数学中，欧几里得距离或欧几里得度量是欧几里得空间中两点间“普通”（即直线）距离。使用这个距离，欧氏空间成为度量空间。相关联的范数称为欧几里得范数。较早的文献称之为毕达哥拉斯度量。 公式化描述 维平面上两点\( a(x_1,y_1) \)与\( b(x_2,y_2)\)间的欧氏距离： d_{12}=\sqrt{(x_1-x_2)^2+(y_1-y_2)^2} 三维空间两点\( a(x_1,y_1,z_1)\)与\( b(x_2,y_2,z_1)\)间的欧氏距离： d_{12}=\sqrt{(x_1-x_2)^{2}+(y_1-y_2)^{2}+(z_1-z_2)^{2}} 两个n维向量\(a(x_1,x_2,…,x_n)\)与\(b(y_1,y_2,…,y_n)\)间的欧氏距离： 表示成向量运算的形式：\(d_{12}=\sqrt{(a-b)(a-b)^{T}}\) Python实现123456789101112131415161718import numpy as npvec1 =np.mat([1,2,3,4])vec2 =np.mat([5,6,7,8])#方法一：根据公式求解dist1=np.sqrt(np.sum(np.square(vec1-vec2)))print("欧氏距离测试结果是：\t"+str(dist1))#方法二：根据scipy库求解from scipy.spatial.distance import pdistVec=np.vstack([vec1,vec2])dist2=pdist(Vec)print("欧氏距离测试结果是：\t"+str(dist2))# 方法三：根据公式求解from numpy import *dist3 = sqrt((vec1-vec2)*(vec1-vec2).T)print("欧氏距离测试结果是：\t"+str(dist3)) 曼哈顿距离形式化描述计程车几何或曼哈顿距离或方格线距离是由十九世纪的赫尔曼·闵可夫斯基所创辞汇，为欧几里得几何度量空间的几何学之用语，用以标明两个点上在标准坐标系上的绝对轴距之总和。简言之，曼哈顿从一个十字路口开车到另外一个十字路口，驾驶距离不是两点间的直线距离。实际驾驶距离就是这个“曼哈顿距离”。而这也是曼哈顿距离名称的来源， 曼哈顿距离也称为城市街区距离(City Block distance)。 公式化描述二维平面两点a (x1,y1)与b(x2,y2)间的曼哈顿距离: d_{12}=|{(x_1-x_2)|+|(y_1-y_2)|} 两个n维向量\( a(x_1,x_2,…,x_n)\)与\( b(y_1,y_2,…,y_n)\)间的曼哈顿距离: d_{12}=\sum_{i=1}^{n}|x_i-y_i|Python实现1234567891011121314151617import numpy as npvec1 = np.mat([1,2,3,4])vec2 = np.mat([5,6,7,8])#方法一：根据公式求解dist1=np.sum(np.abs(vec1-vec2))print("曼哈顿距离测试结果是：\t"+str(dist1))#方法二：根据scipy库求解from scipy.spatial.distance import pdistVec=np.vstack([vec1,vec2])dist2=pdist(Vec,'cityblock')print("曼哈顿距离测试结果是：\t"+str(dist2))from numpy import *dist3 = sum(abs(vec1-vec2))print("曼哈顿距离测试结果是：\t"+str(dist3)) 明可夫斯基距离形式化描述明氏距离又叫做明可夫斯基距离，是欧氏空间中的一种测度，被看做是欧氏距离和曼哈顿距离的一种推广。 公式化描述 两个n维向量\(a(x_1,x_2,…,x_n)\)与\(b(y_1,y_2,…,y_n)\)间的明可夫斯基距离: d_{12}=\sqrt[p]{\sum_{i=1}^{n}|x_i-y_i|^p}也可以写成： 其中p是一个变参数。 当p=1时，就是曼哈顿距离 当p=2时，就是欧氏距离 当p→∞时，就是切比雪夫距离 Python实现12345678910111213141516171819202122import numpy as npvec1 = np.mat([1,2,3,4])vec2 = np.mat([5,6,7,8])#方法一：根据scipy库求解from scipy.spatial.distance import pdistVec=np.vstack([vec1,vec2])dist2=pdist(Vec,'minkowski',p=1)print("当p=1时就是曼哈顿距离,结果是：\t"+str(dist2))#方法二：根据公式求解,p=2dist1=np.sqrt(np.sum(np.square(vec1-vec2)))print("当p=2时就是欧式距离,结果是：\t"+str(dist1))from numpy import *#方法三：根据公式求解,p=1dist3 = sum(abs(vec1-vec2))print("当p=1时就是曼哈顿距离,结果是：\t"+str(dist3))#方法四：根据公式求解,p=2dist4 = sqrt((vec1-vec2)*(vec1-vec2).T)print("当p=2时就是欧式距离,测试结果是：\t"+str(dist4)) 切比雪夫距离形式化描述数学上，切比雪夫距离（Chebyshev distance）是向量空间中的一种度量，二个点之间的距离定义为其各座标数值差的最大值。以(x1,y1)和(x2,y2)二点为例，其切比雪夫距离为max(|x2-x1|,|y2-y1|)。切比雪夫距离得名自俄罗斯数学家切比雪夫。 公式化描述 二维平面两点\(a(x_1,y_1)\)与\(b(x_2,y_2)\)间的切比雪夫距离: d_{12}=max(|x_1-x_2|,|y_1-y2|) 两个n维向量\(a(x_1,x_2,…,x_n)\)与\(b(y_1,y_2,…,y_n)\)间的切比雪夫距离:d_{12}=\max_{i}(|x_i-y_i|) 还可以表示为：d_{12}=\lim_{k \to \infty }(\sum_{i=1}^{n}|x_i-y_i|^k)^{1/k} Python实现12345678910111213import numpy as npvec1 = np.mat([1,2,3,4])vec2 = np.mat([5,6,7,8])#方法一：根据公式求解dist1=np.max(np.abs(vec1-vec2))print("切比雪夫距离测试结果是：\t"+str(dist1))#方法二：根据scipy库求解from scipy.spatial.distance import pdistVec=np.vstack([vec1,vec2])dist2=pdist(Vec,'chebyshev')print("切比雪夫距离测试结果是：\t"+str(dist2)) 杰卡德距离形式化描述杰卡德相似系数：两个集合A和B的交集元素在A，B的并集中所占的比例，称为两个集合的杰卡德相似系数，用符号J(A,B)表示。 杰卡德距离：与杰卡德相似系数相反的概念即杰卡德距离用两个集合中不同元素占所有元素的比例来衡量两个集合的区分度。这是杰卡德距离(Jaccard distance)。 杰卡德相似系数与杰卡德距离的应用： 可将杰卡德相似系数用在衡量样本的相似度上。样本A与样本B是两个n维向量，而且所有维度的取值都是0或1。例如：A(0111)和B(1011)。我们将样本看成是一个集合，1表示集合包含该元素，0表示集合不包含该元素。 公式化描述 杰卡德相似系数公式化表示： 杰卡德距离可用如下公式表示： Python实现123456789101112131415161718import numpy as npv1=np.random.random(10)&gt;0.5v2=np.random.random(10)&gt;0.5vec1=np.asarray(v1,np.int32)vec2=np.asarray(v2,np.int32)#方法一：根据公式求解up=np.double(np.bitwise_and((vec1 != vec2),np.bitwise_or(vec1 != 0, vec2 != 0)).sum())down=np.double(np.bitwise_or(vec1 != 0, vec2 != 0).sum())dist1=(up/down)print("杰卡德距离测试结果是：\t"+str(dist1))#方法二：根据scipy库求解from scipy.spatial.distance import pdistVec=np.vstack([vec1,vec2])dist2=pdist(Vec,'jaccard')print("杰卡德距离测试结果是：\t"+str(dist2)) 汉明距离形式化描述在信息论中，两个等长字符串之间的汉明距离是两个字符串对应位置的不同字符的个数。换句话说，它就是将一个字符串变换成另外一个字符串所需要替换的字符个数。 范例公式化描述 1011101与1001001之间的汉明距离是2。 2143896与2233796之间的汉明距离是3。 “toned”与”roses”之间的汉明距离是3。 Python实现123456789101112131415v1=np.random.random(10)&gt;0.5v2=np.random.random(10)&gt;0.5vec1=np.asarray(v1,np.int32)vec2=np.asarray(v2,np.int32)#方法一：根据公式求解dist1=np.mean(vec1!=vec2)print("汉明距离测试结果是：\t"+str(dist1))#方法二：根据scipy库求解from scipy.spatial.distance import pdistVec=np.vstack([vec1,vec2])dist2=pdist(Vec,'hamming')print("汉明距离测试结果是：\t"+str(dist2)) 标准化欧式距离形式化描述标准化欧氏距离是针对简单欧氏距离的缺点而作的一种改进方案。标准欧氏距离的思路：既然数据各维分量的分布不一样，那我先将各个分量都“标准化”到均值、方差相等。均值和方差标准化到多少呢？这里先复习点统计学知识吧，假设样本集X的均值(mean)为m，标准差(standard deviation)为s，那么X的“标准化变量”表示为： 标准化后的值 = ( 标准化前的值 － 分量的均值 ) /分量的标准差 公式化描述两个n维向量\(a(x_1,x_2,…,x_n)\)与\(b(y_1,y_2,…,y_n)\)间的标准化欧氏距离的公式： d_{12}=\sqrt{\sum_{k=1}^n(\frac{x_i-y_i}{S_k})^2}Python实现1234567891011121314import numpy as npvec1 = np.array([1,2,3,4])vec2 = np.array([5,6,7,8])Vec=np.vstack([vec1,vec2])#方法一：根据公式求解sk=np.var(Vec,axis=0,ddof=1)dist1=np.sqrt(((vec1 - vec2) ** 2 /sk).sum())print("标准化欧氏距离测试结果是：\t"+str(dist1))#方法二：根据scipy库求解from scipy.spatial.distance import pdistdist2=pdist(Vec,'seuclidean')print("标准化欧氏距离测试结果是：\t"+str(dist2)) 皮尔逊相关系数形式化描述在统计学中，皮尔逊积矩相关系数（英语：Pearson product-moment correlation coefficient，常用r表示）用于度量两个变量X和Y之间的相关（线性相关），其值介于-1与1之间。在自然科学领域中，该系数广泛用于度量两个变量之间的相关程度。 公式化描述夹角余弦公式写成： Corr(x,y)=\frac{\sum_i(x_i-\vec{x})(y_i-\vec{y})}{\sqrt{\sum(x_i-\vec{x})^2}\sqrt{\sum(y_i-\vec{y})^2}}=\frac{}{||x-\vec{x}|| ||y-\vec{y}||}=CosSim(x-\vec{x},y-\vec{y})向量x和向量y之间的夹角余弦，则皮尔逊相关系数则可表示为： Python实现123456789101112131415import numpy as npvec1 = np.array([1,2,3,4])vec2 = np.array([5,6,7,8])#方法一：根据公式求解vec1_=vec1-np.mean(vec1)vec2_=vec2-np.mean(vec2)dist1=np.dot(vec1_,vec2_)/(np.linalg.norm(vec1_)*np.linalg.norm(vec2_))print("皮尔逊相关系数测试结果是：\t"+str(dist1))#方法二：根据numpy库求解Vec=np.vstack([vec1,vec2])dist2=np.corrcoef(Vec)[0][1]print("皮尔逊相关系数测试结果是：\t"+str(dist2)) 参考文献 Python官网 中文维基百科 GitHub 图书：《机器学习实战》 图书：《自然语言处理理论与实战》 完整代码下载 源码请进【机器学习和自然语言QQ群：436303759】文件下载： 作者声明 本文版权归作者所有，旨在技术交流使用。未经作者同意禁止转载，转载后需在文章页面明显位置给出原文连接，否则相关责任自行承担。]]></content>
      <categories>
        <category>数学</category>
        <category>距离计算</category>
      </categories>
      <tags>
        <tag>数学</tag>
        <tag>距离</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[编程数学之矩阵]]></title>
    <url>%2F2018%2F10%2F10%2F%E7%BC%96%E7%A8%8B%E6%95%B0%E5%AD%A6%E4%B9%8B%E7%9F%A9%E9%98%B5%2F</url>
    <content type="text"><![CDATA[摘要：数学在计算机科学、机器学习、深度学习、自然语言处理等多个领域占据比较重要的位置。比如：特征值和特征向量在PCA降维中会使用；均值在回归算法中估计公式参数中使用；期望和均值在回归方程应用；余弦定理求相似度中运用；拉格朗日定理在支持向量机中使用；求导在解决梯度下降中使用；矩阵和向量在数据格式转换和预处理中运用等等。还有在算法建模、参数设置、验证策略、识别欠拟合和过拟合等方面依然应用广泛。本系列数学文章旨在带领大家快速回顾常用知识，重在理解。（本文原创，转载必须注明出处.） 矩阵定义矩阵定义数学上，一个m×n的矩阵是一个由m行n列元素排列成的矩形阵列。矩阵里的元素可以是数字、符号或数学式。以下是一个由6个数字元素构成的2行3列的矩阵： 大小相同（行数列数都相同）的矩阵之间可以相互加减，具体是对每个位置上的元素做加减法。矩阵的乘法则较为复杂。两个矩阵可以相乘，当且仅当第一个矩阵的列数等于第二个矩阵的行数。矩阵的乘法满足结合律和分配律，但不满足交换律。 矩阵的理解将一些元素排列成若干行，每行放上相同数量的元素，就是一个矩阵。这里说的元素可以是数字，例如以下的矩阵： 排列成的形状是矩形，所以称为矩阵。矩阵一般用大写拉丁字母表示，一般用方括号或圆括号括起。以上的矩阵A是一个4行3列的矩阵。 行数是1或列数是1的矩阵又可分别称为行向量和列向量。矩阵的任一行（列）都是一个行（列）向量，例如矩阵A的第一行\({\begin{bmatrix}9&amp;13&amp;5\end{bmatrix}}\)就是一个行向量。行（列）向量可以看成一个向量，因此可以称矩阵的两行（列）相等，或者某一行等于某一列，表示其对应的向量相等。 增广矩阵矩阵的一个重要用途是解线性方程组。线性方程组中未知量的系数可以排成一个矩阵，加上常数项，则称为增广矩阵。 矩阵表示一个矩阵A从左上角数起的第i行第j列上的元素称为第i,j项，通常记为Ai,j、Aij、ai,j或A[i,j]，在上述例子中A[4,3] = 7。 矩阵运算矩阵加法定义：m×n矩阵A和B的和，A+B为一个m×n矩阵，其中每个元素是A和B相应元素的和，\((A+B)_{ij}=A_{i,j}+B_{i,j}\),其中1 ≤ i ≤ m , 1 ≤ j ≤ n. 例子： 运算律(A，B，C都是同型矩阵)： \( A+B=B+A \) \((A+B)+C=A+(B+C)\) 应该注意的是只有同型矩阵之间才可以进行加法 矩阵减法定义：m×n矩阵A和B的差，A-B为一个m×n矩阵，其中每个元素是A和B相应元素的差，\( (A-B)_{ij}=A_{i,j}-B_{i,j}\),其中1 ≤ i ≤ m , 1 ≤ j ≤ n. 例子： 矩阵数乘定义：标量c与矩阵A的数乘：cA的每个元素是A的相应元素与c的乘积，\((cA)_{ij}=cA_{i,j}\) 例子： 运算律： \((\lambda\mu )A=\lambda(\mu A)\) \((\lambda+\mu )A=\lambda A+\mu A\) \(\lambda(A+B)=\lambda A+\lambda B\) 矩阵转置定义：m×n矩阵A的转置是一个n×m的矩阵，记为\(A^T\)（其中的第i个行向量是原矩阵A的第i个列向量；或者说，转置矩阵\(A^T\)第i行第j列的元素是原矩阵A第j行第i列的元素，\((A^T)_{i,j}=A_{j,i}\) 例子： 运算律： \((A^T)^T=A\) \((\lambda A)^T=\lambda A^T\) \((A+B)^T=B^T+ A^T\) 共轭矩阵定义：矩阵的共轭定义为:\( A_{i,j}= \overline A_{ij} \)。 例子： 一个2×2复数矩阵的共轭如下： 则A的共轭矩阵是： 线性方程组矩阵乘法的一个基本应用是在线性方程组上。线性方程组是方程组的一种，它符合以下的形式： 其中的\( a{1,1},a{1,2} \) 以及\( b{1,1},b{1,2}\)等等是已知的常数，而\( x_1,x_2\)等等则是要求的未知数。运用矩阵的方式，可以将线性方程组写成一个向量方程：\( \mathbf {A} \mathbf {x} =\mathbf {b} \) 其中，A是由方程组里未知量的系数排成的m×n 矩阵，x是含有n 个元素的行向量，b 是含有m 个元素的行向量。 这个写法下，将原来的多个方程转化成一个向量方程，在已知矩阵A和向量b的情况下，求未知向量x。 行列式方块矩阵方块矩阵是行数与列数相同的矩阵称为方块矩阵，简称方阵。如下表示： 逆矩阵方阵A称为可逆或非奇异的，如果存在另一个方阵B，使得\(AB=I_n\)成立。这时候可以证明也有\(BA=I_n\)成立，可将矩阵B称为A的逆矩阵。一个矩阵A的逆矩阵如果存在的话，就是唯一的，通常记作\(A^{-1}\)。 矩阵的迹矩阵A的元素Ai,i称为其主对角线上的元素。方块矩阵A的所有主对角线元素之和称为它的迹，写作tr(A)。 例子： A的迹是\(tr(A)=a_{1,1}a_{2,2}\) 矩阵的迹具备以下特征： 尽管矩阵的乘法不满足交换律，方阵相乘时交换顺序会导致乘积变化，但它们的迹不会变，即：\(tr(AB)=tr(BA)\).矩阵转置的迹等于其自身的迹，\(tr(A)=tr(A^T)\) 对角矩阵如果一个方阵只有主对角线上的元素不是0，其它都是0，那么称其为对角矩阵。如下所示： 上三角矩阵如果主对角线下方的元素都是0，那么称为上三角矩阵。如下所示： 下三角矩阵如果主对角线上方的元素都是0，那么称为下三角矩阵，如下所示： 行列式 1×1方阵的行列式为该元素本身。 2×2方阵，其行列式用主对角线元素乘积减去次对角线元素的乘积。 3×3阶方阵 三阶矩阵发现\(a{12}\)的对角线少一部分（也就是\(a{23}\)的右下部分缺失）。一种方法是copy三个完全一样的矩阵做补充。 另一种方式就是利用代数余子式来计算: 在一个n阶行列式A中，把(i,j)元素\(a_{ij}\)所在的第i行和第j列划去后，留下的n−1阶方阵的行列式叫做元素\(a_{ij}\)的余子式，记作\(M_{ij}\)。代数余子式：\( A_{ij}=(-1)^{i+j}M_{ij} \).注意：代数余子式是个数值！ 下图方框里计算的值便是\(a_{11}a_{12}\)的代数余子式\(M_{11}M_{12}\) n阶的行列式等于它的任意一行（或列）的各元素与其对应的代数余子式乘积之和。对于任意一列： 对于任意一行： 所以上面三阶方阵的行列式A就是： 特征值和特征向量n×n的方块矩阵A的一个特征值和对应特征向量是满足\(\mathbf {Av} =\lambda \mathbf {v} \)的标量\(\lambda \)以及非零向量\(\mathbf {v}\)。特征值和特征向量的概念对研究线性变换很有帮助。一个线性变换可以通过它对应的矩阵在向量上的作用来可视化。一般来说，一个向量在经过映射之后可以变为任何可能的向量，而特征向量具有更好的性质。假设在给定的基底下，一个线性变换对应着某个矩阵A，如果一个向量x可以写成矩阵的几个特征向量的线性组合：\( \mathbf{x} = c_1 \mathbf{x}_{\lambda_1}+c_2\mathbf {x}_{\lambda_2}+\cdots +c_k\mathbf{x}_{\lambda_k} \) 其中的\( \mathbf{x}_{\lambda_i}\) 表示此向量对应的特征值是\(\lambda_i\)，那么向量x经过线性变换后会变成：\({\displaystyle \mathbf {Ax} =c_{1}\lambda_{1}\mathbf {x}_{\lambda_{1}}+c_{2}\lambda_{2}\mathbf {x}_{\lambda_{2}}+\cdots +c_{k}\lambda_{k}\mathbf {x}_{\lambda_{k}}} \).可以清楚地知道变换后向量的结构。另一个等价的特征值定义是：标量\(\lambda\)为特征值，如果矩阵\( \mathbf{A} -\lambda {\mathsf {I}}_n \) 是不可逆矩阵。根据不可逆矩阵的性质，这个定义也可以用行列式方程描述：\(\lambda\)为特征值，如果\(\det(\lambda {\mathsf {I}}_n-\mathbf {A} )=0.\) 这个定义中的行列式可以展开成一个关于\(\lambda\)的n阶多项式，叫做矩阵A的特征多项式，记为\(p_A\)。特征多项式是一个首一多项式（最高次项系数是1的多项式）。它的根就是矩阵A特征值。哈密尔顿－凯莱定理说明，如果用矩阵A本身代替多项式中的不定元 \( \lambda\) ，那么多项式的值是零矩阵：\( p_{\mathbf {A} }(\mathbf {A} )=0 \) 参考文献 Python官网 中文维基百科 GitHub 图书：《机器学习实战》 图书：《自然语言处理理论与实战》 完整代码下载 源码请进【机器学习和自然语言QQ群：436303759】文件下载： 作者声明 本文版权归作者所有，旨在技术交流使用。未经作者同意禁止转载，转载后需在文章页面明显位置给出原文连接，否则相关责任自行承担。]]></content>
      <categories>
        <category>数学</category>
        <category>矩阵</category>
      </categories>
      <tags>
        <tag>数学</tag>
        <tag>矩阵</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[编程数学之向量]]></title>
    <url>%2F2018%2F10%2F10%2F%E7%BC%96%E7%A8%8B%E6%95%B0%E5%AD%A6%E4%B9%8B%E5%90%91%E9%87%8F%2F</url>
    <content type="text"><![CDATA[摘要：数学在计算机科学、机器学习、深度学习、自然语言处理等多个领域占据比较重要的位置。比如：特征值和特征向量在PCA降维中会使用；均值在回归算法中估计公式参数中使用；期望和均值在回归方程应用；余弦定理求相似度中运用；拉格朗日定理在支持向量机中使用；求导在解决梯度下降中使用；矩阵和向量在数据格式转换和预处理中运用等等。还有在算法建模、参数设置、验证策略、识别欠拟合和过拟合等方面依然应用广泛。本系列数学文章旨在带领大家快速回顾常用知识，重在理解。（本文原创，转载必须注明出处.） 向量定义向量向量也称为欧几里得向量、几何向量、矢量，指具有大小和方向的量。它可以形象化地表示为带箭头的线段。箭头所指：代表向量的方向；线段长度：代表向量的大小。与向量对应的只有大小，没有方向的量叫做标量。 零向量始点与终点重合，也就是重合点的向量。 \( \vec{0}=\vec{AA}=\vec{BB}=.. \)，具有方向性，但方向不定。因此，零向量与任一向量平行。 等向量两向量长度、方向相等，即为等向量 有向线段有向线段的概念建构于向量的方向与长度，差别在于多定义了始点与终点。在文字描述时，如果已知某有向线段的起点和终点分别是A和B，此线段的长度可以记为\(|\vec {AB} |\) ，即 |\overrightarrow {AB}|=|\overline {AB} |向量的记法向量由方向和长度两个因素所组成，可以记为 \( \vec{a} \)。 数量积数量积也叫点积，它是向量与向量的乘积，其结果为一个标量（非向量）。几何上，数量积可以定义如下：设\( \vec {A} \)、\( \vec {B} \) 为两个任意向量，它们的夹角为 \( \theta \)，则他们的数量积为： \vec{A} \cdot \vec {B}=\left|\vec{A}\right|\left|\vec{B}\right|\cos\theta即\( \vec {A} \)向量在\( \vec {B} \)向量方向上的投影长度（同方向为正反方向为负号），与\( \vec {B} \)向量长度的乘积。 数量积被广泛应用于物理中，如做功就是用力的向量乘位移的向量，即\( W= \vec {F} \cdot \vec {s} \) 向量积向量积也叫叉积，外积，它也是向量与向量的乘积，它的结果是个向量。它的几何意义是所得的向量与被乘向量所在平面垂直，方向由右手定则规定，大小是两个被乘向量张成的平行四边形的面积。所以向量积不满足交换律。举例来说: (1,0,0)\times (0,1,0)=(0,0,1)(0,1,0)\times (1,0,0)=(0,0,-1)设有向量 \vec{A} =( A_x\vec{i},A_y\vec{j},A_z\vec{k} )\vec{B}=(B_x\vec{i},B_y\vec{j},B_z\vec{k})则其向量积的矩阵表达式可用下列符号表示： \vec{A}\times\vec{B}= \begin{vmatrix} \vec{i}&\vec{j}&\vec{k} \\\ A_x&A_y&A_z \\\ B_x&B_y&B_z \\\ \end{vmatrix}线性相关性\({\vec {v}}_1,{\vec {v}}_2,…,{\vec {v}}_m\) 对于m个向量 \( \vec{v}_1,\vec{v}_2,…,\vec{v}_m \) 如果存在一组不全为零的m个数\( \vec{a}_1,\vec{a}_2,…,\vec{a}_m \) \( {\displaystyle {\sum_{i=1}^m a_i{\vec{v}}_i}}=\vec{0} \) 那么，称 m个向量 \({\displaystyle {\vec {v}}_1},{\displaystyle {\vec {v}}_2},…,{\displaystyle {\vec {v}}_m} \)线性相关。如果这样不全为零的m个数不存在，即上述向量等式仅当\( {\displaystyle a_1} = {\displaystyle a_2} = … = {\displaystyle a_m} = 0 \)时才能成立，就称向量 \({\displaystyle {\vec {v}}_1},{\displaystyle {\vec {v}}_2},…,{\displaystyle {\vec {v}}_m} \)线性无关。 例子解析某人家门口是一条南北向的道路。他散步时先向南行走100米，那么他位置的移动就可以用一个大小为100米，方向为南的向量来表示。之后他再向北走300米，这一次的移动可以用一个大小为300米，方向为北的向量来表示。散步的人总共相对于他家的位移则可以用大小为200米，方向为北的向量来表示。几何学上看来，这些向量都在同一条一维的直线上，只有两个互相平行的方向。 向量运算代数表示一般计算机上采用加粗小写英文字母如：（a、b、c等）来表示；手写用在a、b、c等字母上加一箭头（→）表示，如 \( \vec{a}\vec{b}\vec{c} \)。 几何表示向量可以用有向线段来表示。有向线段的长度表示向量的大小，向量的大小，也就是向量的长度。长度为0的向量叫做零向量，记作长度等于1个单位的向量，叫做单位向量。向量表示箭头所指的方向表示向量的方向，如图所示。 向量定理 共线定理 若b≠0，则a//b的充要条件是存在唯一实数λ，使 \( \vec{a}=\lambda\vec{b} \)。若设\(a=(x_1,y_1)，b=(x_2,y_2)\) ，则有 \( x_1*y_2=x_2*y_1 \)，与平行概念相同。 \( \vec{0} \)平行于任何向量。 垂直定理 a⊥b的充要条件是a·b=0，即\( x_1x_2+y_1y_2=0 \) 。 加法向量的加法满足平行四边形法则和三角形法则。两个向量 \( \vec{a} \) 和 \( \vec{b} \)相加，得到的是另一个向量。这个向量可以表示为 \( \vec{a} \) 和 \( \vec{b} \)的起点重合后，以它们为邻边构成的平行四边形的一条对角线（以共同的起点为起点的那一条，如图所示），或者表示为将 \( \vec{a} \)的终点和\( \vec{b} \)的起点重合后，从\( \vec{a} \)的起点指向 \( \vec{b} \)的终点的向量 例子： \( \vec{a}=(x_1,y_1),\vec{b}=(x_2,y_2) \) 则:\( \vec{a}+ \vec{b} = (x_1+x_2,y_1+y_2)\) 向量加法的运算律： \( \vec{a}+ \vec{0} = \vec{0}+ \vec{a}= \vec{a}\) 交换律：\( \vec{a}+ \vec{b} = \vec{b}+ \vec{a}\)结合律：\( \vec{a}+ \vec{b})+ \vec{c}= \vec{a}+(\vec{b}+ \vec{c})\) 减法两个向量 \( \vec{a} \)和 \( \vec{b} \)的相减，则可以看成是向量 \( \vec{a} \)加上一个与 \( \vec{b} \)大小相等，方向相反的向量。换言之，\( \vec{a} \)和 \( \vec{b} \)的的相减得到的向量可以表示为\( \vec{a} \)和 \( \vec{b} \)的起点重合后，从 \( \vec{b} \)的终点指向 \( \vec{a} \)的终点的向量，如图所示。 例子： \(\vec{a}=(x_1,y_1),\vec{b}=(x_2,y_2)\) 则:\(\vec{a}- \vec{b} = (x_1-x_2,y_1-y_2)\) 加减变换律： \(\vec{a}+ \vec{(-b)} = \vec{a}- \vec{b}\) 数乘一个标量k和一个向量 \( \vec{a} \)之间可以做乘法，得出的结果是另一个与 \( \vec{a} \)方向相同或相反，大小为 \( \vec{a} \)的大小的｜k｜倍的向量，可以记成 \( k\vec{a} \)。 当k&gt;0时， \( k\vec{a} \)的方向与 \( \vec{a} \)的方向相同。 当k&lt;0时， \( k\vec{a} \)的方向与 \( \vec{a} \)的方向相反。 当k=0时， \( k\vec{a}=\vec{0} \)，方向任意。当\( \vec{a}=0\)时，对于任意实数k，都有\( k\vec{a}=\vec{0} \)。 -1乘以任意向量会得到它的反向量，0乘以任何向量都会得到零向量 \( \vec{0}\)。 数与向量满足运算律 结合律： \( (k\vec{a})\vec{b}=k(\vec{a}\vec{b})=(\vec{a}k\vec{b}) \) 向量对于数的分配律（第一分配律）： \( (k+m)\vec{a}=k\vec{a}+m\vec{a} \) 数对于向量的分配律（第二分配律）： \( k(\vec{a}+\vec{b})=k\vec{a}+k\vec{b} \) 数乘向量的消去律： 如果实数k≠0且\( k\vec{a}=k\vec{b} \)，那么\(\vec{a}=\vec{b}\)。 如果\( \vec{a}\neq \vec{0}\)且\(k\vec{a}= m\vec{a}\)，那么k=m。 数量积定义：已知两个非零向量 \( \vec{a} \)， \( \vec{b} \)。作\( OA= \vec{a}\),\(OB= \vec{b}\)，则∠AOB称作向量 \( \vec{b} \)和向量 \( \vec{a} \)的夹角，记作θ并规定0≤θ≤π 定义：两个向量的数量积（内积、点积）是一个数量（没有方向），记作\( \vec{a}\vec{b}\)。 若 \( \vec{a} \)、 \( \vec{b} \)不共线，则 \( \vec{a}*\vec{b}=|a|*|b|*cosθ\)。 若 \( \vec{a} \)、 \( \vec{b} \)共线，则\(\vec{a}*\vec{b}=\pm|a|*|b|\) 。 向量的数量积的坐标表示：\(\vec{a} \vec{b} = (x_1x_2+y_1y_2)\) 数量积的运算律交换律： \(\vec{a}* \vec{b} = \vec{b}* \vec{a} \) 结合律： \((k\vec{a})* \vec{b} =k(\vec{b}* \vec{a}) \) 分配律：\((\vec{a}+\vec{b})* \vec{c} =\vec{a}* \vec{c}+\vec{b}*\vec{c})\) 向量积定义：两个向量 \( \vec{a} \)和 \( \vec{b} \)的向量积（外积、叉积）是一个向量。记作\(\vec{a} \wedge \vec{b}\) 若 \( \vec{a} \)、 \( \vec{b} \)不共线，则\(\vec{a} \wedge \vec{b}\)的模是：\(|\vec{a} \wedge \vec{b}|=|\vec{a}|*|\vec{b}|*sin\langle\vec{a},\vec{b}\rangle\)；\(\vec{a} \wedge \vec{b}\)的方向是：垂直于\( \vec{a} \)和\( \vec{b} \)，且\( \vec{a} \)、\( \vec{b} \)和\(\vec{a} \wedge \vec{b}\)按这个次序构成右手系。 若\( \vec{a} \)、 \( \vec{b} \)垂直，则\(|\vec{a} \wedge \vec{b}|=|\vec{a}|*|\vec{b}|\)（此处与数量积不同，请注意），若\(|\vec{a} \wedge \vec{b}|=\vec{0}\)，则\( \vec{a} \)、\( \vec{b} \)平行。 向量积即两个不共线非零向量所在平面的一组法向量。 运算法则：运用三阶行列式设\( \vec{a} \)、\( \vec{b} \)、\( \vec{c} \)分别为沿x,y,z轴的单位向量\(A=(x_1,y_1,z_1)\)，\(B=(x_2,y_2,z_2)\)，则 A\*B=\begin{bmatrix} \vec{a} &\vec{b} & \vec{c}\\ x_1&y_1 &z_1 \\ x_2&y_2 &z_2 \end{bmatrix}向量积性质向量积\(|\vec{a} \wedge \vec{b}|\)是以\( \vec{a} \)和\( \vec{b} \)为边的平行四边形面积。 \(|\vec{a} \wedge \vec{a}|=\vec{0}\) \(\vec{a}//\vec{b}=\vec{a} \wedge \vec{a}|=\vec{0}\) 向量积运算律 \(|\vec{a} \wedge \vec{b}|=\vec{-b} \wedge \vec{a}|\) \(|(k\vec{a})\wedge \vec{b}=k(\vec{a}\wedge \vec{b})=\vec{a}\wedge (k\vec{b})\) \(\vec{a} \wedge (\vec{b}+\vec{c})=\vec{a} \wedge \vec{b}+\vec{a} \wedge \vec{c}\) \((\vec{a} + \vec{b}) \wedge \vec{c}=\vec{a} \wedge \vec{c}+\vec{a} \wedge \vec{c}\) \(a×(b+c)=a×b+a×c\).\((a+b)×c=a×c+b×c\).上两个分配律分别称为左分配律和右分配律。在演算中应注意不能交换“×”号两侧向量的次序。注：向量没有除法，“向量AB/向量CD”是没有意义的。 向量的模长(范数)向量的大小也叫做范数或者模长，有限维空间中，已知向量的坐标，就可以知道它的模长。设向量\( vec {v}=(v_1,v_2,\cdots ,v_n)\)， 范数记作：\({\displaystyle \left|{\vec {v}}\right|} \) 模长记作：\({\displaystyle \left|{\vec {v}}\right|} \) 计算表达式由弗罗贝尼乌斯范数（一种同时适用于向量和矩阵的范数计算方法）给出：\( \left|{\vec {v}}\right|={\sqrt {v_1^{2}+v_2^{2}+\cdots +v_n^{2}}} \) 或：\( \left|{\vec {v}}\right|={\sqrt {v_1^{2}+v_2^{2}+\cdots +v_n^{2}}}\) 参考文献 Python官网 中文维基百科 GitHub 图书：《机器学习实战》 图书：《自然语言处理理论与实战》 完整代码下载 源码请进【机器学习和自然语言QQ群：436303759】文件下载： 作者声明 本文版权归作者所有，旨在技术交流使用。未经作者同意禁止转载，转载后需在文章页面明显位置给出原文连接，否则相关责任自行承担。]]></content>
      <categories>
        <category>数学</category>
        <category>向量</category>
      </categories>
      <tags>
        <tag>数学</tag>
        <tag>向量</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简明的Python教程之NumPy]]></title>
    <url>%2F2018%2F10%2F10%2F%E7%AE%80%E6%98%8E%E7%9A%84Python%E6%95%99%E7%A8%8B%E4%B9%8BNumPy%2F</url>
    <content type="text"><![CDATA[摘要：Python作为一门简洁优美且功能强大的语言，越来越受编程人员的喜欢。在工业界和学术界也是非常受欢迎的编程语言。在python处理数据中，Numpy也是常有的工具，本文简要介绍，旨在大家熟悉相关应用。（本文原创，转载必须注明出处.） 导入相关的numpy包 import numpy from numpy import array from numpy import mat,matrix # 导入矩阵包 from numpy import shape # 查看矩阵或数组的方法 from numpy import multiply # 元素相乘 import random 计算方法向量求和 def nu_add(): mm = array((1,1,1)) pp = array((2,2,2)) rr = mm + pp**3 # 数组的和运算 print(rr) 运行结果： [9 9 9] 矩阵相乘 def nu_matrix(): ss = mat([1,2,3]) # 矩阵 mm = matrix([1,2,3]) print('an element: '.title()+str(mm[0,0])) # 访问矩阵中的单个元素 print('Number of dimensions of mm '.title()+str(shape(mm))) print('mat is equal matrix: '.title()+str(ss==mm)) print('Matrix multiplication: '.title()+str(ss*mm.T)) # 矩阵相乘需要进行转置 print('Multiplication of elements： '.title()+str(multiply(mm,ss))) # mm每个元素和ss每个元素相乘 运行结果： An Element: 1 Number Of Dimensions Of Mm (1, 3) Mat Is Equal Matrix: [[ True True True]] Matrix Multiplication: [[14]] Multiplication Of Elements： [[1 4 9]] 列表转化成矩阵 def nu_list_mat(): pylist = [1,2,3] rr = mat(pylist) # 列表转化成矩阵 print('list values: '.title()+str(pylist)) print('rr type: '.title()+str(type(rr))) print('mat values: '.title()+str(rr)) 运行结果： List Values: [1, 2, 3] Rr Type: &lt;class &#39;numpy.matrixlib.defmatrix.matrix&#39;&gt; Mat Values: [[1 2 3]] 矩阵求均值 def nu_mean(): dd = mat([4,5,1]) rr = dd.mean() # 矩阵的均值 print('mean of dd: '.title()+ str(rr)) 运行结果： Mean Of Dd: 3.3333333333333335 矩阵维度 def nu_mul_array(): jj = mat([[1,2,3],[8,8,8]]) print('Number of dimensions of jj '.title()+str(shape(jj))) one_row = jj[1,0:2] print(one_row) 运行结果： Number Of Dimensions Of Jj (2, 3) [[8 8]] 矩阵转置 def nu_tran_mat(): # 矩阵转置 radMat = numpy.random.random((3,3)) print('Before matrix transposition:\n '+str(radMat)) print('After matrix transposition:\n '+str(radMat.T)) 运行结果： Before matrix transposition: [[0.44400432 0.39811184 0.15014053] [0.33399525 0.35953194 0.92571106] [0.3307816 0.38282677 0.01282852]] After matrix transposition: [[0.44400432 0.33399525 0.3307816 ] [0.39811184 0.35953194 0.38282677] [0.15014053 0.92571106 0.01282852]] 矩阵的逆 def nu_inverse_mat(): # 矩阵的逆 radMat = numpy.random.random((3,3)) print('Before matrix inverse:\n '+str(radMat)) print('After matrix inverse:\n '+str(mat(radMat).I)) 运行结果： Before matrix inverse: [[0.67119611 0.28502048 0.00101008] [0.66644679 0.61934535 0.39481694] [0.10938817 0.22939074 0.45364209]] After matrix inverse: [[ 3.52467918 -2.38933642 2.07165582] [-4.79734922 5.63471564 -4.89336308] [ 1.57593243 -2.27312782 4.17923641]] 矩阵与逆矩阵相乘 def nu_mat_mul_imat(): # 矩阵与其逆矩阵相乘 bmat = mat(numpy.random.random((3,3))) imat = bmat.I rus = bmat * imat print(rus) # 结果是3*3的单位矩阵，其位置原则应该都是0，实际中是非常小的数，这个计算机处理的问题 运行结果： [[ 1.00000000e+00 -2.23066267e-17 -3.47038713e-17] [-7.97119302e-17 1.00000000e+00 1.74271619e-17] [-3.16553513e-16 -8.86129567e-17 1.00000000e+00]] 参考文献 Python官网 中文维基百科 GitHub 图书：《机器学习实战》 图书：《自然语言处理理论与实战》 完整代码下载 源码请进【机器学习和自然语言QQ群：436303759】文件下载： 作者声明 本文版权归作者所有，旨在技术交流使用。未经作者同意禁止转载，转载后需在文章页面明显位置给出原文连接，否则相关责任自行承担。]]></content>
      <categories>
        <category>Python</category>
        <category>NumPy</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>NumPy</tag>
        <tag>数学</tag>
        <tag>矩阵</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简明的Python教程之matplotlib可视化]]></title>
    <url>%2F2018%2F10%2F10%2F%E7%AE%80%E6%98%8E%E7%9A%84Python%E6%95%99%E7%A8%8B%E4%B9%8Bmatplotlib%E5%8F%AF%E8%A7%86%E5%8C%96%2F</url>
    <content type="text"><![CDATA[摘要：Python作为一门简洁优美且功能强大的语言，越来越受编程人员的喜欢。在工业界和学术界也是非常受欢迎的编程语言。在python处理数据中，需要数据分析，我们直面冷冰冰的数值或者信息很难得到直观的感受，如果借助图形化分析，常常可以一目了然，所谓一图胜千言就是这个意思。本文对可视化库matplotlib介绍，方便大家掌握。（本文原创，转载必须注明出处.） matplotlib绘制折线图绘制y=2x+1的折线图 import matplotlib import matplotlib.pyplot as plt #加入中文显示 import matplotlib.font_manager as fm # 解决中文乱码，本案例使用宋体字 myfont=fm.FontProperties(fname=r"C:\\Windows\\Fonts\\simsun.ttc") def line_chart(xvalues,yvalues): # 绘制折线图,c颜色设置，alpha透明度 plt.plot(xvalues,yvalues,linewidth=5,alpha=0.5,c='red') # num_squares数据值，linewidth设置线条粗细 # 设置折线图标题和横纵坐标标题 plt.title("Python绘制折线图",fontsize=30,fontname='宋体',fontproperties=myfont) plt.xlabel('横坐标',fontsize=20,fontname='宋体',fontproperties=myfont) plt.ylabel('纵坐标',fontsize=20,fontname='宋体',fontproperties=myfont) # 设置刻度标记大小,axis='both'参数影响横纵坐标，labelsize刻度大小 plt.tick_params(axis='both',labelsize=10) # 显示图形 plt.show() if __name__ == "__main__": # 1 打印y=2X+1 的折线图 line_chart([0,2,3,4,5],[1,7,10,13,16]) ## 运行结果 ![](https://i.imgur.com/A3omLb0.png) *** # matplotlib绘制散点图 ## 绘制y=2x+1的散点图 import matplotlib import matplotlib.pyplot as plt #加入中文显示 import matplotlib.font_manager as fm # 解决中文乱码，本案例使用宋体字 myfont=fm.FontProperties(fname=r"C:\\Windows\\Fonts\\simsun.ttc") def scatter_chart(xvalues,yvalues): # 绘制散点图，s设置点的大小,c数据点的颜色，edgecolors数据点的轮廓 plt.scatter(xvalues,yvalues,c='green',edgecolors='none',s=40) # 设置散点图标题和横纵坐标标题 plt.title("Python绘制折线图",fontsize=30,fontname='宋体',fontproperties=myfont) plt.xlabel('横坐标',fontsize=20,fontname='宋体',fontproperties=myfont) plt.ylabel('纵坐标',fontsize=20,fontname='宋体',fontproperties=myfont) # 设置刻度标记大小,axis='both'参数影响横纵坐标，labelsize刻度大小 plt.tick_params(axis='both',which='major',labelsize=10) # 设置每个坐标轴取值范围 # plt.axis([80,100,6400,10000]) # 显示图形 plt.show() # 自动保存图表,bbox_inches剪除图片空白区 # plt.savefig('squares_plot.png',bbox_inches='tight') if __name__ == "__main__": # 1 绘制y=2X+1 的折线图 scatter_chart([0,2,3,4,5],[1,7,10,13,16]) ## 运行结果 ![](https://i.imgur.com/6ZTgFhT.png) *** # matplotlib读取csv文件显示折线图 ## 加利福尼亚死亡谷日气温最高最低图 import csv from datetime import datetime from matplotlib import pyplot as plt import matplotlib as mpl # 解决中文乱码问题 mpl.rcParams['font.sans-serif']=['SimHei'] mpl.rcParams['axes.unicode_minus']=False # Get dates, high, and low temperatures from file. filename = 'death_valley_2014.csv' with open(filename) as f: reader = csv.reader(f) header_row = next(reader) # print(header_row) # for index,column_header in enumerate(header_row): # print(index,column_header) dates, highs,lows = [],[], [] for row in reader: try: current_date = datetime.strptime(row[0], "%Y-%m-%d") high = int(row[1]) low = int(row[3]) except ValueError: # 处理 print(current_date, 'missing data') else: dates.append(current_date) highs.append(high) lows.append(low) # 汇制数据图形 fig = plt.figure(dpi=120,figsize=(10,6)) plt.plot(dates,highs,c='red',alpha=0.5)# alpha指定透明度 plt.plot(dates,lows,c='blue',alpha=0.5) plt.fill_between(dates,highs,lows,facecolor='orange',alpha=0.1)#接收一个x值系列和y值系列，给图表区域着色 #设置图形格式 plt.title('2014年加利福尼亚死亡谷日气温最高最低图',fontsize=24) plt.xlabel('日（D）',fontsize=16) fig.autofmt_xdate() # 绘制斜体日期标签 plt.ylabel('温度（F）',fontsize=16) plt.tick_params(axis='both',which='major',labelsize=16) # plt.axis([0,31,54,72]) # 自定义数轴起始刻度 plt.savefig('highs_lows.png',bbox_inches='tight') plt.show() ## 运行结果 ![](https://i.imgur.com/X6HpFU4.png) *** # matplotlib生成随机漫步图 ## 随机数据生成 from random import choice class RandomWalk(): '''一个生成随机漫步数据的类''' def __init__(self,num_points=5000): '''初始化随机漫步属性''' self.num_points = num_points self.x_values = [0] self.y_values = [0] def fill_walk(self): '''计算随机漫步包含的所有点''' while len(self.x_values)]]></content>
      <categories>
        <category>Python</category>
        <category>matplotlib</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简明的Python教程之70个常备知识点]]></title>
    <url>%2F2018%2F10%2F10%2F%E7%AE%80%E6%98%8E%E7%9A%84Python%E6%95%99%E7%A8%8B%E4%B9%8B70%E4%B8%AA%E5%B8%B8%E5%A4%87%E7%9F%A5%E8%AF%86%E7%82%B9%2F</url>
    <content type="text"><![CDATA[摘要：Python作为一门简洁优美且功能强大的语言，越来越受编程人员的喜欢。在工业界和学术界也是非常受欢迎的编程语言。在阅读python相关书籍中，对其进行简单的笔记记录。旨在注意一些细节问题，在今后项目中灵活运用，并对部分小知识点进行代码标注。（本文原创，转载必须注明出处.） python始终记录变量最新值。 变量应简短且具有描述性，如student_name等。 变量名推荐小写。 单双引号括起来的，字符串可以包含引号和撇号。用法：”this’s a cup” title()将每个单词的首字母都改为大写。用法：str.title() upper()将字符串转化为大写。用法：str.upper() lower()将字符串转化为小写。用法：str.lower() 空白泛指任何非打印字符。如空格、制表符和换行符。 rstrip()剔除字符串末尾空白。用法：str.rstrip() lstrip()剔除字符串开头空白。用法：str.lstrip() strip()剔除字符串两端空白。用法：str.strip() Python使用两个称号表示乘方。用法：3 ** 2 编程理念。Python之禅：import this list中使用逗号来分割其中的元素。 list索引-1返回最后一个元素列表，-2以此类推。用法：list[-3:] list[0] = ‘update’ 修改列表元素 list.append(‘add’) 列表中添加元素 list.insert(0.’insert’) 列表中指定位置插入元素 del list[0] del元素删除list元素 newlist = list.pop()方法pop()删除元素 从列表中删除元素且不再使用用del方法，删除元素后还有可能用选择pop() list.remove(‘element’) 根据值移除第一个指定的元素，可接着使用。 sort()列表按照字母永久性排序。如：list.sort() sort()列表按照字母相反的排序。如：list.sort(reverse=True) reverse() 反转列表元素排序。用法：list.reverse() for循环遍历时候选择有意义的名称。用法： for cat in cats: range() 生成一系列数字。用法： numbers= list(range(1,11,2)) list的内建统计函数。用法：min(list)/max(list)/sum(list) python的切片功能。用法： list[0:3]/list[:]/list[-3:]/list[:9] list复制。用法：new_foods = old_food[:] 元组包括一些列不可修改的元素且用圆括号标识。用法：tulple = (2,3) 检查是否相等时不考虑大小写。用法：str.lower() == ‘somestr’ 使用and检查多个条件。用法：condition1&gt;=1 and condition2&gt;=2 and … 使用or检查多个条件。用法：condition1&gt;=1 or condition2&gt;=2 or … 使用多个列表。用法： list1 = ['1','2','3','4'] list2 = ['1','4'] for l2 in list2: if l2 in list1: go() else: pass 36 比较运算符两边各添加空格，便于可读性。用法：if age &gt; 40：37 dict修改值，用法：dict[‘key’] = value38 dict删除键值对，用法： del dict[‘key’]39 字典的遍历，用法： for key,value in dict.items(): for key in dict: for key in dict.keys(): for value in dict.values(): for value in set(dict.values()): # 遍历字典的无重复值 40 字典列表，用法： dict1 = ['key1':'values1','key2':'values2'] dict2 = ['key1':'values3','key2':'values4'] dict3 = ['key1':'values5','key2':'values6'] dicts = [dict1,dict2,dict3] for dict in dicts: pass 41 字典中存储列表，用法： dict1 = {'key1':'values1','key2':['values1','values2']} for dict in dict1['key2']: 42 字典中存储字典，用法： dicts = { 'keys1':{'key1':'values1'，'key1':'values2''key1':'values3'}, 'keys2':{'key2':'values2'，'key2':'values2''key2':'values3'} } 43 input接收用户输入，用法：message = input(‘user input some values!’)44 %取模运算判断奇偶，用法： if (4 % 3) == 0: print('偶数')： else: print('奇数') 45 while循环的常规用法： current_number = 1 while current_number]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简明的Python教程之网站篇]]></title>
    <url>%2F2018%2F10%2F10%2F%E7%AE%80%E6%98%8E%E7%9A%84Python%E6%95%99%E7%A8%8B%E4%B9%8B%E7%BD%91%E7%AB%99%E7%AF%87%2F</url>
    <content type="text"><![CDATA[摘要：Python作为一门简洁优美且功能强大的语言，越来越受编程人员的喜欢。在工业界和学术界也是非常受欢迎的编程语言。python语言可以跨平台跨应用开发。本系列文章首先介绍Python语言及其可以做什么事情。哪些人群适合学习python和python语法特点。其次介绍了Python进阶，以实际案例演示常用的语句和控制流、表达式、函数、数据结构、标准库等知识点。然后扩展介绍了python第三方库，使读者对python有个全面的理解和认识。最后一节，采用实际案例帮助读者综合运用python知识。 （本文原创，转载必须注明出处.） Django搞定用户管理系统Django是什么Django是一个开放源代码的Web应用框架，由Python写成。采用了MVC的软件设计模式，即模型M，视图V和控制器C。它最初是被开发来用于管理劳伦斯出版集团旗下的一些以新闻内容为主的网站的。并于2005年7月在BSD许可证下发布。这套框架是以比利时的吉普赛爵士吉他手Django Reinhardt来命名的。 Django的主要目标是使得开发复杂的、数据库驱动的网站变得简单。Django注重组件的重用性和“可插拔性”，敏捷开发和DRY法则（Don’t Repeat Yourself）。在Django中Python被普遍使用，甚至包括配置文件和数据模型。 Django开发模型Django是一个基于MVC构造的框架。但是在Django中，控制器接受用户输入的部分由框架自行处理，所以 Django 里更关注的是模型（Model）、模板(Template)和视图（Views），称为 MTV模式。它们各自的职责如图所示： 模型（Model），即数据存取层 处理与数据相关的所有事务： 如何存取、如何验证有效性、包含哪些行为以及数据之间的关系等。 视图（View），即表现层 处理与表现相关的决定： 如何在页面或其他类型文档中进行显示。 模板(Template)，即业务逻辑层 存取模型及调取恰当模板的相关逻辑。模型与模板的桥梁。 Django的架构让我们一览 Django 全貌： urls.py 网址入口，关联到对应的views.py中的一个函数（或者generic类），访问网址就对应一个函数。 views.py 处理用户发出的请求，从urls.py中对应过来, 通过渲染templates中的网页可以将显示内容，比如登陆后的用户名，用户请求的数据，输出到网页。 models.py 与数据库操作相关，存入或读取数据时用到这个，当然用不到数据库的时候 你可以不使用。 forms.py 表单，用户在浏览器上输入数据提交，对数据的验证工作以及输入框的生成等工作，当然你也可以不使用。 templates 文件夹 views.py 中的函数渲染templates中的Html模板，得到动态内容的网页，当然可以用缓存来提高速度。 admin.py 后台，可以用很少量的代码就拥有一个强大的后台。 settings.py Django 的设置，配置文件，比如 DEBUG 的开关，静态文件的位置等。 上面的py文件不理解也没有关系，后面会详细介绍。一图胜千言，架构全貌工作机制如图所示： Django商业网站Sohu 邮箱 、果壳网 、 豆瓣 、 爱调研 、 易度在线云办公 、 优容网 、 快玩游戏、九九房、贷帮网 、 趣奇网 、知乎、时尚时空 、游嘻板: YxPad webpy、DNSPod 国际版 、下厨房 、 贝太厨房 、 Wopus问答 、 咕咚网 、扇贝网 、站长工具、易度文档管理系统、个人租房、 在线文档查看-易度云查看 、 FIFA310 足球数据分析专家、 搜狐随身看等等。 Django安装配置前置条件 假设读者具备Python、web开发、html、css、js、SQL基础。 Anaconda(包含pip和python插件)、sublime环境已经安装。其中pip用来在线安装工具包。 系统环境：WIN10 64bit 开发环境：sublime+Anaconda 数据库：Mysql 5.6.17 语言：python3.5 框架：django1.11 通过“Win+R”键打开运行对话框，输入“pip”查看是否安装成功。如果pip输入报错，则需要自行下载安装即可。如果成功安装如图所示： 在线安装Django通过“Win+R”键打开运行对话框，输入“pip install django”代码，自动运行结束后。打开Sublime编辑器，点击“F6”切换的编辑环境，输入如下代码查看django是否安装成功。 >>> import django >>> django.VERSION (1, 11, 0, 'final', 1) 创建用户管理项目创建项目和App 第一步 通过“Win+R”键打开运行对话框，并在电脑的E盘根目录下创建名为UserProject的项目,输入如下创建命名： django-admin startproject xmjc_analysis 命令执行完成，通过Sublime Text3打开菜单“File->Open Folder”选择刚刚创建的UserProject文件夹，如图所示： ![](https://i.imgur.com/h0AgYXf.png) - settings.py 项目的设置文件 - urls.py 总的urls配置文件 - wsgi.py 部署服务器文件 - __ init__.py python包的目录结构必须的，与调用有关 > 第二步 进入UserProject文件夹下创建App名为myuser，执行如下命令，命令成功后如图所示： django-admin startapp myuser 一个项目可以包含多个App，执行完app创建命令后，打开项目查看如图所示： admin.py 后台，可以用很少量的代码就拥有一个强大的后台。 apps.py 本app项目名称等配置。 models.py 与数据库操作相关，存入或读取数据时用到这个，当然用不到数据库的时候 你可以不使用。 tests.py 单元测试文件 views.py 处理用户发出的请求，从urls.py中对应过来, 通过渲染templates中的网页可以将显示内容，比如登陆后的用户名，用户请求的数据，输出到网页。 第三步 将新定义名为“myuser”的app添加到UserProject项目中。修改UserProject/UserProject/settings.py代码如下： # Application definition INSTALLED_APPS = [ 'django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', # 添加新建app 'myuser', ] 到此，完成项目的创建和app的添加。是不是很简单，下面我们创建一个新的欢迎界面。 第一个欢迎页面1 在myuser文件夹下，打开UserProject/myuser/views.py文件,修改里面的代码如下： ''' 第一个页面 author：白宁超 site：http://www.cnblogs.com/baiboy/ ''' #coding:utf-8 from django.shortcuts import render from django.http import HttpResponse def index(request): return HttpResponse(u"欢迎进入第一个Django页面!") 第一行是声明编码为utf-8, 因为我们在代码中用到了中文,如果不声明就报错.第二行引入HttpResponse，它是用来向网页返回内容的，就像Python中的 print 一样，只不过 HttpResponse 是把内容显示到网页上。我们定义了一个index()函数，第一个参数必须是 request，与网页发来的请求有关，request 变量里面包含get或post的内容。 2 我们打开UserProject/UserProject/urls.py 这个文件, 修改其中的代码 from django.conf.urls import url from django.contrib import admin # 导入myuser下的视图文件 from myuser import views as myuser_views urlpatterns = [ # 第一个欢迎界面的配置 url(r'^index/$', myuser_views.index,name='index'), url(r'^admin/', admin.site.urls), ] 3 在UserProject目录下输入“python manage.py runserver”命令本地运行服务器，如图所示： 4 将上图的本地网址复制到浏览器访问，本地网址后面输入刚刚配置的方法名“index”即可访问我们刚刚设计的网页。如图所示： 带参数的欢迎页面 打开UserProject/myuser/views.py文件,修改里面的代码如下：‘’’coding:utf-8from django.shortcuts import renderfrom django.http import HttpResponse 访问第一个页面def index(request): return HttpResponse(u”欢迎进入第一个Django页面!”) 访问第一个带参数的页面def indexPara(request): name = request.GET[‘name’] return HttpResponse(u”欢迎进入\t”+name+”,第一个Django页面!”)&lt;/pre&gt; 我们打开UserProject/UserProject/urls.py 这个文件, 修改其中的代码from django.conf.urls import urlfrom django.contrib import admin 导入myuser下的视图文件from myuser import views as myuser_views urlpatterns = [ # 带参数的欢迎界面的配置 url(r&#39;^para/$&#39;, myuser_views.indexPara,name=&#39;para&#39;), # 第一个欢迎界面的配置 url(r&#39;^index/$&#39;, myuser_views.index,name=&#39;index&#39;), url(r&#39;^admin/&#39;, admin.site.urls), ]&lt;/pre&gt; 如果服务器关闭，需要如上方法重新打开。反之，直接输入网址查看，如图所示： 单数据库配置 在UserProject/UserProject/settings.py文件下,默认sqlites数据库，现在假设项目需求是mysql数据库。具体修改如下： # Database # https://docs.djangoproject.com/en/1.11/ref/settings/#databases DATABASES = { # 配置默认为mysql数据库 'default': { 'ENGINE': 'django.db.backends.mysql', # 设置数据库引擎 'NAME': 'test', # 数据库名 'USER': 'test', # 数据库登陆名 'PASSWORD': 'test123', # 数据库登陆密码 'HOST':'localhost', # 数据库服务器IP，默认设置本地 'PORT':'3306', # 端口号 }, # 默认配置为sqlite3数据库 # 'default': { # 'ENGINE': 'django.db.backends.sqlite3', # 'NAME': os.path.join(BASE_DIR, 'db.sqlite3'), # } } UserProject/UserProject/ init.py文件添加如下代码： import pymysql pymysql.install_as_MySQLdb() 在UserProject/myuser/models.py下设计数据库表，采用ORM方式，完整代码如下：from django.db import models Create your models here.class User(models.Model): username = models.CharField(‘用户名’, max_length=30) userpass = models.CharField(‘密码’,max_length=30) useremail = models.EmailField(‘邮箱’,max_length=30) usertype = models.CharField(‘用户类型’,max_length=30) def __str__(self): return self.username &lt;/pre&gt; 在analysis/admin.py中定义显示数据 from django.contrib import admin from .models import User class UserAdmin(admin.ModelAdmin): list_display = ('username','userpass','useremail') # 自定义显示字段 admin.site.register(User,UserAdmin) 打开对话框，在“UserProject”下将生成的py文件应用到mysql数据库。输入如下命令： python manage.py makemigrations python manage.py migrate 创建超级管理员：用户名，admin1；密码密码：admin123，如图所示 python manage.py createsuperuser 创建后台超级管理员 创建后台超级管理员 登录后台查看信息 可以看到后台信息，并对数据表进行增删改查操作，但是后台全部英文如图示，可以改为中文显示？ 后台管理设置为中文显示,UserProject/UserProject/settings.py下修改代码： LANGUAGE_CODE = 'zh-Hans' # 中文显示 再去查看： ![](https://i.imgur.com/yCz4Atj.png) *** # Django数据库操作 ## QuerySet API，shell玩转MySql 在UserProject项目下输入【 python manage.py shell】，然后查询数据表如图所示： ![](https://i.imgur.com/MwMAay7.png) 创建一条用户信息，输入如下shell命令： User.objects.create(username="李白", userpass="libai123",useremail="libai@163.com",usertype="超级管理员") 登录项目后台查看，如图所示。 ![](https://i.imgur.com/tDHK2vF.png) 其他shell操作语句 # 方法 1 User.objects.create(username="李白", userpass="libai123",useremail="libai@163.com",usertype="超级管理员") # 方法 2 twz =User(username="李白", userpass="libai123",useremail="libai@163.com",usertype="超级管理员") twz.save() # 获取对象： Person.objects.all() # 满足条件查询 User.objects.filter(username="李白") # 迭代查询： es = Entry.objects.all() for e in es: print(e.headline) # 查询排序： User.objects.all().order_by('username') # 链式查询： User.objects.filter(name__contains="WeizhongTu").filter(email="tuweizhong@163.com") # 去重查询： qs = qs.distinct() # 删除操作： User.objects.all().delete() # 更新操作： Person.objects.filter(name__contains="abc").update(name='xxx') # 数据的导出： python manage.py dumpdata [appname] > appname_data.json python manage.py dumpdata blog > blog_dump.json # 导出用户数据 python manage.py dumpdata auth > auth.json # 导出用户数据 批量向数据表导入数据 将name.txt导入数据库，数据内容如下： 张一|admin|zhang1@qq.com|超级管理员 张二|admin|zhang2@qq.com|超级管理员 张三|admin|zhang3@qq.com|超级管理员 张四|admin|zhang4@qq.com|超级管理员 张五|admin|zhang5@qq.com|超级管理员 张六|admin|zhang6@qq.com|超级管理员 张七|admin|zhang7@qq.com|超级管理员 张八|admin|zhang8@qq.com|超级管理员 张九|admin|zhang9@qq.com|超级管理员 文本数据批量导入mysql数据库中，核心源码如下： def main(): from analysis.models import User f = open('./readme/files/name.txt',encoding='utf-8') for line in f: name,pwd,email,type = line.split('|') User.objects.create(username=name,userpass=pwd,useremail=email,usertype=type) f.close() 重新登录“用户信息管理后台”，查看结果如图所示： 参考文献 Python官网 中文维基百科 GitHub 图书：《机器学习实战》 图书：《自然语言处理理论与实战》 完整代码下载 源码请进【机器学习和自然语言QQ群：436303759】文件下载： 作者声明 本文版权归作者所有，旨在技术交流使用。未经作者同意禁止转载，转载后需在文章页面明显位置给出原文连接，否则相关责任自行承担。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简明的Python教程之实例篇]]></title>
    <url>%2F2018%2F10%2F10%2F%E7%AE%80%E6%98%8E%E7%9A%84Python%E6%95%99%E7%A8%8B%E4%B9%8B%E5%AE%9E%E4%BE%8B%E7%AF%87%2F</url>
    <content type="text"><![CDATA[摘要：Python作为一门简洁优美且功能强大的语言，越来越受编程人员的喜欢。在工业界和学术界也是非常受欢迎的编程语言。python语言可以跨平台跨应用开发。本系列文章首先介绍Python语言及其可以做什么事情。哪些人群适合学习python和python语法特点。其次介绍了Python进阶，以实际案例演示常用的语句和控制流、表达式、函数、数据结构、标准库等知识点。然后扩展介绍了python第三方库，使读者对python有个全面的理解和认识。最后一节，采用实际案例帮助读者综合运用python知识。 （本文原创，转载必须注明出处.） 语句和控制流Hello World编程第一步，打印“Hello World”的输入结果： def callHello(): print("hello world！") callHello() 运行结果： hello world！ if 语句::根据成绩评判等级 # if 语句:根据成绩评判等级 def CallLevel(score): if score >= 90 : print("优秀") elif score >= 60: print("及格") else: print("不及格") CallLevel(score=60) 运行结果： 及格 ## for 语句:循环输出所有评分指标 # for 语句:循环输出所有评分指标 def GetLevel(score): for lev in score: print(lev,end=" ") # 设置不换行 GetLevel(score=["优秀","及格","不及格"]) # 列表参数 运行结果： 优秀 及格 不及格 ## while 语句:循环输出所有评分 # while 语句:循环输出所有评分 def GetLevel2(score): countlen=len(score) while countlen > 0: print(score[countlen-1],end=" ") # 设置不换行 countlen -= 1 GetLevel2(score=[90,30,100,98,60]) 运行结果： 60 98 100 30 90 > range() 函数:循环输出所有评分指标的下标 # range() 函数:循环输出所有评分指标的下标 def GetValue(score): for lev in range(len(score)): print(lev,end=" ") GetValue(score=["优秀","及格","不及格"]) 运行结果： 0 1 2 ## break 语句：统计优秀学生的个数 # break 语句:不及格的跳过 def GetHighLev(score): result=0 for lev in score: if lev < 90: break else: result += 1 print("成绩优秀的学生有："+str(result)+"位。") GetHighLev(score=[90,30,100,98,60]) 运行结果： 成绩优秀的学生有：1位。 分析：实际优秀者个数是90,100,98共计3位，输出结果却是1位。造成这种结果的原因是，当循环输入列表90时候，满足条件自动加1.继续输入30，不满足条件，直接跳出整个程序，输出最后一条语句。 ## continue 语句：统计优秀成绩的个数 # continue 语句:统计优秀成绩的个数 def GetHighLev2(score): result=0 for lev in score: if lev < 90: continue else: result += 1 print("成绩优秀的学生有："+str(result)+"位。") GetHighLev2(score=[90,30,100,98,60]) 运行结果： 成绩优秀的学生有：3位。 > pass 语句：什么也不做，占位符 # pass 语句：什么也不做，占位符 def callPass(): pass print(callPass()) 运行结果： None *** # 函数 ## 定义函数：斐波那契数列 # 输出指定数的斐波那契数列 def fib(n): a, b = 0, 1 while a < n: print(a, end=' ') a, b = b, a+b print() fib(100) 运行结果： 0 1 1 2 3 5 8 13 21 34 55 89 结果分析： 关键字 def 引入了一个函数定义。在其后必须跟有函数名和包括形式参数的圆括号。函数体语句从下一行开始，必须是缩进的。一个函数定义会在当前符号表内引入函数名。函数名指代的值（即函数体）有一个被 Python 解释器认定为用户自定义函数的类型。 ## 函数默认参数 # 函数默认参数 def sayhello(name="Tom"): print("Hello,"+name) sayhello() 运行结果： Hello,Tom ## 关键字参数 # 关键字参数 def person(name, age, **kw): #前两个是必须参数，最后一个可选可变参数 print('name:', name, 'age:', age, 'other:', kw) person("Tome",30) # 只调用必须参数 person("Tom",30,city="ChengDu",sex="man") # 自定义关键字参数 运行结果： name: Tome age: 30 other: {} name: Tom age: 30 other: {'sex': 'man', 'city': 'ChengDu'} ## 可变参数 # 可变参数 def concat(*args, sep="/"): print(sep.join(args)) concat('我','是','可变','参数') 运行结果： 我/是/可变/参数 ## Lambda 形式 # Lambda 形式 def Lambda(nums): nums.sort(key=lambda num: num[0]) print(nums) Lambda(nums = [(1, 'one'), (2, 'two'), (3, 'three'), (4, 'four')]) 运行结果： [(1, 'one'), (2, 'two'), (3, 'three'), (4, 'four')] *** # List 列表 ## list常用方法 list.append(x) 添加元素到列表尾部。 list.extend(L) 列表合并。 list.insert(i, x) 在指定位置插入一个元素。 list.remove(x) 删除列表中值为 x 的第一个元素。 list.pop([i]) 从列表的指定位置删除元素，并将其返回。 list.clear() 从列表中删除所有元素。相当于 del a[:]。 list.index(x) 返回列表中第一个值为 x 的元素的索引。 list.count(x) 返回 x 在列表中出现的次数。 list.sort() 对列表中的元素就地进行排序。 list.reverse() 就地倒排列表中的元素。 list.copy() 返回列表的一个浅拷贝。等同于 a[:]。 列表的切分 # 列表的切分 def calllist(names): print(names[-1:]) # 输出列表最后一个值 print(names[:3]) # 输出列表前3个值 calllist(names=['this','is','a','list']) 运行结果： [&#39;list&#39;] [&#39;this&#39;, &#39;is&#39;, &#39;a&#39;] List :列表作堆栈(先进先出) # 把列表当作堆栈使用 def SomeList(stack): print("原始列表（栈）：",end=' ') print(stack) stack.append('贺知章') stack.append('杜牧') print("追加后列表（栈）：",end=' ') print(stack) stack.pop() stack.pop() print("出栈后的数据：",end=' ') print(stack) SomeList(stack=['李白','杜甫', '白居易']) 运行结果：(先进先出) 原始列表（栈）： [&#39;李白&#39;, &#39;杜甫&#39;, &#39;白居易&#39;] 追加后列表（栈）： [&#39;李白&#39;, &#39;杜甫&#39;, &#39;白居易&#39;, &#39;贺知章&#39;, &#39;杜牧&#39;] 出栈后的数据： [&#39;李白&#39;, &#39;杜甫&#39;, &#39;白居易&#39;] List :列表作队列(先进后出) # 把列表当作队列使用：使用队列时调用collections.deque，它为在首尾两端快速插入和删除而设计。 from collections import deque def SomeList2(queue): print("原始列表：",end=' ') print(queue) queue.append("李商隐") queue.append("杜牧") print("入队的列表：",end=' ') print(queue) queue.popleft() queue.popleft() print("出队后列表：",end=' ') print(queue) SomeList2(queue = deque(['李白','杜甫', '白居易'])) 运行结果：(先进后出) 原始列表： deque([&#39;李白&#39;, &#39;杜甫&#39;, &#39;白居易&#39;]) 入队的列表： deque([&#39;李白&#39;, &#39;杜甫&#39;, &#39;白居易&#39;, &#39;李商隐&#39;, &#39;杜牧&#39;]) 出队后列表： deque([&#39;白居易&#39;, &#39;李商隐&#39;, &#39;杜牧&#39;]) 列表推导式 # 列表推导式 def callList(nums): squares = [n**2 for n in nums] print(squares) callList(nums=[2,4,6,8]) 运行结果： [4, 16, 36, 64] 列表的矩阵转秩 # 矩阵转秩 def countList(matrix): result = [[row[i] for row in matrix] for i in range(4)] print(result) matrix = [ [1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], ] countList(matrix) 运行结果： [[1, 5, 9], [2, 6, 10], [3, 7, 11], [4, 8, 12]] 元组/集合/字典虽然元组和列表很类似，它们经常被用来在不同的情况和不同的用途。元组有很多用途。例如 (x, y) 坐标对，数据库中的员工记录等等。元组就像字符串， 不可变的。列表是可变的 ，它们的元素通常是相同类型的并通过迭代访问。 操作元组 def calltuple(tuples): for t in tuples: print(t+"\t",end=" ") print() calltuple(tuples=('百度','阿里巴巴','腾讯')) # 元组参数 运行结果： 百度 阿里巴巴 腾讯 set集合集合是一个无序不重复元素的集。基本功能包括关系测试和消除重复元素。集合对象还支持 union（联合），intersection（交），difference（差）和 sysmmetric difference（对称差集）等数学运算。 def callset(basket): result= set(basket) print(result) callset(basket = {'apple', 'orange', 'apple', 'pear', 'orange', 'banana'}) 运行结果： {&#39;pear&#39;, &#39;orange&#39;, &#39;apple&#39;, &#39;banana&#39;} 字典理解字典的最佳方式是把它看做无序的键：值对（key:value 对）集合，键必须是互不相同的（在同一个字典之内）。一对大括号创建一个空的字典： {} 。初始化列表时，在大括号内放置一组逗号分隔的键：值对，这也是字典输出的方式。 def calldict(dicts): print("原始字典",end=' ') print(dicts) # 原始字典 dicts['欧阳修'] = '宋朝' # 追加字典 print("追加后的字典",end=' ') print(dicts) # 追加后的字典 print("字典键的集合",end=' ') print(list(dicts.keys())) # 字典键的集合 print("字典键的排序",end=' ') print(sorted(dicts.keys())) # 字典键的排序 print("字典值的集合",end=' ') print(list(dicts.values())) # 字典值的集合 calldict(dicts = {'李白': '唐朝', '杜甫': '唐朝'}) 运行结果： 原始字典 {&#39;杜甫&#39;: &#39;唐朝&#39;, &#39;李白&#39;: &#39;唐朝&#39;} 追加后的字典 {&#39;杜甫&#39;: &#39;唐朝&#39;, &#39;李白&#39;: &#39;唐朝&#39;, &#39;欧阳修&#39;: &#39;宋朝&#39;} 字典键的集合 [&#39;杜甫&#39;, &#39;李白&#39;, &#39;欧阳修&#39;] 字典键的排序 [&#39;李白&#39;, &#39;杜甫&#39;, &#39;欧阳修&#39;] 字典值的集合 [&#39;唐朝&#39;, &#39;唐朝&#39;, &#39;宋朝&#39;] 面向对象编程：类通过案例理解python中的类：父类是动物类，有初始化函数，且有动物讲话的方法，子类是一个狗类，继承父类所有属性，并扩展自己方法，调用子类讲话方法，并直接调用父类讲话方法。 """ # 欢迎进入我的主页：http://www.cnblogs.com/baiboy/. """ class BaseAnimal: # 父类：动物 def __init__(self,name,age): # 初始化方法：括号里是形参 self.name=name self.age=age def speak(self): # 父类的行为方法 print("我的名字是[ %s ],今年[ %d ]岁" %(self.name,self.age)) class SubDog(BaseAnimal): # 子类：小狗 def __init__(self,name,age,say): # 初始化方法：括号里是形参 BaseAnimal.__init__(self,name,age) self.say=say print("这是子类[ %s ]."%(self.name)) print('_'*20+'调用子函数方法'+'_'*20) def talk(self): # 子类的行为方法 # BaseAnimal.speak(self) # 调用父类的行为方法 print("我的名字是[ %s ],今年[ %d ]岁,我想说： %s" %(self.name,self.age,self.say)) ani=SubDog('dog',12,'汪汪...') print(ani.talk()) print('_'*20+'直接调用父函数方法'+'_'*20) BaseAnimal('tom',13).speak() 运行结果： 这是子类[ dog ]. ____________________调用子函数方法____________________ 我的名字是[ dog ],今年[ 12 ]岁,我想说： 汪汪... ____________________直接调用父函数方法____________________ 我的名字是[ tom ],今年[ 13 ]岁 解析： init 方法（双下划线）： init 方法在类的一个对象被建立时，马上运行。这个方法可以用来对你的对象初始化。注意，这个名称的开始和结尾都是双下划线。我们把init方法定义为取一个参数name（以及普通的参数self）。在 init 方法里，我们只是创建一个新的域，也称为name。注意:它们是两个不同的变量，尽管它们有相同的名字。点号使我们能够区分它们。最重要的是，在创建一个类的新实例的时候，把参数包括在圆括号内跟在类名后面，从而传递给 init 方法。这是这种方法的重要之处。现在，我们能够在我们的方法中使用self.name域。给C++/Java/C#程序员的注释 init 方法类似于C++、C#和Java中的constructor 。 标准库Python拥有一个强大的标准库。Python语言的核心只包含数字、字符串、列表、字典、文件等常见类型和函数，而由Python标准库提供了系统管理、网络通信、文本处理、数据库接口、图形系统、XML处理等额外的功能。Python标准库的主要功能有： 文本处理，包含文本格式化、正则表达式匹配、文本差异计算与合并、Unicode支持，二进制数据处理等功能 文件处理，包含文件操作、创建临时文件、文件压缩与归档、操作配置文件等功能 操作系统功能，包含线程与进程支持、IO复用、日期与时间处理、调用系统函数、日志（logging）等功能 网络通信，包含网络套接字，SSL加密通信、异步网络通信等功能 网络协议，支持HTTP，FTP，SMTP，POP，IMAP，NNTP，XMLRPC等多种网络协议，并提供了编写网络服务器的框架 W3C格式支持，包含HTML，SGML，XML的处理。 其它功能，包括国际化支持、数学运算、HASH、Tkinter等 Python社区提供了大量的第三方模块，使用方式与标准库类似。它们的功能覆盖科学计算、Web开发、数据库接口、图形系统多个领域。第三方模块可以使用Python或者C语言编写。SWIG,SIP常用于将C语言编写的程序库转化为Python模块。Boost C++ Libraries包含了一组库，Boost.Python，使得以Python或C++编写的程序能互相调用。Python常被用做其他语言与工具之间的“胶水”语言。 Python第三方库Web框架库 Django： 开源Web开发框架，它鼓励快速开发,并遵循MVC设计，开发周期短。 Flask： 轻量级的Web框架。 Pyramid： 轻量，同时有可以规模化的Web框架，Pylon projects 的一部分。 ActiveGrid： 企业级的Web2.0解决方案。 Karrigell： 简单的Web框架，自身包含了Web服务，py脚本引擎和纯python的数据库PyDBLite。 Tornado： 一个轻量级的Web框架，内置非阻塞式服务器，而且速度相当快 webpy： 一个小巧灵活的Web框架，虽然简单但是功能强大。 CherryPy： 基于Python的Web应用程序开发框架。 Pylons： 基于Python的一个极其高效和可靠的Web开发框架。 Zope： 开源的Web应用服务器。 TurboGears： 基于Python的MVC风格的Web应用程序框架。 Twisted： 流行的网络编程库，大型Web框架。 Quixote： Web开发框架。科学计算库 Matplotlib： 用Python实现的类matlab的第三方库，用以绘制一些高质量的数学二维图形。 Pandas： 用于数据分析、数据建模、数据可视化的第三方库。 SciPy： 基于Python的matlab实现，旨在实现matlab的所有功能。 NumPy： 基于Python的科学计算第三方库，提供了矩阵，线性代数，傅立叶变换等等的解决方案。GUI库 PyGtk： 基于Python的GUI程序开发GTK+库。 PyQt： 用于Python的QT开发库。 WxPython： Python下的GUI编程框架，与MFC的架构相似。其他库 BeautifulSoup： 基于Python的HTML/XML解析器，简单易用。 gevent： python的一个高性能并发框架,使用了epoll事件监听、协程等机制将异步调用封装为同步调用。 PIL： 基于Python的图像处理库，功能强大，对图形文件的格式支持广泛。目前已无维护，另一个第三方库Pillow实现了对PIL库的支持和维护。 PyGame： 基于Python的多媒体开发和游戏软件开发模块。 Py2exe： 将python脚本转换为windows上可以独立运行的可执行程序。 Requests： 适合于人类使用的HTTP库，封装了许多繁琐的HTTP功能，极大地简化了HTTP请求所需要的代码量。 scikit-learn： 机器学习第三方库，实现许多知名的机器学习算法。 TensorFlow： Google开发维护的开源机器学习库。 Keras： 基于TensorFlow，Theano与CNTK的高级神经网络API。 SQLAlchemy： 关系型数据库的对象关系映射(ORM)工具 参考文献 Python官网 中文维基百科 GitHub 图书：《机器学习实战》 图书：《自然语言处理理论与实战》 完整代码下载 源码请进【机器学习和自然语言QQ群：436303759】文件下载： 作者声明 本文版权归作者所有，旨在技术交流使用。未经作者同意禁止转载，转载后需在文章页面明显位置给出原文连接，否则相关责任自行承担。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简明的Python教程之概述篇]]></title>
    <url>%2F2018%2F10%2F10%2F%E7%AE%80%E6%98%8E%E7%9A%84Python%E6%95%99%E7%A8%8B%E4%B9%8B%E6%A6%82%E8%BF%B0%E7%AF%87%2F</url>
    <content type="text"><![CDATA[摘要：Python作为一门简洁优美且功能强大的语言，越来越受编程人员的喜欢。在工业界和学术界也是非常受欢迎的编程语言。python语言可以跨平台跨应用开发。本系列文章首先介绍Python语言及其可以做什么事情。哪些人群适合学习python和python语法特点。其次介绍了Python进阶，以实际案例演示常用的语句和控制流、表达式、函数、数据结构、标准库等知识点。然后扩展介绍了python第三方库，使读者对python有个全面的理解和认识。最后一节，采用实际案例帮助读者综合运用python知识。 （本文原创，转载必须注明出处.） Python概述Python介绍Python是一种面向对象、直译式的计算机程序语言。它包含了一组功能完备的标准库，能够轻松完成很多常见的任务。它的语法简单，与其它大多数程序设计语言使用大括号不一样，它使用缩进来定义语句块。Python同样是一种动态语言，具备垃圾回收功能，能够自动管理内存使用。它经常被当作脚本语言用于处理系统管理任务和网络程序编写，然而它也非常适合完成各种高级任务。Python支持命令式程序设计、面向对象程序设计、函数式编程、面向侧面的程序设计、泛型编程多种编程范式。Python是完全面向对象的语言。函数、模块、数字、字符串都是对象。并且完全支持继承、重载、派生、多重继承，有益于增强源代码的复用性。Python支持重载运算符，因此Python也支持泛型设计。 Python发展历史Python的创始人是Guido van Rossum。1989年的圣诞节期间，Guido van Rossum为了在阿姆斯特丹打发时间，决心开发一个新的脚本解释程序，作为ABC语言的一种继承。之所以选中Python作为程序的名字，是因为他是BBC电视剧——蒙提·派森的飞行马戏团（Monty Python’s Flying Circus）的爱好者。ABC是由Guido参加设计的一种教学语言。就Guido本人看来，ABC这种语言非常优美和强大，是专门为非专业程序员设计的。但是ABC语言并没有成功，究其原因，Guido认为是非开放造成的。吉多决心在Python中避免这一错误，并获取了非常好的效果，完美结合了C和其他一些语言。就这样，Python在吉多手中诞生了。目前Guido仍然是Python的主要开发者，决定整个Python语言的发展方向。Python社区经常称呼他是仁慈的独裁者。 Python标准库的主要功能 文本处理，包含文本格式化、正则表达式匹配、文本差异计算与合并、Unicode支持，二进制数据处理等功能。 文件处理，包含文件操作、创建临时文件、文件压缩与归档、操作配置文件等功能。 操作系统功能，包含线程与进程支持、IO复用、日期与时间处理、调用系统函数、日志（logging）等功能。 网络通信，包含网络套接字，SSL加密通信、异步网络通信等功能。 网络协议，支持HTTP，FTP，SMTP，POP，IMAP，NNTP，XMLRPC等多种网络协议，并提供了编写网络服务器的框架。 W3C格式支持，包含HTML，SGML，XML的处理。 其它功能，包括国际化支持、数学运算、HASH、Tkinter等。 Python优缺点 优点 简单、易学、免费、开源、高层语言（无需考虑如何管理你的程序使用的内存等细节。 可移植性（这些平台包括Linux、Windows、FreeBSD、Macintosh、Solaris、OS/2、Amiga、AROS、AS/400、BeOS、OS/390、z/OS、Palm OS、QNX、VMS、Psion、Acom RISC OS、VxWorks、PlayStation、Sharp Zaurus、Windows CE、PocketPC、Symbian以及Google基于linux开发的android平台 解释性、面向对象、可扩展性（可以部分程序用C或C++编写，然后在Python程序中使用它们。 可嵌入性（可以把Python嵌入C/C++程序，从而向程序用户提供脚本功能。）丰富的库、规范的代码。 缺点 单行语句和命令行输出问题、独特的语法、运行速度慢（与C和C++相比。 Python开发环境通用IDE / 文本编辑器，很多并非集成开发环境软件的文本编辑器，也对Python有不同程度的支持。本文默认开发环境均集成在Anaconda中，第1章已经详细介绍过。此外，还有如下开发环境： Eclipse + pydev插件，目前对Python 3.X只支持到3.0 emacs +插件 NetBeans +插件 SlickEdit TextMate Python Tools for Visual Studio Vim +插件 Sublime Text +插件 EditPlus UltraEdit PSPad Editra由Python开发的程序编辑器。 Notepad++ Python能做什么？Python能做什么 系统编程：提供API，能方便进行系统维护和管理，很多系统管理员理想的编程工具 。 图形处理：有PIL、Tkinter等图形库支持，能方便进行图形处理。 数学处理：NumPy扩展提供大量与许多标准数学库的接口。 文本处理：python提供的re模块能支持正则表达式，还提供SGML，XML分析模块，许多程序员利用python进行XML程序的开发。 数据库编程：程序员可通过遵循Python DB-API（数据库应用程序编程接口）规范的模块与Microsoft SQL Server，Oracle，Sybase，DB2，MySQL、SQLite等数据库通信。python自带有一个Gadfly模块，提供了一个完整的SQL环境。 网络编程：提供丰富的模块支持sockets编程，能方便快速地开发分布式应用程序。 Web编程：应用的开发语言，支持最新的XML技术。 多媒体应用：能进行二维和三维图像处理。PyGame模块可用于编写游戏软件。 黑客编程：python有一个hack的库,内置你熟悉的或不熟悉的函数，但是缺少成就感。 Python开发的应用案例 Reddit - 社交分享网站 Dropbox - 文件分享服务 豆瓣网 - 图书、唱片、电影等文化产品的资料数据库网站 Django - 鼓励快速开发的Web应用框架 Pylons - Web应用框架 Zope - 应用服务器 Plone - 内容管理系统 TurboGears - 另一个Web应用快速开发框架 Twisted - Python的网络应用程序框架 Fabric - 用于管理成百上千台Linux主机的程序库 Python Wikipedia Robot Framework - MediaWiki的机器人程序 MoinMoinWiki - Python写成的Wiki程序 Trac - 使用Python编写的BUG管理系统 Mailman - 使用Python编写的邮件列表软件 Mezzanine - 基于Django编写的内容管理系统系统 flask - Python微Web框架 Webpy - Python微Web框架 Bottle - Python微Web框架 EVE - 网络游戏EVE大量使用Python进行开发 Blender - 使用Python作为建模工具与GUI语言的开源3D绘图软件 Inkscape - 一个开源的SVG矢量图形编辑器。 知乎 - 一个问答网站 果壳 - 一个泛科技主题网站 Python适合谁去学？ 知乎精选： 于这个问题，我先带着大家去知乎看看，大家感兴趣可以去知乎搜索下，基本上语调是一致的。笔者本人而言，本科主要net技术研究，在C#学习上花费很多精力和时间。后来读研初期又开始java方面学习。虽说技不压身，但是总是因为研究方向的客观变化去转战于不同语言之间，外加语言环境平台还是浪费了不少时间的，且均没有深入下去。反之，python的跨平台性就优势凸显了，你习惯Linux命令行，完全可以适应。接近伪代码的操作为你节省不少时间，特别在文本处理，自然语言分析方面，笔者之前用java编写，耗费一番功夫。总而言之，园子里面，多数同学为本科在读生，在拥有一门入门语言的情况下，研究下python我觉得是值得的，也是大的趋势。无论你做运维或者web开发，亦或算法研究，大数据分析。前天与一家大数据公司技术负责人聊天，他们产品全是python，从文本处理到数据清洗分析，直到模型构建结果评价。读者也可以看看：我爱自然语言处理社区，里面无论求职数据挖掘、自然语言处理、机器学习等均要求python经验。 Python语法和特点 编码 默认情况下，Python 3 源码文件以 UTF-8 编码，所有字符串都是 unicode 字符串。 当然你也可以为源码文件指定不同的编码： # -*- coding: cp-1252 -*- 标识符： 在python里，标识符有字母、数字、下划线组成。在python中，所有标识符可以包括英文、数字以及下划线（），但不能以数字开头。python中的标识符是区分大小写的。以下划线开头的标识符是有特殊意义的。以单下划线开头（foo）的代表不能直接访问的类属性，需通过类提供的接口进行访问，不能用”from xxx import *”而导入.以双下划线开头的（foo）代表类的私有成员；以双下划线开头和结尾的（foo）代表python里特殊方法专用的标识。 python保留字 保留字即关键字，我们不能把它们用作任何标识符名称。 &gt;&gt;&gt; import keyword &gt;&gt;&gt; keyword.kwlist [&#39;False&#39;, &#39;None&#39;, &#39;True&#39;, &#39;and&#39;, &#39;as&#39;, &#39;assert&#39;, &#39;break&#39;, &#39;class&#39;, &#39;continue&#39;, &#39;def&#39;, &#39;del&#39;, &#39;elif&#39;, &#39;else&#39;, &#39;except&#39;, &#39;finally&#39;, &#39;for&#39;, &#39;from&#39;, &#39;global&#39;, &#39;if&#39;, &#39;import&#39;, &#39;in&#39;, &#39;is&#39;, &#39;lambda&#39;, &#39;nonlocal&#39;, &#39;not&#39;, &#39;or&#39;, &#39;pass&#39;, &#39;raise&#39;, &#39;return&#39;, &#39;try&#39;, &#39;while&#39;, &#39;with&#39;, &#39;yield&#39;] 注释 Python中单行注释以 # 开头，实例如下： #!/usr/bin/python3 print (&quot;Hello, Python!&quot;) # 第二个注释 行与缩进 python最具特色的就是使用缩进来表示代码块，不需要使用大括号({})。四个空格或者Tab进行缩进。 if True: print (“True”) else: print (“False”) 多行语句：Python 通常是一行写完一条语句，但如果语句很长，我们可以使用反斜杠()来实现多行语句，例如： total = item_one + \ item_two + \ item_three 在 [], {}, 或 () 中的多行语句，不需要使用反斜杠()，例如： total = [&#39;item_one&#39;, &#39;item_two&#39;, &#39;item_three&#39;, &#39;item_four&#39;, &#39;item_five&#39;] 引号 Python 接收单引号(‘ )，双引号(“ )，三引号(‘’’ “””) 来表示字符串，引号的开始与结束必须的相同类型的。 word = &#39;word&#39; sentence = &quot;这是一个句子。&quot; paragraph = &quot;&quot;&quot;这是一个段落。 包含了多个语句&quot;&quot;&quot; 字符串 python中单引号和双引号使用完全相同。使用三引号(‘’’或”””)可以指定一个多行字符串。转义符 ‘\’自然字符串， 通过在字符串前加r或R。 如 r”this is a line with \n” 则\n会显示，并不是换行。python允许处理unicode字符串，加前缀u或U， 如 u”this is an unicode string”。字符串是不可变的。按字面意义级联字符串，如”this “ “is “ “string”会被自动转换为this is string。 空行 函数之间或类的方法之间用空行分隔，表示一段新的代码的开始。类和函数入口之间也用一行空行分隔，以突出函数入口的开始。 Print 输出 print 默认输出是换行的，如果要实现不换行需要在变量末尾加上 end=””： #!/usr/bin/python3 x="a" y="b" # 换行输出 print( x ) print( y ) print('---------') # 不换行输出 print( x, end=" " ) print( y, end=" " ) print() 运行结果： a b --------- a b import 与 from…import 在 python 用 import 或者 from…import 来导入相应的模块。 (1) 将整个模块(somemodule)导入，格式为： import somemodule (2) 从某个模块中导入某个函数,格式为： from somemodule import somefunction (3) 从某个模块中导入多个函数,格式为： from somemodule import firstfunc, secondfunc, thirdfunc (4) 将某个模块中的全部函数导入，格式为： from somemodule import * 导入 sys 模块 import sys print('================Python import mode=========================='); print ('命令行参数为:') for i in sys.argv: print (i) print ('\n python 路径为',sys.path) 导入 sys 模块的 argv,path 成员 from sys import argv,path # 导入特定的成员 print('================python from import===================================') print('path:',path) # 因为已经导入path成员，所以此处引用时不需要加sys.path 数据类型 参考文献 Python官网 中文维基百科 GitHub 图书：《机器学习实战》 图书：《自然语言处理理论与实战》 完整代码下载 源码请进【机器学习和自然语言QQ群：436303759】文件下载： 作者声明 本文版权归作者所有，旨在技术交流使用。未经作者同意禁止转载，转载后需在文章页面明显位置给出原文连接，否则相关责任自行承担。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[谈谈自然语言处理技术的应用领域]]></title>
    <url>%2F2018%2F10%2F09%2F%E8%B0%88%E8%B0%88%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF%E7%9A%84%E5%BA%94%E7%94%A8%E9%A2%86%E5%9F%9F%2F</url>
    <content type="text"><![CDATA[摘要：通过《十分钟速览自然语言处理》一文，读者对自然语言基本概况进行了解。那么自然语言处理过程中总不是那么一帆风顺的，针对自然语言处理的任务和限制将在本文第一部分进行介绍。很多初学者发出疑问即自然语言处理能干什么？接下来我们回答这类问题，概要介绍自然语言处理所涉及的主要技术范畴，其中包括文本分类、问答系统、情感分析、中文分词等等。最后，我们介绍相关研究内容。（本文原创，转载必须注明出处.） 自然语言处理的任务和限制 任务和限制 NLP是一种很吸引人的人机交互方式。早期的语言处理系统如SHRDLU，当它们处于一个有限的“积木世界”，运用有限的词汇表会话时，工作得相当好。这使得研究员们对此系统相当乐观，然而，当把这个系统拓展到充满了现实世界的含糊与不确定性的环境中时，他们很快丧失了信心。由于理解自然语言，需要关于外在世界的广泛知识以及运用操作这些知识的能力，自然语言认知，同时也被视为一个人工智能完备的问题。在自然语言处理中，”理解”的定义也变成一个主要的问题。 一些NLP面临的问题实例： 句子“我们把香蕉给猴子，因为(它们)饿了”和“我们把香蕉给猴子，因为(它们)熟透了”有同样的结构。但是代词“它们”在第一句中指的是“猴子”，在第二句中指的是“香蕉”。如果不了解猴子和香蕉的属性，无法区分。(英文的it没有区分，但在中文里“它”和“它”是有区别的，只是代词在中文里常常被省略，因此需区别属性并且标示出来) 自然语言处理的主要技术范畴语音合成 语音合成（Speech synthesis）/文本朗读（Text to speech） 语音合成是将人类语音用人工的方式所产生。若是将电脑系统用在语音合成上，则称为语音合成器，而语音合成器可以用软/硬件所实现。文字转语音（text-to-speech，TTS）系统则是将一般语言的文字转换为语音，其他的系统可以描绘语言符号的表示方式，就像音标转换至语音一样。 一个语音合成器的质量通常是决定于人声的相似度以及语义是否能被了解。一个清晰的文字转语音程序应该提供人类在视觉受到伤害或是得到失读症时，能够听到并且在个人电脑上完成工作。从80年代早期开始，许多的电脑操作系统已经包含了语音合成器了。语音合成的应用包括：智能仪表、智能玩具、电子地图、电子导游、电子词典等。 语音识别 语音识别（Speech recognition） 语音识别（speech recognition）技术，也被称为语音转文本识别（英语：Speech To Text, STT），其目标是以电脑自动将人类的语音内容转换为相应的文字。与说话人识别及说话人确认不同，后者尝试识别或确认发出语音的说话人而非其中所包含的词汇内容。 语音识别技术的应用包括语音拨号、语音导航、室内设备控制、语音文档检索、简单的听写数据录入等。语音识别技术与其他自然语言处理技术如机器翻译及语音合成技术相结合，可以构建出更加复杂的应用，例如语音到语音的翻译。语音识别技术所涉及的领域包括：信号处理、模式识别、概率论和信息论、发声机理和听觉机理、人工智能等等。 中文自动分词 中文自动分词（Chinese word segmentation） 中文自动分词指的是使用计算机自动对中文文本进行词语的切分，即像英文那样使得中文句子中的词之间有空格以标识。中文自动分词被认为是中文自然语言处理中的一个最基本的环节。 现有方法包括： 基于词典的匹配：前向最大匹配、后向最大匹配。 基于字的标注：最大熵模型、条件随机场模型、感知器模型。 其它方法：与词性标注结合、与句法分析结合。 常用中文分词 中文文本词与词之间没有像英文那样有空格分隔，因此很多时候中文文本操作都涉及切词，这里整理了一些中文分词工具。 结巴分词（个人推荐，基于Python开发的） StanfordNLP汉语分词工具 哈工大语言云 庖丁解牛分词 盘古分词 ICTCLAS（中科院）汉语词法分析系统 IKAnalyzer（Luence项目下，基于java的） FudanNLP(复旦大学) 词性标注 词性标注（Part-of-speech tagging） 词性标注（Part-of-Speech tagging 或POS tagging)，又称词类标注或者简称标注，是指在词性标记集已确定，并且词典中每个词都有确定词性的基础上，对一个输入词串转换成相应词性标记串的过程叫做词性标注。 在汉语中，因为汉语词汇词性多变的情况比较少见，大多词语只有一个词性，或者出现频次最高的词性远远高于第二位的词性，相对比较简单。同时，它也受到一些条件约束。比如：兼类词在具体语境中的词性判定问题、未登录词即新词词性问题、兼类词问题等。词性标注方法包括：概率方法、隐马尔可夫模型的词性标注方法、机器学习规则的方法等 词性标注原理 原理描述：标注一篇文章中的句子，即语句标注，使用标注方法BIO标注或者BIEO标注（B代表开始，I中间相关字词，E代表结束，O代表其他词）。则观察序列X就是一个语料库（此处假设一篇文章，seg代表文章中的每一句，SEG是seg的集合），标识序列Y是BIO，即对应SEG序列的识别，从而可以根据条件概率P(标注|句子)，推测出正确的句子标注。 显然，这里针对的是序列状态，即CRF是用来标注或划分序列结构数据的概率化结构模型，CRF可以看作无向图模型或者马尔科夫随机场。 用过CRF的都知道，CRF是一个序列标注模型，指的是把一个词序列的每个词打上一个标记。一般通过，在词的左右开一个小窗口，根据窗口里面的词，和待标注词语来实现特征模板的提取。最后通过特征的组合决定需要打的tag是什么。 句法分析 句法分析（Parsing） 句法分析(Parsing)就是指对句子中的词语语法功能进行分析。比如“我来晚了”，这里“我”是主语，“来”是谓语，“晚了”是补语。 句法分析在中文信息处理中的主要应用包括：如机器翻译、命名实体识别等。 自然语言生成（Natural language generation） 自然语言生成是研究使计算机具有人一样的表达和写作的功能。即能够根据一些关键信息及其在机器内部的表达形式，经过一个规划过程，来自动生成一段高质量的自然语言文本。自然语言处理包括自然语言理解和自然语言生成。自然语言生成是人工智能和计算语言学的分支,相应的语言生成系统是基于语言信息处理的计算机模型,其工作过程与自然语言分析相反,是从抽象的概念层次开始,通过选择并执行一定的语义和语法规则来生成文本。 文本挖掘文本挖掘是信息挖掘的一个研究分支，用于基于文本信息的知识发现。文本挖掘的准备工作由文本收集、文本分析和特征修剪三个步骤组成。目前研究和应用最多的几种文本挖掘技术有：文档聚类、文档分类和摘要抽取。 文本分类 文本分类（Text categorization） 文本分类用电脑对文本集按照一定的分类器模型进行自动分类标记。文本分类的总体过程如下： 预处理：将原始语料格式化为同一格式，便于后续的统一处理； 索引：将文档分解为基本处理单元，同时降低后续处理的开销； 统计：词频统计，项（单词、概念）与分类的相关概率； 特征抽取：从文档中抽取出反映文档主题的特征； 分类器：分类器的训练； 评价：分类器的测试结果分析。 文本分类常用算法包括：决策树、朴素贝叶斯、神经网络、支持向量机、线性最小平方拟合、kNN、遗传算法、最大熵等。广泛应用于垃圾过滤，新闻分类，词性标注等。 文本挖掘 文本挖掘 文本挖掘一般指文本处理过程中产生高质量的信息。高质量的信息通常通过分类和预测来产生，如模式识别。文本挖掘通常涉及输入文本的处理过程，产生结构化数据，并最终评价和解释输出。 典型的文本挖掘方法包括文本分类、文本聚类、信息抽取、概念/实体挖掘、情感分析和观点分析等。 命名实体识别命名实体识别常见三种主流算法即基于CRF，字典法和混合的方法 CRF：在CRF for Chinese NER这个任务中，提取的特征大多是该词是否为中国人名姓氏用字，该词是否为中国人名名字用字之类的，True or false的特征。所以一个可靠的百家姓的表就十分重要啦~在国内学者做的诸多实验中，效果最好的人名可以F1测度达到90%，最差的机构名达到85%。 字典法：在NER中就是把每个字都当开头的字放到trie-tree中查一遍，查到了就是NE。中文的trie-tree需要进行哈希，因为中文字符太多了，不像英文就26个。 对六类不同的命名实体采取不一样的手段进行处理，例如对于人名，进行字级别的条件概率计算。 中文：哈工大（语言云）上海交大 英文：stanfordNER等。 信息抽取 信息抽取（Information extraction） 信息抽取（Information Extraction）主要是从大量文字数据中自动抽取特定消息作为数据库访问之用的技术。 简单可以理解为从给定文本中抽取重要的信息，比如，时间、地点、人物、事件、原因、结果、数字、日期、货币、专有名词等等。通俗说来，就是要了解谁在什么时候、什么原因、对谁、做了什么事、有什么结果。涉及到实体识别、时间抽取、因果关系抽取等关键技术。 问答系统 问答系统（Question answering） 问答系统（英语：Question answering），是，当下自然语言处理研究的热点，未来自然语言处理的明日之星。问答系统外部的行为上来看，其与目前主流资讯检索技术有两点不同：首先是查询方式为完整而口语化的问句，再来则是其回传的为高精准度网页结果或明确的答案字串。 以Ask Jeeves为例，使用者不需要思考该使用什么样的问法才能够得到理想的答案，只需要用口语化的方式直接提问如“请问谁是美国总统？”即可。而系统在了解使用者问句后，会非常清楚地回答“特朗普是美国总统”。从系统内部来看，问答系统使用了大量有别于传统资讯检索系统自然语言处理技术，如自然语言剖析（Natural Language Parsing）、问题分类（Question Classification）、专名辨识（Named Entity Recognition）等等。 机器翻译 机器翻译（Machine translation） 机器翻译（英语：Machine Translation，经常简写为MT）属于计算语言学的范畴，其研究借由计算机程序将文字或演说从一种自然语言翻译成另一种自然语言。 简单来说，机器翻译是通过将一个自然语言的字辞取代成另一个语言的字辞。借由使用语料库的技术，可达成更加复杂的自动翻译，包含可更佳的处理不同的文法结构、辞汇辨识、惯用语的对应等。一般而言，大众使用机器翻译的目的只是为了获知原文句子或段落的要旨，而不是精确的翻译。总的来说，机器翻译的效果并没有达到可以取代人工翻译的程度，所以无法成为正式的翻译。不过现在已有越来越多的公司尝试以机器翻译的技术来提供其公司网站多语系支援的服务。例如微软公司试将其 MSDN 以机器翻译来自动翻译成多国语言，如上文所说，知识库作为专业领域 ，其文法较为制式化，翻译结果亦更加符合自然语言。 情感分析 文本情感分析 文本情感分析（也称为意见挖掘）是指用自然语言处理、文本挖掘以及计算机语言学等方法来识别和提取原素材中的主观信息。 通常来说，情感分析的目的是为了找出说话者/作者在某些话题上或者针对一个文本两极的观点的态度。这个态度或许是他或她的个人判断或是评估，也许是他当时的情感状态（就是说，作者在做出这个言论时的情绪状态），或是作者有意向的情感交流（就是作者想要读者所体验的情绪）。 自动摘要 自动摘要（Automatic summarization） 所谓自动文摘就是利用计算机自动地从原始文献中提取文摘，文摘是全面准确地反映某一文献中心内容地简单连贯的短文。常用方法是自动摘要将文本作为句子的线性序列，将句子视为词的线性序列。 自动摘要可以按照技术类型和信息提取分为如下： 技术应用类型：自动提取给定文章的摘要信息、自动计算文章中词的权重、自动计算文章中句子的权重。 信息提取：单篇文章的摘要自动提取、大规模文档的摘要自动提取、基于分类的摘要自动提取。 文字蕴涵 文字蕴涵（Textual entailment） 文字蕴涵（Textual entailment，TE）在自然语言处理是一个文字片段之间的定向关系。拥有一个文字片段的含意时，可以从另一个文字如下关系。 范例 正向蕴涵 文本T:日本时间2011年3日11日，日本宫城县发生里氏震级9.0强震，造死伤失踪约3万多人。 假设H:日本时间2011年3日11日，日本宫城县发生里氏震级9.0强震。 矛盾蕴涵 文本T:张学友在1961年7月10日，生于香港，祖籍天津。 假设H:张学友生于1960年。 独立蕴涵 文本T:黎姿与”残障富豪”马廷强结婚。 假设H:马廷强为香港”东方报业集团”创办人之一马惜如之子。 自然语言处理的难点语言环境复杂自然语言处理语言环境较为复杂，以命名实体识别进行分析，对于同一个汉字某些情况下可以看作实体处理，某些情况就不能看作实体。例如： 人名，比如《天龙八部》中“婢子四姊妹一胎孪生，童姥姥给婢子取名为梅剑，这三位妹子是兰剑、竹剑、菊剑。”人物“竹剑”，某些情况下就是指的一种竹子做的剑。 地名，比如《射雕英雄传》中“陆庄主知道此人是湖南铁掌帮的帮主”中地点“湖南”，在某种情况下就指代地理方位“湖的那边”。 机构名，比如《鹿鼎记》中“这位是莲花堂香主蔡德忠蔡伯伯。”组织机构名(帮派名)“莲花堂”，在某种情况就指代种植莲花的一个地方，变成地点名了。 文本结构形式多样文本内部结构形式多样。还是以自然语言处理中的命名实体识别任务为例子，诸如： 人名，人名由姓和名构成。其中姓氏包括单姓和复姓(如：赵、钱、孙、李、慕容、东方、西门等)，名由若干个汉字组成。姓氏的用字范围相对有限，比较容易识别。然而名就比较灵活，既可以用名、字、号表示，也可以使用职务名和用典。比如：“李白、李十二、李翰林 、李供奉、李拾遗、李太白、青莲居士，谪仙人”都是同一个人。 地名，一般由若干个字组成地名，可以为作为后缀关键字或者别名(比如：“成都、蓉城、锦城、芙蓉城、锦官城、天府之国”)都是指代一个地方，其中“蓉城、锦城、芙蓉城、锦官城、天府之国”为别名。除了全称的名称之外，还有地理位置代表地名的。比如：“河南、河南省、豫”都是指的一个省份，其中“豫”是简称。 组织机构名，组织机构命名方式比较复杂，有些是修饰性的命名，有些表示历史典故，有些表示地理方位，有些表示地名，有些表示风俗习惯和关键字等等。例如：组织名“广州恒大淘宝足球俱乐部”中，“广州”表示地名的成分，“恒大”“淘宝”表示公司名称成分，“足球”是一项体育赛事成分，“俱乐部”是关键字的成分。比如：“四川大学附属中学”(四川省成都市第十二中学)中包括另一个机构名“四川大学”。机构名还可以以简称形式表示，比如：“四川大学附属中学”简称“川大附中”，“成都信息工程大学”简称“成信大”。 边界识别限制在自然语言处理任务中，边界识别最广泛应用于在命名识别识别当中，边界识别可以分解为两大任务。(1)如何去识别实体的边界？(2)如何去判定实体的类别(诸如：人名、地名、机构名)？中文命名实体识别要比英文命名实体识别更为复杂，一是受中文自身语言特性限制，不同于英语文本中词间有空格界定。二是英文中的实体一般首字母大写容易区分，诸如：‘Jobs was adopted at birth in San Francisco，and raised in a hotbed of counterculture’中，人名乔布斯Jobs的首字母大写，地名旧金山San Francisco首字母也是大写。而中文不具备这样的特征，例如：“周总理忙了一日，早已神困眼倦。”人名“周总理”就很难在一串汉字中识别出来。 词义消歧 词义消歧 词义消歧是一个自然语言处理和本体论的开放问题。歧义与消歧是自然语言理解中最核心的问题，在词义、句义、篇章含义层次都会出现语言根据上下文语义不同的现象，消歧即指根据上下文确定对象语义的过程。词义消歧即在词语层次上的语义消歧。语义消歧/词义消歧 是自然语言处理任务的一个核心与难点，影响了几乎所有任务的性能，比如搜索引擎、意见挖掘、文本理解与产生、推理等。 词性标注与词义消歧 词性标注与词义消歧是相互关联的2个问题，在人的系统他们同时能到满足。但是目前系统一般并不能让2者公用参数，同时输出。语义理解，包括分词、词性标注、词义消歧、句法解析、语义解析 并不是前馈的，是相互依赖的存在反馈的。词性标注与语义消歧都要依赖上下文来标注，但是词性标注比语义消歧要简单以及成功。原因主要是词性标注的标注集合是确定的，而语义消歧并没有，并且量级要大的多；词性标注的上下文依赖比语义消歧要短。 典型例子 许多字词不单只有一个意思，因而我们必须选出使句意最为通顺的解释。看下面歧义的句子：词意消歧就是分析出特定上下文的词被赋予的哪个意思。 川大学生上网成瘾如患绝症。歧义在于“川大学生”（1）四川大学的学生（2）四川的大学生 两代教授，人格不同。歧义：“两代”（1）两位代理教授（2）两个时代的教授 被控私分国有资产，专家总经理成了被告人。歧义：“专家总经理”（1）专家和总经理（2）有专家身份的总经理 新生市场苦熬淡季。歧义：“新生”（1）新学生的市场（2）新产生的市场 朝鲜十年走近国际社会一步。歧义：“十年走近国际社会一步”（1）每十年就向国际社会走近一步（2）最近十年间向国际社会走近了一步 新汽车牌照。歧义：“新”（1）新的汽车（2）新的牌照 咬死了猎人的狗。歧义：（1）猎人的狗被咬死了（2）把猎人咬死了的那条狗 菜不热了。歧义：“热”（1）指菜凉了（2）指菜不加热了 还欠款四万元。歧义：“还”（1）读huai（2）读hai 北京人多。歧义：（1）北京/人多（2）北京人/多 指代消解 定义 指代消解（Anaphora Resolution）是自然语言处理的重要内容，在信息抽取时，就用到了指代消解技术。 中文的三种典型指代 人称代词：李明怕高妈妈一人呆在家里寂寞，【他】便将家里的电视搬了过来。 指示代词：很多人都想创造一个美好的世界留给孩子，【这】可以理解，但不完全正确。 有定描述：贸易制裁似乎成了美国政府在对华关系中惯用的大棒。然而，这【大棒】果真如美国政府所希望的那样灵验吗? 典型指代消解 显性代词消解 所谓显性代词消解，就是指在篇章中确定显性代词指向哪个名词短语的问题，代词称为指示语或照应语（Anaphor），其所指向的名词短语一般被称为先行语（Antecedent），根据二者之间的先后位置，可分为回指（Anaphora）与预指（Cataphora），其中：如果先行语出现在指示语之前，则称为回指，反之则称为预指。 零代词消解 所谓零代词消解，是代词消解中针对零指代（Zero Anaphora）现象的一类特殊的消解。在篇章中，用户能够根据上下文关系推断出的部分经常会省略，而省略的部分（用零代词（Zero Pronoun）表示）在句子中承担着相应的句法成分，并且回指前文中的某个语言学单位。零指代现象在中文中更加常见，（中华语言博大精深。。）近几年随着各大评测任务的兴起开始受到学者们的广泛关注。 共指消解 所谓共指消解，是将篇章中指向同一现实世界客观实体（Entity）的词语划分到同一个等价集的过程，其中被划分的词语称为表述或指称语（Mention），形成的等价集称为共指链（Coreference Chain）。在共指消解中，指称语包含：普通名词、专有名词和代词，因此可以将显性代词消解看作是共指消解针对代词的子问题。共指消解与显性代词消解不同，它更关注在指称语集合上进行的等价划分，评测方法与显性代词消解也不近相同，通常使用MUC、B-CUBED、CEAF和BLANC评价方法。 指代消解的研究方法大致可以分为基于启发式规则的、基于统计的和基于深度学习的方法，目前看来，基于有监督统计机器学习的消解算法仍然是主流算法。 典型例子 指代消解是解决“谁对谁做了 什么”，处理如上所述自然语言的问题，下面看看例子: 美国政府表示仍然支持强势美元，但这到底只是嘴上说说还是要采取果断措施，经济学家对此的看法是否定的。 今天老师又在班会上表扬了自己，但是我觉得还需要继续努力。 三妹拉着葛姐的手说，她老家在偏远的山区，因为和家里赌气才跑到北京打工的，接着她又哭泣起自己的遭遇来。 当他把证书发给小钱时，他对他笑了。 小明和肖华去公园玩,他摔了一跤,他急忙把他扶起来. 星期天,小雨和小英到田老师家补习功课,她一早就打电话给她约好在红旗饭店吃早餐. 场景案例案例1（解决交叉歧义）分词（Word Segmentation） ：将连续的自然语言文本，切分成具有语义合理性和完整性的词汇序列 例句：致毕业和尚未毕业的同学。 分词： 致 毕业 和 尚未 毕业 的 同学 致 毕业 和尚 未 毕业 的 同学 推荐： 校友 和 老师 给 尚未 毕业 同学 的 一 封 信 本科 未 毕业 可以 当 和尚 吗 案例2（从粒度整合未登录体词）命名实体识别（Named Entity Recognition）：识别自然语言文本中具有特定意义的实体（人、地、机构、时间、作品等） 例句：天使爱美丽在线观看 分词：天使 爱 美丽 在线 观看 实体： 天使爱美丽 -&gt; 电影 推荐： 网页：天使爱美丽 土豆 高清视频 网页：在线直播爱美丽的天使 案例3（结构歧义问题）词性标注（Part-Speech Tagging）： 为自然语言文本中的每个词汇赋予一个词性（名词、动词、形容词等）依存句法分析（Dependency Parsing）：自动分析句子中的句法成分（主语、谓语、宾语、定语、状语和补语等成分） 评论：房间里还可以欣赏日出 歧义： 房间还可以 可以欣赏日出 词性：（？？？）房间里：主语还可以：谓语欣赏日出： 动宾短语 案例4（词汇语言相似度）词向量与语义相似度（Word Embedding &amp; Semantic Similarity）：对词汇进行向量化表示，并据此实现词汇的语义相似度计算。 例如：西瓜 与 （呆瓜/草莓），哪个更接近？ 向量化表示： 西瓜(0.1222, 0.22333, .. )相似度计算： 呆瓜（0.115） 草莓（0.325）向量化表示：(-0.333, 0.1223 .. ) (0.333， 0.3333, .. ) 案例5（文本语义相似度）文本语义相似度（Text Semantic Similarity）：依托全网海量数据和深度神经网络技术，实现文本间的语义相似度计算的能力 例如：车头如何防止车牌 与 （前牌照怎么装/如何办理北京牌照），哪个更接近？ 向量化表示： 车头如何防止车牌(0.1222, 0.22333, .. )相似度计算： 前牌照怎么装（0.762） 如何办理北京牌照（0.486）向量化表示： (-0.333, 0.1223 .. ) (0.333， 0.3333, .. ) 自然语言处理相关研究内容语料库知识语料库作为一个或者多个应用目标而专门收集的，有一定结构的、有代表的、可被计算机程序检索的、具有一定规模的语料的集合。 语料库划分： 时间划分 加工深度划分：标注语料库和非标注语料库 结构划分 语种划分 动态更新程度划分：参考语料库和监控语料库 语料库构建原则： 代表性 结构性 平衡性 规模性 元数据：元数据对 语料标注的优缺点 优点： 研究方便。可重用、功能多样性、分析清晰。 缺点： 语料不客观（手工标注准确率高而一致性差，自动或者半自动标注一致性高而准确率差）、标注不一致、准确率低 隐马尔可夫模型 应用 词类标注、语音识别、局部句法剖析、语块分析、命名实体识别、信息抽取等。应用于自然科学、工程技术、生物科技、公用事业、信道编码等多个领域。 马尔可夫链：在随机过程中，每个语言符号的出现概率不相互独立，每个随机试验的当前状态依赖于此前状态，这种链就是马尔可夫链。 多元马尔科夫链：考虑前一个语言符号对后一个语言符号出现概率的影响，这样得出的语言成分的链叫做一重马尔可夫链，也是二元语法。二重马尔可夫链，也是三元语法，三重马尔可夫链，也是四元语法 隐马尔可夫模型思想的三个问题 问题1（似然度问题）：给一个HMM λ=（A,B） 和一个观察序列O，确定观察序列的似然度问题 P(O|λ) 。（向前算法解决） 问题2（解码问题）：给定一个观察序列O和一个HMM λ=（A,B），找出最好的隐藏状态序列Q。（维特比算法解决） 问题3（学习问题）：给定一个观察序列O和一个HMM中的状态集合，自动学习HMM的参数A和B。（向前向后算法解决） Viterbi算法解码 算法思路 计算时间步1的维特比概率 计算时间步2的维特比概率，在第一步的基础计算 计算时间步3的维特比概率，在第二步的基础计算 维特比反向追踪路径 维特比算法与向前算法的区别 维特比算法要在前面路径的概率中选择最大值，而向前算法则计算其总和，除此之外，维特比算法和向前算法一样。 维特比算法有反向指针，寻找隐藏状态路径，而向前算法没有反向指针.HMM和维特比算法解决随机词类标注问题，利用Viterbi算法的中文句法标注 模型评价方法模型：方法=模型+策略+算法 模型问题涉及：训练误差、测试误差、过拟合等问题。通常将学习方法对未知数据的预测能力称为泛化能力。 模型评价参数： 准确率P=识别正确的数量/全部识别出的数量 错误率 =识别错误的数量/全部识别出的数量 精度=识别正确正的数量/识别正确的数量 召回率R=识别正确的数量/全部正确的总量（识别出+识别不出的） F度量=2PR/(P+R) 注意：如果数据正负均衡适合准确率，如果数据不均适合召回率，精度，F度量。 几种模型评估的方法：K-折交叉验证、随机二次抽样评估、ROC曲线评价两个模型好坏 生产模型与判别模型区别 生产式模型：直接对联合分布进行建模，如：隐马尔科夫模型、马尔科夫随机场等 判别式模型：对条件分布进行建模，如：条件随机场、支持向量机、逻辑回归等。 生成模型优点： 由联合分布 收敛速度比较快。 能够应付隐变量。 缺点：为了估算准确，样本量和计算量大，样本数目较多时候不建议使用。 判别模型优点： 计算和样本数量少。 准确率高。 缺点：收敛慢，不能针对隐变量。 知识图谱与本体 领域本体构建方法 确定领域本体的专业领域和范畴 考虑复用现有的本体 列出本体涉及领域中的重要术语 定义分类概念和概念分类层次 定义概念之间的关系 构建领域本体的知识工程方法： 主要特点：本体更强调共享、重用，可以为不同系统提供一种统一的语言，因此本体构建的工程性更为明显。 方法：目前为止，本体工程中比较有名的几种方法包括TOVE 法、Methontology方法、骨架法、IDEF-5法和七步法等。（大多是手工构建领域本体） 现状： 由于本体工程到目前为止仍处于相对不成熟的阶段，领域本体的建设还处于探索期，因此构建过程中还存在着很多问题。 方法成熟度： 以上常用方法的依次为:七步法、Methontology方法、IDEF-5法、TOVE法、骨架法。 特征工程 特征工程是什么 数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。特征工程本质是一项工程活动，目的是最大限度地从原始数据中提取特征以供算法和模型使用。通过总结和归纳，人们认为特征工程包括以下方面： 特征处理是特征工程的核心部分，特征处理方法包括数据预处理，特征选择，降维等。 特征提取 特征提取是指将机器学习算法不能识别的原始数据转化为算法可以识别的特征的过程。 实例解析：文本是由一系列文字组成的，这些文字在经过分词后会形成一个词语集合，对于这些词语集合（原始数据），机器学习算法是不能直接使用的，我们需要将它们转化成机器学习算法可以识别的数值特征（固定长度的向量表示），然后再交给机器学习的算法进行操作。再比如说，图片是由一系列像素点构（原始数据）成的，这些像素点本身无法被机器学习算法直接使用，但是如果将这些像素点转化成矩阵的形式（数值特征），那么机器学习算法就可以使用了。特征提取实际上是把原始数据转化为机器学习算法可以识别的数值特征的过程，不存在降维的概念，特征提取不需要理会这些特征是否是有用的；而特征选择是在提取出来的特征中选择最优的一个特征子集。 文本分类特征提取步骤： 假设一个语料库里包含了很多文章，在对每篇文章作了分词之后，可以把每篇文章看作词语的集合。然后将每篇文章作为数据来训练分类模型，但是这些原始数据是一些词语并且每篇文章词语个数不一样，无法直接被机器学习算法所使用，机器学习算法需要的是定长的数值化的特征。因此，我们要做的就是把这些原始数据数值化，这就对应了特征提取。如何做呢？ 对训练数据集的每篇文章，我们进行词语的统计，以形成一个词典向量。词典向量里包含了训练数据里的所有词语（假设停用词已去除），且每个词语代表词典向量中的一个元素。 在经过第一步的处理后，每篇文章都可以用词典向量来表示。这样一来，每篇文章都可以被看作是元素相同且长度相同的向量，不同的文章具有不同的向量值。这也就是表示文本的词袋模型（bag of words）。 针对于特定的文章，如何给表示它的向量的每一个元素赋值呢？最简单直接的办法就是0-1法了。简单来说，对于每一篇文章，我们扫描它的词语集合，如果某一个词语出现在了词典中，那么该词语在词典向量中对应的元素置为1，否则为0。 在经过上面三步之后，特征提取就完成了。对于每一篇文章，其中必然包含了大量无关的特征，而如何去除这些无关的特征，就是特征选择要做的事情了。 数据预处理 未经处理的特征，这时的特征可能有以下问题：（标准化的前提是特征值服从正态分布，标准化后，其转换成标准正态分布） 特征的规格不一样。无量纲化可以解决。 信息冗余：对于某些定量特征，其包含的有效信息为区间划分，例如学习成绩，假若只关心“及格”或不“及格”，那么需要将定量的考分，转换成“1”和“0”表示及格和未及格。二值化可以解决这一问题。 定性特征不能直接使用：某些机器学习算法和模型只能接受定量特征的输入，那么需要将定性特征转换为定量特征。假设有N种定性值，则将这一个特征扩展为N种特征，当原始特征值为第i种定性值时，第i个扩展特征赋值为1，其他扩展特征赋值为0。 存在缺失值：缺失值需要补充。 信息利用率低：不同的机器学习算法和模型对数据中信息的利用是不同的。 使用sklearn中的preproccessing库来进行数据预处理，可以覆盖以上问题的解决方案。 特征选择 当数据预处理完成后，我们需要选择有意义的特征输入机器学习的算法和模型进行训练。特征选择是指去掉无关特征，保留相关特征的过程，也可以认为是从所有的特征中选择一个最好的特征子集。特征选择本质上可以认为是降维的过程。 1、Filter（过滤法）：按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。如：方差选择法、相关系数法、卡方检验法、互信息法 方差选择法：使用方差选择法，先要计算各个特征的方差，然后根据阈值，选择方差大于阈值的特征。 相关系数法：使用相关系数法，先要计算各个特征对目标值的相关系数以及相关系数的P值。 卡方检验法：经典的卡方检验是检验定性自变量对定性因变量的相关性。假设自变量有N种取值，因变量有M种取值，考虑自变量等于i且因变量等于j的样本频数的观察值与期望的差距，构建统计量。 互信息法：经典的互信息也是评价定性自变量对定性因变量的相关性的。 2、Wrapper（包装法）：根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。如：递归特征消除法 递归特征消除法：递归消除特征法使用一个基模型来进行多轮训练，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行下一轮训练。 3、Embedded（嵌入法）：先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。 基于惩罚项的特征选择法：使用带惩罚项的基模型，除了筛选出特征外，同时也进行了降维。使用feature_selection库的SelectFromModel类结合带L1惩罚项的逻辑回归模型。如：基于惩罚项的特征选择法、基于树模型的特征选择法 基于树模型的特征选择法：树模型中GBDT也可用来作为基模型进行特征选择，使用feature_selection库的SelectFromModel类结合GBDT模型。 4、深度学习方法：从深度学习模型中选择某一神经层的特征后就可以用来进行最终目标模型的训练了。 降维 当特征选择完成后，可以直接训练模型了，但是可能由于特征矩阵过大，导致计算量大，训练时间长的问题，因此降低特征矩阵维度也是必不可少的。常见的降维方法：L1惩罚项的模型、主成分分析法（PCA）、线性判别分析（LDA）。PCA和LDA有很多的相似点，其本质是要将原始的样本映射到维度更低的样本空间中。所以说PCA是一种无监督的降维方法，而LDA是一种有监督的降维方法。 主成分分析法（PCA）：使用decomposition库的PCA类选择特征。 线性判别分析法（LDA）：使用lda库的LDA类选择特征。 奇异值分解（SVD） 参考文献 自然语言处理概述 中文维基百科 GitHub 图书：《机器学习实战》 图书：《自然语言处理理论与实战》 完整代码下载 源码请进【机器学习和自然语言QQ群：436303759】文件下载： 作者声明 本文版权归作者所有，旨在技术交流使用。未经作者同意禁止转载，转载后需在文章页面明显位置给出原文连接，否则相关责任自行承担。]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>自然语言处理</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[十分钟速览自然语言处理]]></title>
    <url>%2F2018%2F10%2F09%2F%E5%8D%81%E5%88%86%E9%92%9F%E9%80%9F%E8%A7%88%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[摘要：随着人工智能的快速发展，自然语言处理和机器学习技术应用愈加广泛。但是，初学者入门还是有一定难度，对该领域整体概况不能明晰。本章主要从发展历程、研究现状、应用前景等角度整体介绍自然语言处理及相关的机器学习技术，使读者对该技术领域有个系统而全面的认识。（本文原创，转载必须注明出处.） 课程导读 适合人群 具备一定编程基础的计算机专业、软件工程专业、通信专业、电子技术专业和自动化专业的学生和自然语言处理感兴趣的人群。 学习前技术储备 具备Python编程语言基础 具备面向对象的编程思想 具备一定的数学基础知识 快速了解自然语言处理什么是自然语言处理 自然语言 我们要对自然语言进行理解，其实就是我们日常使用的语言（书面文字和语音视频等）。简言之，汉语、日语、韩语、英语、法语等语言都属于此范畴。而自然语言处理是对自然语言处理的一种技术，就是通过我们的语音文字与计算机进行通信，我们称之为“人机交互”。 自然语言处理 自然语言处理（英语：Natural Language Processing，简称NLP）是人工智能和语言学领域的分支学科。此领域探讨如何处理及运用自然语言；自然语言认知则是指让电脑“懂”人类的语言。自然语言生成系统把计算机数据转化为自然语言。自然语言理解系统把自然语言转化为计算机程序更易于处理的形式。 我们要对自然语言进行理解，其实就是我们日常使用的语言（书面文字和语音视频等）。简言之，汉语、日语、韩语、英语、法语等语言都属于此范畴。而自然语言处理是对自然语言处理的一种技术，就是通过我们的语音文字与计算机进行通信，我们称之为“人机交互”。 自然语言处理发展背景和历程 自然语言处理发展背景 自然语言处理相关研究，最早是从机器翻译系统的研究开始的。20世纪60年代，国外对机器翻译曾有大规模的研究工作，投入了大量的人力物力财力。但是，受的客观历史因素的限制，当时人们低估了自然语言的复杂性，语言处理的理论和技术均不成热，所以进展并不大。其主要的做法是存储两种语言的单词、短语对应译法的大辞典，翻译时一一对应，技术上只是调整语言的同条顺序。但日常生活中语言的翻译远不是如此简单，很多时候还要参考某句话前后的意思。 我国机器翻译的起步较晚，是继美国、前苏联、英国之后世界上第四个开展机器翻译研究的国家。早在20世纪50年代机器翻译就被列入我国科学研究的发展规划。一些研究人员还进行了俄汉机器翻译实验，取得了一定的研究成果，但60年代的有关研究很快因“文革”而完全停顿。我国机器翻译研究的全面展开始于80年代中期以后，特别是90年代以来，一批机器翻译系统相继问世，其中影响力较大的有:中软总公司开发的汉英-汉日翻译系统(1993);中科院计算所研制的IMTEC英汉翻译系统(1992)等。 自然语言处理发展历史 1948年，香农（Shannon）把离散马尔可夫过程的概率模型应用于描述语言的自动机;同时又把“熵” (entropy)的概念引用到语言处理中。而克莱尼(Kleene)在同一时期研究了有限自动机和正则表达式。 1956年，乔姆斯基（Chomsky）又提出了上下文无关语法。这些工作导致了基于规则和基于概率两种不同的自然语言处理方法的诞生，使得该领域的研究分成了采用规则方法的符号派和采用概率方法的随机派两大阵营，进而引发了数十年有关这两种方法孰优孰劣的争执 。同年，人工智能诞生以后，自然语言处理迅速融入了人工智能的研究中。随机派学者在这一时期利用贝叶斯方法等统计学原理取得了一定的进步;而以乔姆斯基为代表的符号派也进行了形式语言理论生成句法和形式逻辑系统的研究。由于这一时期， 多数学者注重研究推理和逻辑问题，只有少数学者在研究统计方法和神经网络，所以 ，符号派的势头明显强于随机派的势头。 1967 年美国心理学家 奈瑟尔(Neisser)提出了认知心理学， 从而把自然语言处理与人类的认知联系起来。 70年代初，由于自然语言处理研究中的一些问题未能在短时间内得到解决，而新的问题又不断地涌现，许多人因此丧失了信心，自然语言处理的研究进入了低谷时期。尽管如此，一些发达国家的学者依旧地研究着。基于隐马尔可夫模型 (Hidden Markov Model，HMM)的统计方法和话语分析 (Discourse Analysis)在这一时期取得了重大进展 。 80年代， 在人们对于过去的工作反思之后 ， 有限状态模型和经验主义的研究方法开始复苏 。 90年代以后，随着计算机的速度和存储量大幅增加，自然语言处理的物质基础大幅改善，语音和语言处理的商品化开发成为可能;同时，网络技术的发展和Internet商业化使得基于自然语言的信息检索和信息抽取的需求变得更加突出。然语言处理的应用面不再局限于机器翻译、语音控制等早期研究领域了。 从90年代末到21世纪初 ，人们逐渐认识到，仅用基于规则的方法或仅用基于统计的方法都是无法成功进行自然语言处理的。基于统计、基于实例和基于规则的语料库技术在这一时期开始蓬勃发展， 各种处理技术开始融合，自然语言处理的研究又开始兴旺起来。 思考？ 基于规则的方法和基于统计的方法孰优孰劣？ 自然语言处理工作原理计算机对自然语言处理的过程：形式化描述-数学模型算法化-程序化-实用化。具体步骤如下： 形式化描述： 把需要研究是问题在语言上建立形式化模型，使其可以数学形式表示出来。 数学模型算法化： 把数学模型表示为算法的过程称之为“算法化“。 程序化： 根据算法，计算机进行实现，建立各种自然语言处理系统，这个过程是“程序化“。 实用化： 对系统进行评测和改进最终满足现实需求，这个过程是“实用化“。 自然语言处理涉及的学科领域 语言学 计算机科学（提供模型表示、算法设计、计算机实现） 数学（数学模型） 心理学（人类言语心理模型和理论） 哲学（提供人类思维和语言的更深层次理论） 统计学（提供样本数据的预测统计技术） 电子工程（信息论基础和语言信号处理技术） 生物学（人类言语行为机制理论）。 自然语言处理技术体系 NLP相关的技术 中文 英文 描述 分词 Word Segmentation 将连续的自然语言文本，切分成具有语义合理性和完整性的词汇序列 命名实体识别 Named Entity Recognition 识别自然语言文本中具有特定意义的实体（人、地、机构、时间、作品等） 词性标注 Part-Speech Tagging 为自然语言文本中的每个词汇赋予一个词性（名词、动词、形容词等） 依存句法分析 Dependency Parsing 自动分析句子中的句法成分（主语、谓语、宾语、定语、状语和补语等成分） 词向量与语义相似度 Word Embedding &amp; Semantic Similarity 依托全网海量数据和深度神经网络技术，实现了对词汇的向量化表示，并据此实现了词汇的语义相似度计算 文本语义相似度 Text Semantic Similarity 依托全网海量数据和深度神经网络技术，实现文本间的语义相似度计算的能力 篇章分析 Document Analysis 分析篇章级文本的内在结构，进而分析文本情感倾向，提取评论性观点，并生成反映文本关键信息的标签与摘要 机器翻译技术 Machine Translating 基于互联网大数据，融合深度神经网络、统计、规则多种翻译方法，帮助用户跨越语言鸿沟，与世界自由沟通 自然语言处理就业与发展前景随着自然语言处理的蓬勃发展和深入研究，新的应用方向会不断呈现出来。自然语言处理发展前景广阔，主要研究领域有： 文本方面：基于自然语言理解的智能搜索引擎和智能检索、智能机器翻译、自动摘要与文本综合、文本分类与文件整理、智能自动作文系统、自动判卷系统、信息过滤与垃圾邮件处理、文学研究与古文研究、语法校对、文本数据挖掘与智能决策、基于自然语言的计算机程序设计等。 语音方面：机器同声传译、智能远程教学与答疑、语音控制、智能客户服务、机器聊天与智能参谋、智能交通信息服务、智能解说与体育新闻实时解说 、语音挖掘与多媒体挖掘、多媒体信息提取与文本转化、对残疾人智能帮助系统等。 发展前景 自然语言处理的十个发展趋势：https://blog.csdn.net/heyc861221/article/details/80130981 自然语言处理产业情况：https://www.itjuzi.com/ai#map 自然语言处理相关工作的前景：http://www.52nlp.cn/job-prospects-for-natural-language-processing 2017 年中国人工智能产业数据报告：http://www.caict.ac.cn/kxyj/qwfb/qwsj/201804/P020180213603539476032.pdf 思考：如何学习自然语言处理的问题？ - 综述了解，整体技术框架掌握 - 侧重方向，多看论文和会议文章 - 知其原理，重在实际应用 - 归纳总结，提高研究效率 - 资料检索，高效学习效率 自然语言处理相关学科NLP与数学 线性数学 自然语言处理是以计算机科学、统计学、数学和信息论等多个领域交叉的学科。线性代数又是数学的一个重要分支，对自然语言处理有着很直接的影响。诸如：算法建模、参数设置、验证策略、识别欠拟合和过拟合等等。读者往往知道线性代数很有用，常常全书通读。造成时间不足和效率较低。归因于对线性代数在机器学习中的重点和用途不明。本章主要以简明的方式介绍最常用的线性代数知识，并使读者知道线性代数常用于哪些方面。 概率论 由于基于规则方法向基于统计方法的转型，概率就显得尤为重要，诸如一些随机事件、独立假设、条件概率、完全概率等等。然后对贝叶斯模型进行案例式介绍，旨在读者深度理解。 NLP与统计学在数据科学中，统计地位尤为显著。其在数据分析的基础上，研究如何测定、收集、整理、归纳和分析反映数据规律，以便给出正确消息的科学。通过揭示数据背后的规律和隐藏信息，给相关角色提供参照价值，做出相应的决策。这在数据挖掘、自然语言处理、机器学习都广泛应用。 百度EChart：http://echarts.baidu.com/examples/ 地图案例应用场景 适合的场景 某年度国家各个省州的人口情况。 分级统计地图较多的是反映呈面状但属分散分布的现象，如反映人口密度、某农作物播种面积的比、人均收入等。 不适合的场景 2008 年美国总统大选结果。 民主党候选人奥巴马和共和党候选人麦凯恩胜出的州分别用蓝色和红色表示。这个例子的选举可视化很容易给用户造成简介中提到的错觉：数据分布和地理区域大小的不对称。共和党比民主党获得了更多的投票，因为红色的区域所占的面积更大。但是在美国总统大选中，最后的结果是看候选人获得的选举人票数，每个州拥有的选举人票数是不一样的，在一个州获胜的选举人将得到该州所有的选举人票数。纽约州虽然面积很小，却拥有33张选举人票，而蒙大拿州虽然面积很大，却只有3票。 统计可视化 统计学知识 信息图形化（饼图，线形图等） 集中趋势度量（平均值 中位数 众数 方差等） 概率 排列组合 分布（几何二项泊松正态卡方） 统计抽样 样本估计 假设检验 回归 NLP与机器学习 什么是机器学习 机器学习是人工智能的一个分支。人工智能的研究是从以“推理”为重点到以“知识”为重点，再到以“学习”为重点，一条自然、清晰的脉络。显然，机器学习是实现人工智能的一个途径，即以机器学习为手段解决人工智能中的问题。机器学习在近30多年已发展为一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、计算复杂性理论等多门学科。机器学习理论主要是设计和分析一些让计算机可以自动“学习”的算法。机器学习算法是一类从数据中自动分析获得规律，并利用规律对未知数据进行预测的算法。因为学习算法中涉及了大量的统计学理论，机器学习与推断统计学联系尤为密切，也被称为统计学习理论。算法设计方面，机器学习理论关注可以实现的，行之有效的学习算法。很多推论问题属于无程序可循难度，所以部分的机器学习研究是开发容易处理的近似算法。机器学习已广泛应用于数据挖掘、计算机视觉、自然语言处理、生物特征识别、搜索引擎、医学诊断、检测信用卡欺诈、证券市场分析、DNA序列测序、语音和手写识别、战略游戏和机器人等领域。 机器学习就是指“计算机利用经验自动改善系统自身性能的行为”。简言之，机器学习是指通过计算机学习数据中的内在规律性信息，获得新的经验和知识，以提高计算机的智能性，使计算机能够像人那样去决策。本质上机器学习是一个从未知到已知的过程。假设计算机拥有这样一个程序，随着机器解决问题的增多，在该程序的作用下，机器性能或解决问题的能力增强，我们就说这台机器拥有学习能力。机器解决问题能力的增强主要表现在：初始状态下，对于问题Q，机器给出结果A，该机器在解决问题{Q1，Q2，… ，Qn}后，再次遇到问题Q时给出结果A1，而结果 A1比结果A更精确，我们就说机器解决问题的能力得到了增强。 机器学习发展历程 1943年， Warren McCulloch 和 Walter Pitts 提出了神经网络层次结构模型 ， 确立为神经网络的计算模型理论， 从而为机器学习的发展奠定了基础。 1950年， “人工智能之父”图灵发提出了著名的“图灵测试”，使人工智能成为了计算机科学领域一个重要的研究课题。 1957年， 康内尔大学教授 Frank Rosenblatt 提出Perceptron 概念，并且首次用算法精确定义了自组织自学习的神经网络数学模型， 设计出了第一个计算机神经网络，这个机器学习算法成为神经网络模型的开山鼻祖。 1959年， 美国 IBM 公司的 A． M． Samuel设计了一个具有学习能力的跳棋程序， 曾经战胜了美国一个保持 8 年不败的冠军。这个程序向人们初步展示了机器学习的能力。 1962年， Hubel 和 Wiesel 发现猫脑皮层中独特的神经网络结构可以有效降低学习的复杂性， 从而提出著名的 Hubel－Wiesel 生物视觉模型， 以后提出的神经网络模型均受此启迪。 1969 年， 人工智能研究的先驱者 Marvin Minsky和 Seymour Papert 出版了对机器学习研究具有深远影响的著作《Perceptron》， 虽然提出的 XOR 问题把感知机研究送上不归路、此后的十几年基于神经网络的人工智能研究进入低潮， 但是对于机器学习基本思想的论断:解决问题的算法能力和计算复杂性，影响深远、延续至今。 1980 年， 在美国卡内基·梅隆大学举行了第一届机器学习国际研讨会， 标志着机器学习研究在世界范围内兴起。1986 年， 《Machine Learning》创刊，标志着机器学习逐渐为世人瞩目并开始加速发展。 1982 年， Hopfield 发表了一篇关于神经网络模型的论文 ， 构造出能量函数并把这一概念引入Hopfield 网络，同时通过对动力系统性质的认识， 实现了 Hopfield 网络的最优化求解， 推动了神经网络的深入研究和发展应用。 1986 年，Rumelhart、Hinton 和 Williams 联合在《自然》杂志发表了著名的反向传播算法(BP) ， 首次阐述了 BP 算法在浅层前向型神经网络模型的应用， 不但明显降低了最优化问题求解的运算量，还通过增加一个隐层解决了感知器无法解决的 XOR Gate 难题，该算法成为神经网络的最基本算法。从此，神经网络的研究与应用开始复苏。 1989 年， 美国贝尔实验室学者 Yann LeCun 教授提出了目前最为流行的卷积神经网络( CNN) 计算模型，推导出基于 BP 算法的高效训练方法， 并成功地应用于英文手写体识别。CNN 是第一个被成功训练的人工神经网络，也是后来深度学习最成功、应用最广泛的模型之一。 90 年代后， 多种浅层机器学习模型相继问世，诸如逻辑回归、支持向量机等， 这些机器学习算法的共性是数学模型为凸代价函数的最优化问题，理论分析相对简单，训练方法也容易掌握，易于从训练样本中学习到内在模式，来完成对象识别、任务分类等初级智能工作。基于统计规律的浅层学习方法比起传统的基于规则的方法具备很多优越性， 取得了不少成功的商业应用的同时， 浅层学习的问题逐渐暴露出来，由于有限的样本和计算单元导致对数据间复杂函数的表示能力有限，学习能力不强，只能提取初级特征。 2006 年， 在学界及业界巨大需求刺激下， 特别是计算机硬件技术的迅速发展提供了强大的计算能力。机器学习领域的泰斗 Geoffrey Hinton 和 Ruslan Salakhutdinov 发表文章 ，提出了深度学习模型， 主要论点包括:多个隐层的人工神经网络具有良好的特征学习能力;通过逐层初始化来克服训练的难度，实现网络整体调优。这个模型的提出， 开启了深度神经网络机器学习的新时代。 2012 年， Hinton 研究团队采用深度学习模型赢得计算机视觉领域最具影响力的 ImageNet 比赛冠军，从而标志着深度学习进入第二个阶段。 至今， 随着Hinton、LeCun 和 Andrew Ng 对深度学习的研究，以及云计算、大数据、计算机硬件技术发展的支撑下，深度学习近年来在多个领域取得了令人赞叹的进展，推出一批成功的商业应用，诸如谷歌翻译，苹果语音工具 Siri， 微软的 Cortana 个人语音助手，蚂蚁金服的 Smile to Pay 扫脸技术， 特别是谷歌 AlphaGo 人机大战获胜的奇迹等， 使机器学习成为计算机科学的一个新的领域。深度学习是目前最接近人类大脑的分层智能学习方法，通过建立类似于人脑的分层模型结构，突破浅层学习的限制，能够表征复杂函数关系，对输入数据逐层提取从底层到高层的特征，并且逐层抽象，从而建立从底层简单特征到高层抽象语义的非线性映射关系 ，实现机器学习智能化的进一步提升， 成为机器学习的一个里程碑。 机器学习应用前景 2016年，引世人关注的人机大战以 AlphaGo 以 4:1 胜利而告终，这为世人所震撼惊叹的同时，更让人感受到机器学习的强大威力，更昭示出机器学习研究与应用的灿烂前景。以此为契机， 机器学习理论研究将会成为一个新的热点，在认知计算、类脑计算的支撑下将促进机器学习向更高阶段发展，在此基础上将会出现性能更好、结构优化、学习高效、功能强大的机器模型，非监督机器学习将会取得实质性的进展。机器学习的自主学习能力将进一步提高，逐渐跨越弱人工智能阶段，不断提高智能性。机器学习将向人类的学习、认知、理解、思考、推理和预测能力迈进，必将推动人工智能及整个科学技术的迈向更高台阶。随着机器学习与大数据、云计算、物联网的深度融合，将会掀起一场新的数字化技术革命，借助自然语言理解、情感及行为理解将会开启更加友好的人机交互新界面、自动驾驶汽车将成为现实，我们的工作、生活中将出现更多的智能机器人，在医疗、金融、教育等行业将能够给我们提供更多智能化、个性化服务定制服务，机器学习一定会造福于我们整个人类，使明天的生活更美好! 自然语言处理和机器学习的联系 语言是人类区别其他动物的本质特性。在所有生物中，只有人类才具有语言能力。人类的多种智能都与语言有着密切的关系。人类的逻辑思维以语言为形式，人类的绝大部分知识也是以语言文字的形式记载和流传下来的。因而，它也是人工智能（机器学习和深度学习为代表的人工智能）的一个重要，甚至核心部分。用自然语言与计算机进行通信，这是人们长期以来所追求的。因为它既有明显的实际意义和理论意义。实现人机间自然语言通信意味着要使计算机既能理解自然语言文本的意义，也能以自然语言文本来表达给定的意图、思想等。前者称为自然语言理解，后者称为自然语言生成。因此，自然语言处理大体包括了自然语言理解和自然语言生成两个部分。无论实现自然语言理解，还是自然语言生成，都远不如人们原来想象的那么简单，而是十分困难的。从现有的理论和技术现状看，通用的、高质量的自然语言处理系统，仍然是较长期的努力目标，但是针对一定应用，具有相当自然语言处理能力的实用系统已经出现，有些已商品化，甚至开始产业化。典型的例子有：多语种数据库和专家系统的自然语言接口、各种机器翻译系统、全文信息检索系统、自动文摘系统等。 现代NLP算法是基于机器学习，特别是统计机器学习。机器学习范式是不同于一般之前的尝试语言处理。语言处理任务的实现，通常涉及直接用手的大套规则编码。许多不同类的机器学习算法已应用于自然语言处理任务。这些算法的输入是一大组从输入数据生成的“特征”。一些最早使用的算法，如决策树，产生硬的if-then规则类似于手写的规则，是再普通的系统体系。然而，越来越多的研究集中于统计模型，这使得基于附加实数值的权重，每个输入要素柔软，概率的决策。此类模型具有能够表达许多不同的可能的答案，而不是只有一个相对的确定性，产生更可靠的结果时，这种模型被包括作为较大系统的一个组成部分的优点。自然语言处理研究逐渐从词汇语义成分的语义转移，进一步的，叙事的理解。然而人类水平的自然语言处理，是一个人工智能完全问题。它是相当于解决中央的人工智能问题使计算机和人一样聪明，或强大的AI。自然语言处理的未来一般也因此密切结合人工智能发展。 参考文献 自然语言处理概述 中文维基百科 GitHub 图书：《机器学习实战》 图书：《自然语言处理理论与实战》 完整代码下载 源码请进【机器学习和自然语言QQ群：436303759】文件下载： 作者声明 本文版权归作者所有，旨在技术交流使用。未经作者同意禁止转载，转载后需在文章页面明显位置给出原文连接，否则相关责任自行承担。]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>自然语言处理</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一步步教你轻松学主成分分析PCA降维算法]]></title>
    <url>%2F2018%2F09%2F29%2F%E4%B8%80%E6%AD%A5%E6%AD%A5%E6%95%99%E4%BD%A0%E8%BD%BB%E6%9D%BE%E5%AD%A6%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90PCA%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[摘要：主成分分析（英语：Principal components analysis，PCA）是一种分析、简化数据集的技术。主成分分析经常用于减少数据集的维数，同时保持数据集中的对方差贡献最大的特征。常常应用在文本处理、人脸识别、图片识别、自然语言处理等领域。可以做在数据预处理阶段非常重要的一环，本文首先对基本概念进行介绍，然后给出PCA算法思想、流程、优缺点等等。最后通过一个综合案例去实现应用。（本文原创，转载必须注明出处.） 数据降维预备知识 均值 方差 标准差 协方差 正交矩阵 什么是降维降维是对数据高维度特征的一种预处理方法。降维是将高维度的数据保留下最重要的一些特征，去除噪声和不重要的特征，从而实现提升数据处理速度的目的。在实际的生产和应用中，降维在一定的信息损失范围内，可以为我们节省大量的时间和成本。降维也成为了应用非常广泛的数据预处理方法。 我们正通过电视观看体育比赛，在电视的显示器上有一个足球。显示器大概包含了100万像素点，而球则可能是由较少的像素点组成，例如说一千个像素点。人们实时的将显示器上的百万像素转换成为一个三维图像，该图像就给出运动场上球的位置。在这个过程中，人们已经将百万像素点的数据，降至为三维。这个过程就称为降维(dimensionality reduction) 数据降维的目的： 使得数据集更容易使用 确保这些变量是相互独立的 降低很多算法的计算开销 去除噪音 使得结果易懂 适用范围: 在已标注与未标注的数据上都有降维技术。 本文主要关注未标注数据上的降维技术，将技术同样也可以应用于已标注的数据。 常见降维技术（PCA的应用目前最为广泛） 主成分分析就是找出一个最主要的特征，然后进行分析。例如： 考察一个人的智力情况，就直接看数学成绩就行(数学、语文、英语成绩) 因子分析(Factor Analysis),将多个实测变量转换为少数几个综合指标。它反映一种降维的思想，通过降维将相关性高的变量聚在一起,从而减少需要分析的变量的数量,而减少问题分析的复杂性.例如： 考察一个人的整体情况，就直接组合3样成绩(隐变量)，看平均成绩就行(存在：数学、语文、英语成绩),应用的领域包括社会科学、金融等。在因子分析中， 假设观察数据的成分中有一些观察不到的隐变量(latent variable)。 假设观察数据是这些隐变量和某些噪音的线性组合。 那么隐变量的数据可能比观察数据的数目少，也就说通过找到隐变量就可以实现数据的降维。 独立成分分析(Independ Component Analysis, ICA)，ICA 认为观测信号是若干个独立信号的线性组合，ICA 要做的是一个解混过程。 例如：我们去ktv唱歌，想辨别唱的是什么歌曲？ICA 是观察发现是原唱唱的一首歌【2个独立的声音（原唱／主唱）】。 ICA 是假设数据是从 N 个数据源混合组成的，这一点和因子分析有些类似，这些数据源之间在统计上是相互独立的，而在 PCA 中只假设数据是不 相关（线性关系）的。同因子分析一样，如果数据源的数目少于观察数据的数目，则可以实现降维过程。 PCA 概述主成分分析(Principal Component Analysis, PCA)：通俗理解：就是找出一个最主要的特征，然后进行分析。 主成分分析（英语：Principal components analysis，PCA）是一种分析、简化数据集的技术。主成分分析经常用于减少数据集的维数，同时保持数据集中的对方差贡献最大的特征。这是通过保留低阶主成分，忽略高阶主成分做到的。这样低阶成分往往能够保留住数据的最重要方面。但是，这也不是一定的，要视具体应用而定。由于主成分分析依赖所给数据，所以数据的准确性对分析结果影响很大。 主成分分析由卡尔·皮尔逊于1901年发明，用于分析数据及建立数理模型。其方法主要是通过对协方差矩阵进行特征分解，以得出数据的主成分（即特征向量）与它们的权值（即特征值）。PCA是最简单的以特征量分析多元统计分布的方法。其结果可以理解为对原数据中的方差做出解释：哪一个方向上的数据值对方差的影响最大？换而言之，PCA提供了一种降低数据维度的有效办法；如果分析者在原数据中除掉最小的特征值所对应的成分，那么所得的低维度数据必定是最优化的（也即，这样降低维度必定是失去讯息最少的方法）。主成分分析在分析复杂数据时尤为有用，比如人脸识别。 PCA是最简单的以特征量分析多元统计分布的方法。通常情况下，这种运算可以被看作是揭露数据的内部结构，从而更好的解释数据的变量的方法。如果一个多元数据集能够在一个高维数据空间坐标系中被显现出来，那么PCA就能够提供一幅比较低维度的图像，这幅图像即为在讯息最多的点上原对象的一个‘投影’。这样就可以利用少量的主成分使得数据的维度降低了。PCA跟因子分析密切相关，并且已经有很多混合这两种分析的统计包。而真实要素分析则是假定底层结构，求得微小差异矩阵的特征向量。 PCA 场景 例如： 考察一个人的智力情况，就直接看数学成绩就行(存在：数学、语文、英语成绩) PCA 思想 去除平均值 计算协方差矩阵 计算协方差矩阵的特征值和特征向量 将特征值排序 保留前N个最大的特征值对应的特征向量 将数据转换到上面得到的N个特征向量构建的新空间中（实现了特征压缩） PCA 原理 找出第一个主成分的方向，也就是数据方差最大的方向。 找出第二个主成分的方向，也就是数据方差次大的方向，并且该方向与第一个主成分方向正交(orthogonal 如果是二维空间就叫垂直)。 通过这种方式计算出所有的主成分方向。 通过数据集的协方差矩阵及其特征值分析，我们就可以得到这些主成分的值。 一旦得到了协方差矩阵的特征值和特征向量，我们就可以保留最大的 N 个特征。这些特征向量也给出了 N 个最重要特征的真实结构，我们就可以通过将数据乘上这 N 个特征向量 从而将它转换到新的空间上。 PCA 算法流程 下面我们看看具体的算法流程。 输入：n维样本集\( D=(x^{(1)},x^{(2)},…,x^{(m)}) \)，要降维到的维数n.输出：降维后的样本集\( D^′\)1) 对所有的样本进行中心化：\( x^{(i)}=x^{(i)}−\frac{1}{m}\sum_{j=1}^{m}x^{(j)} \)2) 计算样本的协方差矩阵\( XX^T\)3) 对矩阵\( XX^T\)进行特征值分解4）取出最大的n’个特征值对应的特征向量\( (w_1,w_2,…,w_n^′) \), 将所有的特征向量标准化后，组成特征向量矩阵W。5）对样本集中的每一个样本\( x^{(i)}\),转化为新的样本\( z^{(i)}=W^Tx^{(i)} \)6) 得到输出样本集\( D^′=(z^{(1)},z^{(2)},…,z^{(m)}) \) PCA 优缺点 优点：降低数据的复杂性，识别最重要的多个特征。缺点：不一定需要，且可能损失有用信息。适用数据类型：数值型数据。 实例理解真实的训练数据总是存在各种各样的问题： 比如拿到一个汽车的样本，里面既有以“千米/每小时”度量的最大速度特征，也有“英里/小时”的最大速度特征，显然这两个特征有一个多余。 拿到一个数学系的本科生期末考试成绩单，里面有三列，一列是对数学的兴趣程度，一列是复习时间，还有一列是考试成绩。我们知道要学好数学，需要有浓厚的兴趣，所以第二项与第一项强相关，第三项和第二项也是强相关。那是不是可以合并第一项和第二项呢？ 拿到一个样本，特征非常多，而样例特别少，这样用回归去直接拟合非常困难，容易过度拟合。比如北京的房价：假设房子的特征是（大小、位置、朝向、是否学区房、建造年代、是否二手、层数、所在层数），搞了这么多特征，结果只有不到十个房子的样例。要拟合房子特征-&gt;房价的这么多特征，就会造成过度拟合。 这个与第二个有点类似，假设在IR中我们建立的文档-词项矩阵中，有两个词项为“learn”和“study”，在传统的向量空间模型中，认为两者独立。然而从语义的角度来讲，两者是相似的，而且两者出现频率也类似，是不是可以合成为一个特征呢？ 在信号传输过程中，由于信道不是理想的，信道另一端收到的信号会有噪音扰动，那么怎么滤去这些噪音呢？ 这时可以采用主成分分析（PCA）的方法来解决部分上述问题。PCA的思想是将n维特征映射到k维上（k&lt;n），这k维是全新的正交特征。这k维特征称为主元，是重新构造出来的k维特征，而不是简单地从n维特征中去除其余n-k维特征。 PCA 算法实现准备数据收集数据：提供文本文件,文件名：testSet.txt.文本文件部分数据格式如下： 10.235186 11.321997 10.122339 11.810993 9.190236 8.904943 9.306371 9.847394 8.330131 8.340352 10.152785 10.123532 10.408540 10.821986 9.003615 10.039206 9.534872 10.096991 9.498181 10.825446 9.875271 9.233426 10.362276 9.376892 10.191204 11.250851 数据集处理代码实现如下 '''加载数据集''' def loadDataSet(fileName, delim='\t'): fr = open(fileName) stringArr = [line.strip().split(delim) for line in fr.readlines()] datArr = [list(map(float, line)) for line in stringArr] #注意这里和python2的区别，需要在map函数外加一个list（），否则显示结果为 map at 0x3fed1d0 return mat(datArr) PCA 数据降维在等式 Av=λv 中，v 是特征向量， λ 是特征值。表示 如果特征向量 v 被某个矩阵 A 左乘，那么它就等于某个标量 λ 乘以 v.幸运的是： Numpy 中有寻找特征向量和特征值的模块 linalg，它有 eig() 方法，该方法用于求解特征向量和特征值。具体代码实现如下： 方差：（一维）度量两个随机变量关系的统计量,数据离散程度，方差越小越稳定 协方差： （二维）度量各个维度偏离其均值的程度 协方差矩阵：（多维）度量各个维度偏离其均值的程度 当 cov(X, Y)&gt;0时，表明X与Y正相关(X越大，Y也越大；X越小Y，也越小。) 当 cov(X, Y)&lt;0时，表明X与Y负相关； 当 cov(X, Y)=0时，表明X与Y不相关。 '''pca算法 cov协方差=[(x1-x均值)*(y1-y均值)+(x2-x均值)*(y2-y均值)+...+(xn-x均值)*(yn-y均值)]/(n-1) Args: dataMat 原数据集矩阵 topNfeat 应用的N个特征 Returns: lowDDataMat 降维后数据集 reconMat 新的数据集空间 ''' def pca(dataMat, topNfeat=9999999): # 计算每一列的均值 meanVals = mean(dataMat, axis=0) # print('meanVals', meanVals) # 每个向量同时都减去均值 meanRemoved = dataMat - meanVals # print('meanRemoved=', meanRemoved) # rowvar=0，传入的数据一行代表一个样本，若非0，传入的数据一列代表一个样本 covMat = cov(meanRemoved, rowvar=0) # eigVals为特征值， eigVects为特征向量 eigVals, eigVects = linalg.eig(mat(covMat)) # print('eigVals=', eigVals) # print('eigVects=', eigVects) # 对特征值，进行从小到大的排序，返回从小到大的index序号 # 特征值的逆序就可以得到topNfeat个最大的特征向量 eigValInd = argsort(eigVals) # print('eigValInd1=', eigValInd) # -1表示倒序，返回topN的特征值[-1到-(topNfeat+1)不包括-(topNfeat+1)] eigValInd = eigValInd[:-(topNfeat+1):-1] # print('eigValInd2=', eigValInd) # 重组 eigVects 最大到最小 redEigVects = eigVects[:, eigValInd] # print('redEigVects=', redEigVects.T) # 将数据转换到新空间 # print( "---", shape(meanRemoved), shape(redEigVects)) lowDDataMat = meanRemoved * redEigVects reconMat = (lowDDataMat * redEigVects.T) + meanVals # print('lowDDataMat=', lowDDataMat) # print('reconMat=', reconMat) return lowDDataMat, reconMat ### 可视化结果分析 接下来我们查看降维后的数据与原始数据可视化效果，我们将原始数据采用绿色△表示，降维后的数据采用红色○表示。可视化代码如下： '''降维后的数据和原始数据可视化''' def show_picture(dataMat, reconMat): fig = plt.figure() ax = fig.add_subplot(111) ax.scatter(dataMat[:, 0].flatten().A[0], dataMat[:, 1].flatten().A[0], marker='^', s=90,c='green') ax.scatter(reconMat[:, 0].flatten().A[0], reconMat[:, 1].flatten().A[0], marker='o', s=50, c='red') plt.show() 调用代码： # 2 主成分分析降维特征向量设置 lowDmat, reconMat = pca(dataMat, 1) print(shape(lowDmat)) # 3 将降维后的数据和原始数据一起可视化 show_picture(dataMat, reconMat) 运行结果显示： PCA对半导体制造数据降维项目概述半导体是在一些极为先进的工厂中制造出来的。设备的生命早期有限，并且花费极其巨大。虽然通过早期测试和频繁测试来发现有瑕疵的产品，但仍有一些存在瑕疵的产品通过测试。如果我们通过机器学习技术用于发现瑕疵产品，那么它就会为制造商节省大量的资金。具体来讲，它拥有590个特征。我们看看能否对这些特征进行降维处理。对于数据的缺失值的问题，将缺失值NaN(Not a Number缩写)，全部用平均值来替代(如果用0来处理的策略就太差了)。收集数据：提供文本文件,文件名：secom.data.文本文件部分数据格式如下： 3030.93 2564 2187.7333 1411.1265 1.3602 100 97.6133 0.1242 1.5005 0.0162 -0.0034 0.9455 202.4396 0 7.9558 414.871 10.0433 0.968 192.3963 12.519 1.4026 -5419 2916.5 -4043.75 751 0.8955 1.773 3.049 64.2333 2.0222 0.1632 3.5191 83.3971 9.5126 50.617 64.2588 49.383 66.3141 86.9555 117.5132 61.29 4.515 70 352.7173 10.1841 130.3691 723.3092 1.3072 141.2282 1 624.3145 218.3174 0 4.592 数据预处理将数据集中NaN替换成平均值，代码实现如下： '''将NaN替换成平均值函数''' def replaceNanWithMean(): datMat = loadDataSet('./secom.data', ' ') numFeat = shape(datMat)[1] for i in range(numFeat): # 对value不为NaN的求均值 # .A 返回矩阵基于的数组 meanVal = mean(datMat[nonzero(~isnan(datMat[:, i].A))[0], i]) # 将value为NaN的值赋值为均值 datMat[nonzero(isnan(datMat[:, i].A))[0],i] = meanVal return datMat ### 分析数据 我们拿到数据进行数据预处理之后，再跑下程序，看看中间结果如果，分析数据代码如下： '''分析数据''' def analyse_data(dataMat): meanVals = mean(dataMat, axis=0) meanRemoved = dataMat-meanVals covMat = cov(meanRemoved, rowvar=0) eigvals, eigVects = linalg.eig(mat(covMat)) eigValInd = argsort(eigvals) topNfeat = 20 eigValInd = eigValInd[:-(topNfeat+1):-1] cov_all_score = float(sum(eigvals)) sum_cov_score = 0 for i in range(0, len(eigValInd)): line_cov_score = float(eigvals[eigValInd[i]]) sum_cov_score += line_cov_score ''' 我们发现其中有超过20%的特征值都是0。 这就意味着这些特征都是其他特征的副本，也就是说，它们可以通过其他特征来表示，而本身并没有提供额外的信息。 最前面15个值的数量级大于10^5，实际上那以后的值都变得非常小。 这就相当于告诉我们只有部分重要特征，重要特征的数目也很快就会下降。 最后，我们可能会注意到有一些小的负值，他们主要源自数值误差应该四舍五入成0. ''' print('主成分：%s, 方差占比：%s%%, 累积方差占比：%s%%' % (format(i+1, '2.0f'), format(line_cov_score/cov_all_score*100, '4.2f'), format(sum_cov_score/cov_all_score*100, '4.1f'))) 去均值化的特征值结果显示如下： [ 5.34151979e+07 2.17466719e+07 8.24837662e+06 2.07388086e+06 1.31540439e+06 4.67693557e+05 2.90863555e+05 2.83668601e+05 2.37155830e+05 2.08513836e+05 1.96098849e+05 1.86856549e+05 1.52422354e+05 1.13215032e+05 1.08493848e+05 1.02849533e+05 1.00166164e+05 8.33473762e+04 8.15850591e+04 7.76560524e+04 ... 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 ] 数据分析结果如下： 主成分： 1, 方差占比：59.25%, 累积方差占比：59.3% 主成分： 2, 方差占比：24.12%, 累积方差占比：83.4% 主成分： 3, 方差占比：9.15%, 累积方差占比：92.5% 主成分： 4, 方差占比：2.30%, 累积方差占比：94.8% 主成分： 5, 方差占比：1.46%, 累积方差占比：96.3% 主成分： 6, 方差占比：0.52%, 累积方差占比：96.8% 主成分： 7, 方差占比：0.32%, 累积方差占比：97.1% 主成分： 8, 方差占比：0.31%, 累积方差占比：97.4% 主成分： 9, 方差占比：0.26%, 累积方差占比：97.7% 主成分：10, 方差占比：0.23%, 累积方差占比：97.9% 主成分：11, 方差占比：0.22%, 累积方差占比：98.2% 主成分：12, 方差占比：0.21%, 累积方差占比：98.4% 主成分：13, 方差占比：0.17%, 累积方差占比：98.5% 主成分：14, 方差占比：0.13%, 累积方差占比：98.7% 主成分：15, 方差占比：0.12%, 累积方差占比：98.8% 主成分：16, 方差占比：0.11%, 累积方差占比：98.9% 主成分：17, 方差占比：0.11%, 累积方差占比：99.0% 主成分：18, 方差占比：0.09%, 累积方差占比：99.1% 主成分：19, 方差占比：0.09%, 累积方差占比：99.2% 主成分：20, 方差占比：0.09%, 累积方差占比：99.3% 我们发现其中有超过20%的特征值都是0。这就意味着这些特征都是其他特征的副本，也就是说，它们可以通过其他特征来表示，而本身并没有提供额外的信息。最前面值的数量级大于10^5，实际上那以后的值都变得非常小。这就相当于告诉我们只有部分重要特征，重要特征的数目也很快就会下降。最后，我们可能会注意到有一些小的负值，他们主要源自数值误差应该四舍五入成0. 根据实验结果我们绘制半导体数据中前七个主要成分所占的方差百分比如下 主成分 方差百分比（%） 累积方差百分比（%） 1 59.25 59.3 2 24.12 83.4 3 9.15 92.5 4 2.30 94.8 5 1.46 96.3 6 0.52 96.8 7 0.32 97.1 20 0.09 99.3 PCA降维结果可视化调用我们上文写的代码如下： lowDmat, reconMat = pca(dataMat, 20) print(shape(lowDmat)) show_picture(dataMat, reconMat) 运行结果如下： 参考文献 主成分分析 中文维基百科 GitHub 图书：《机器学习实战》 图书：《自然语言处理理论与实战》 一篇深入剖析PCA的好文 主成分分析原理总结 完整代码下载 源码请进【机器学习和自然语言QQ群：436303759】文件下载： 作者声明 本文版权归作者所有，旨在技术交流使用。未经作者同意禁止转载，转载后需在文章页面明显位置给出原文连接，否则相关责任自行承担。]]></content>
      <categories>
        <category>数据准备</category>
        <category>PCA</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>PCA</tag>
        <tag>数据降维</tag>
        <tag>数据预处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一步步教你轻松学关联规则Apriori算法]]></title>
    <url>%2F2018%2F09%2F27%2F%E4%B8%80%E6%AD%A5%E6%AD%A5%E6%95%99%E4%BD%A0%E8%BD%BB%E6%9D%BE%E5%AD%A6%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99Apriori%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[摘要：先验算法（Apriori Algorithm）是关联规则学习的经典算法之一，常常应用在商业等诸多领域。本文首先介绍什么是Apriori算法，与其相关的基本术语，之后对算法原理进行多方面剖析，其中包括思路、原理、优缺点、流程步骤和应用场景。接着再通过一个实际案例进行语言描述性逐步剖析。至此，读者基本了解该算法思想和过程。紧接着我们进行实验，重点的频繁项集的生成和关联规则的生成。最后我们采用综合实例进行实际演示。（本文原创，转载必须注明出处.） 理论介绍算法概述 维基百科 在计算机科学以及数据挖掘领域中，先验算法（Apriori Algorithm）是关联规则学习的经典算法之一。先验算法的设计目的是为了处理包含交易信息内容的数据库（例如,顾客购买的商品清单，或者网页常访清单。）而其他的算法则是设计用来寻找无交易信息（如Winepi算法和Minepi算法）或无时间标记（如DNA测序）的数据之间的联系规则。 先验算法采用广度优先搜索算法进行搜索并采用树结构来对候选项目集进行高效计数。它通过长度为\( k-1 \)的候选项目集来产生长度为 k 的候选项目集，然后从中删除包含不常见子模式的候选项。根据向下封闭性引理,该候选项目集包含所有长度为 k 的频繁项目集。之后，就可以通过扫描交易数据库来决定候选项目集中的频繁项目集。 数据挖掘十大算法 Apriori 算法是一种最有影响力的挖掘布尔关联规则的频繁项集算法，它是由Rakesh Agrawal 和RamakrishnanSkrikant 提出的。它使用一种称作逐层搜索的迭代方法，k- 项集用于探索（k+1）- 项集。首先，找出频繁 1- 项集的集合。该集合记作L1。L1 用于找频繁2- 项集的集合 L2，而L2 用于找L3，如此下去，直到不能找到 k- 项集。每找一个 Lk 需要一次数据库扫描。为提高频繁项集逐层产生的效率，一种称作Apriori 性质用于压缩搜索空间。其约束条件：一是频繁项集的所有非空子集都必须也是频繁的，二是非频繁项集的所有父集都是非频繁的。 基本概念 关联分析 关联分析是一种在大规模数据集中寻找相互关系的任务。 这些关系可以有两种形式: 频繁项集（frequent item sets）: 经常出现在一块的物品的集合。 关联规则（associational rules）: 暗示两种物品之间可能存在很强的关系。 相关术语 关联分析（关联规则学习): 下面是用一个 杂货店简单交易清单的例子来说明这两个概念，如下表所示: 交易号码 商品 0 豆奶，莴苣 1 莴苣，尿布，葡萄酒，甜菜 2 豆奶，尿布，葡萄酒，橙汁 3 莴苣，豆奶，尿布，葡萄酒 4 莴苣，豆奶，尿布，橙汁 频繁项集: {葡萄酒, 尿布, 豆奶} 就是一个频繁项集的例子。 关联规则: 尿布 -&gt; 葡萄酒 就是一个关联规则。这意味着如果顾客买了尿布，那么他很可能会买葡萄酒。 支持度: 数据集中包含该项集的记录所占的比例。例如上图中，{豆奶} 的支持度为 4/5。{豆奶, 尿布} 的支持度为 3/5。 可信度: 针对一条诸如 {尿布} -&gt; {葡萄酒} 这样具体的关联规则来定义的。这条规则的 可信度 被定义为 支持度({尿布, 葡萄酒})/支持度({尿布})，支持度({尿布, 葡萄酒}) = 3/5，支持度({尿布}) = 4/5，所以 {尿布} -&gt; {葡萄酒} 的可信度 = 3/5 / 4/5 = 3/4 = 0.75。 支持度 和 可信度 是用来量化 关联分析 是否成功的一个方法。 假设想找到支持度大于 0.8 的所有项集，应该如何去做呢？ 一个办法是生成一个物品所有可能组合的清单，然后对每一种组合统计它出现的频繁程度，但是当物品成千上万时，上述做法就非常非常慢了。 我们需要详细分析下这种情况并讨论下 Apriori 原理，该原理会减少关联规则学习时所需的计算量。 k项集如果事件A中包含k个元素，那么称这个事件A为k项集，并且事件A满足最小支持度阈值的事件称为频繁k项集。 由频繁项集产生强关联规则 K维数据项集LK是频繁项集的必要条件是它所有K-1维子项集也为频繁项集，记为LK-1 如果K维数据项集LK的任意一个K-1维子集Lk-1，不是频繁项集，则K维数据项集LK本身也不是最大数据项集。 Lk是K维频繁项集，如果所有K-1维频繁项集合Lk-1中包含LK的K-1维子项集的个数小于K，则Lk不可能是K维最大频繁数据项集。 同时满足最小支持度阀值和最小置信度阀值的规则称为强规则。 算法原理Apriori 思想 算法思想 首先找出所有的频集，这些项集出现的频繁性至少和预定义的最小支持度一样。然后由频集产生强关联规则，这些规则必须满足最小支持度和最小可信度。然后使用第1步找到的频集产生期望的规则，产生只包含集合的项的所有规则，其中每一条规则的右部只有一项，这里采用的是中规则的定义。一旦这些规则被生成，那么只有那些大于用户给定的最小可信度的规则才被留下来。 Apriori算法过程 第一步通过迭代，检索出事务数据库中的所有频繁项集，即支持度不低于用户设定的阈值的项集；第二步利用频繁项集构造出满足用户最小信任度的规则。 具体做法就是：首先找出频繁1-项集，记为L1；然后利用L1来产生候选项集C2，对C2中的项进行判定挖掘出L2，即频繁2-项集；不断如此循环下去直到无法发现更多的频繁k-项集为止。每挖掘一层Lk就需要扫描整个数据库一遍。算法利用了一个性质：任一频繁项集的所有非空子集也必须是频繁的。 Apriori 原理假设我们一共有 4 个商品: 商品0, 商品1, 商品2, 商品3。 所有可能的情况如下: 如果我们计算所有组合的支持度，也需要计算 15 次。即 \( 2^N - 1 = 2^4 - 1 = 15 \)。随着物品的增加，计算的次数呈指数的形式增长 。为了降低计算次数和时间，研究人员发现了一种所谓的 Apriori 原理，即某个项集是频繁的，那么它的所有子集也是频繁的。 例如，如果 {0, 1} 是频繁的，那么 {0}, {1} 也是频繁的。 该原理直观上没有什么帮助，但是如果反过来看就有用了，也就是说如果一个项集是 非频繁项集，那么它的所有超集也是非频繁项集，如下图所示: 在图中我们可以看到，已知灰色部分 {2,3} 是 非频繁项集，那么利用上面的知识，我们就可以知道 {0,2,3} {1,2,3} {0,1,2,3} 都是 非频繁的。 也就是说，计算出 {2,3} 的支持度，知道它是 非频繁 的之后，就不需要再计算 {0,2,3} {1,2,3} {0,1,2,3} 的支持度，因为我们知道这些集合不会满足我们的要求。 使用该原理就可以避免项集数目的指数增长，从而在合理的时间内计算出频繁项集。 Apriori 算法优缺点 优点：易编码实现 缺点：在大数据集上可能较慢 适用数据类型：数值型 或者 标称型数据。 Apriori 算法流程步骤： 收集数据：使用任意方法。 准备数据：任何数据类型都可以，因为我们只保存集合。 分析数据：使用任意方法。 训练数据：使用Apiori算法来找到频繁项集。 测试算法：不需要测试过程。 使用算法：用于发现频繁项集以及物品之间的关联规则。 应用场景Apriori 算法广泛应用于各种领域，通过对数据的关联性进行了分析和挖掘，挖掘出的这些信息在决策制定过程中具有重要的参考价值。 Apriori算法广泛应用于消费市场价格分析中 它能够很快的求出各种产品之间的价格关系和它们之间的影响。通过数据挖掘，市场商人可以瞄准目标客户，采用个人股票行市、最新信息、特殊的市场推广活动或其他一些特殊的信息手段，从而极大地减少广告预算和增加收入。百货商场、超市和一些老字型大小的零售店也在进行数据挖掘，以便猜测这些年来顾客的消费习惯。 Apriori算法应用于网络安全领域，比如网络入侵检测技术中。 早期中大型的电脑系统中都收集审计信息来建立跟踪档，这些审计跟踪的目的多是为了性能测试或计费，因此对攻击检测提供的有用信息比较少。它通过模式的学习和训练可以发现网络用户的异常行为模式。采用作用度的Apriori算法削弱了Apriori算法的挖掘结果规则，是网络入侵检测系统可以快速的发现用户的行为模式，能够快速的锁定攻击者，提高了基于关联规则的入侵检测系统的检测性。 Apriori算法应用于高校管理中。随着高校贫困生人数的不断增加，学校管理部门资助工作难度也越加增大。针对这一现象，提出一种基于数据挖掘算法的解决方法。将关联规则的Apriori算法应用到贫困助学体系中，并且针对经典Apriori挖掘算法存在的不足进行改进，先将事务数据库映射为一个布尔矩阵，用一种逐层递增的思想来动态的分配内存进行存储，再利用向量求”与”运算，寻找频繁项集。实验结果表明，改进后的Apriori算法在运行效率上有了很大的提升，挖掘出的规则也可以有效地辅助学校管理部门有针对性的开展贫困助学工作。 Apriori算法被广泛应用于移动通信领域。 移动增值业务逐渐成为移动通信市场上最有活力、最具潜力、最受瞩目的业务。随着产业的复苏，越来越多的增值业务表现出强劲的发展势头，呈现出应用多元化、营销品牌化、管理集中化、合作纵深化的特点。针对这种趋势，在关联规则数据挖掘中广泛应用的Apriori算法被很多公司应用。依托某电信运营商正在建设的增值业务Web数据仓库平台，对来自移动增值业务方面的调查数据进行了相关的挖掘处理，从而获得了关于用户行为特征和需求的间接反映市场动态的有用信息，这些信息在指导运营商的业务运营和辅助业务提供商的决策制定等方面具有十分重要的参考价值。 Apriori 实例理解实例理解1一个大型超级市场根据最小存货单位（SKU）来追踪每件物品的销售数据。从而也可以得知哪里物品通常被同时购买。通过采用先验算法来从这些销售数据中创建频繁购买商品组合的清单是一个效率适中的方法。假设交易数据库包含以下子集{1,2,3,4}，{1,2}，{2,3,4}，{2,3}，{1,2,4}，{3,4}，{2,4}。每个标号表示一种商品，如“黄油”或“面包”。先验算法首先要分别计算单个商品的购买频率。下表解释了先验算法得出的单个商品购买频率。 商品编号 购买次数 1 3 2 6 3 4 4 5 然后我们可以定义一个最少购买次数来定义所谓的“频繁”。在这个例子中，我们定义最少的购买次数为3。因此，所有的购买都为频繁购买。接下来，就要生成频繁购买商品的组合及购买频率。先验算法通过修改树结构中的所有可能子集来进行这一步骤。然后我们仅重新选择频繁购买的商品组合： 商品编号 购买次数 {1,2} 3 {2,3} 3 {2,4} 4 {3,4} 3 并且生成一个包含3件商品的频繁组合列表（通过将频繁购买商品组合与频繁购买的单件商品联系起来得出）。在上述例子中，不存在包含3件商品组合的频繁组合。最常见的3件商品组合为{1,2,4}和{2,3,4}，但是他们的购买次数为2，低于我们设定的最低购买次数。 实例理解2假设有一个数据库D，其中有4个事务记录，分别表示为： TID Items T1 l1,l3,l4 T2 l2,l3,l5 T3 l1,l2,l3,l5 T4 l2,l5 这里预定最小支持度minSupport=2,下面用图例说明算法运行的过程：1、扫描D，对每个候选项进行支持度计数得到表C1: 项集 支持度计数 {l1} 2 {l2} 3 {l3} 3 {l4} l {l5} 3 2、比较候选项支持度计数与最小支持度minSupport（假设为2），产生1维最大项目集L1： 项集 支持度计数 {l1} 2 {l2} 3 {l3} 3 {l5} 3 3、由L1产生候选项集C2： 项集 {l1,l2} {l1,l3} {l1,l5} {l2,l3} {l2,l5} {l3,l5} 4、扫描D，对每个候选项集进行支持度计数: 项集 支持度计数 {l1,l2} 1 {l1,l3} 2 {l1,l5} 1 {l2,l3} 2 {l2,l5} 3 {l3,l5} 2 5、比较候选项支持度计数与最小支持度minSupport，产生2维最大项目集L2： 项集 支持度计数 {l1,l3} 2 {l2,l3} 2 {l2,l5} 3 {l3,l5} 2 6、由L2产生候选项集C3： 项集 {l2,l3,l5} 7、比较候选项支持度计数与最小支持度minSupport，产生3维最大项目集L3： 项集 支持度计数 {l2,l3,l5} 2 算法终止。 从整体同样的能说明此过程 首先我们收集所有数据集（可以理解为商品清单），经过数据预处理后如Database TDB所示。我们扫描数据集，经过第一步对每个候选项进行支持度计数得到表C1，比较候选项支持度计数与最小支持度minSupport（假设最小支持度为2），产生1维最大项目集L1。再对L1进行组合产生候选项集C2。第二步我们对C2进行支持度计数，比较候选项支持度计数与最小支持度minSupport，产生2维最大项目集L2。由L2产生候选项集C3，对C3进行支持度计数，使用Apriori性质剪枝：频繁项集的所有子集必须是频繁的，对候选项C3，我们可以删除其子集为非频繁的选项，{A,B,C}的2项子集是{A,B},{A,C},{B,C}，其中{A,B}不是L2的元素，所以删除这个选项；{A,C,E}的2项子集是{A,C},{A,E},{C,E}，其中{A,E} 不是L2的元素，所以删除这个选项；{B,C,E}的2项子集是{B,C},{B,E},{C,E}，它的所有2－项子集都是L2的元素，因此保留这个选项。这样，剪枝后得到{B,C,E}，比较候选项支持度计数与最小支持度minSupport，产生3维最大项目集L3：继续进行没有满足条件，算法终止。 Apriori 算法实现关联分析的目标包括两项:发现频繁项集和发现关联规则。Apriori算法是发现频繁项集的一种方法。 Apriori 算法的两个输入参数分别是最小支持度和数据集。该算法首先会生成所有单个物品的项集列表。接着扫描交易记录来查看哪些项集满足最小支持度要求，那些不满足最小支持度要求的集合会被去掉。然后对剩下来的集合进行组合以生成包含两个元素的项集。接下来再重新扫描交易记录，去掉不满足最小支持度的项集。该过程重复进行直到所有项集被去掉。 生成候选项集 下面会创建一个用于构建初始集合的函数，也会创建一个通过扫描数据集以寻找交易记录子集的函数， 数据扫描的伪代码如下: - 对数据集中的每条交易记录 tran - 对每个候选项集 can - 检查一下 can 是否是 tran 的子集: 如果是则增加 can 的计数值 - 对每个候选项集 - 如果其支持度不低于最小值，则保留该项集 - 返回所有频繁项集列表 以下是一些辅助函数。 第一步加载数据集， # 加载数据集 def loadDataSet(): return [[1, 3, 4], [2, 3, 5], [1, 2, 3, 5], [2, 5]] 运行结果如下： [[1, 3, 4], [2, 3, 5], [1, 2, 3, 5], [2, 5]] 第二步创建集合 C1。对 dataSet 进行去重，排序，放入 list 中，然后转换所有的元素为 frozenset '''创建集合C1即对dataSet去重排序''' def createC1(dataSet): C1 = [] for transaction in dataSet: for item in transaction: if not [item] in C1: C1.append([item]) C1.sort() # frozenset表示冻结的set 集合，元素无改变把它当字典的 key 来使用 return C1 # return map(frozenset, C1) 运行结果如下，注意map(frozenset, C1)在后面会用到，注意便于计数处理： [[1], [2], [3], [4], [5]] ### 第三步计算候选数据集CK在数据集D中的支持度。 ''' 计算候选数据集CK在数据集D中的支持度，返回大于最小支持度的数据''' def scanD(D,Ck,minSupport): # ssCnt 临时存放所有候选项集和频率. ssCnt = {} for tid in D: # print('1:',tid) for can in map(frozenset,Ck): #每个候选项集can # print('2:',can.issubset(tid),can,tid) if can.issubset(tid): if not can in ssCnt: ssCnt[can] = 1 else: ssCnt[can] +=1 numItems = float(len(D)) # 所有项集数目 # 满足最小支持度的频繁项集 retList = [] # 满足最小支持度的频繁项集和频率 supportData = {} for key in ssCnt: support = ssCnt[key]/numItems #除以总的记录条数，即为其支持度 if support >= minSupport: retList.insert(0,key) #超过最小支持度的项集，将其记录下来。 supportData[key] = support return retList, supportData 运行结果如下： 满足最小支持度的频繁项集是： [frozenset({1}), frozenset({3}), frozenset({2}), frozenset({5})] 频繁项集的支持度 {frozenset({4}): 0.25, frozenset({5}): 0.75, frozenset({2}): 0.75, frozenset({3}): 0.75, frozenset({1}): 0.5} 第四步 根据上步Lk计算可能的候选项集 Ck ''' Apriori算法：输入频繁项集列表Lk，输出所有可能的候选项集 Ck''' def aprioriGen(Lk, k): retList = [] # 满足条件的频繁项集 lenLk = len(Lk) for i in range(lenLk): for j in range(i+1, lenLk): L1 = list(Lk[i])[: k-2] L2 = list(Lk[j])[: k-2] # print '-----i=', i, k-2, Lk, Lk[i], list(Lk[i])[: k-2] # print '-----j=', j, k-2, Lk, Lk[j], list(Lk[j])[: k-2] L1.sort() L2.sort() if L1 == L2: retList.append(Lk[i] | Lk[j]) return retList L1，k=2的运行结果： [frozenset({1, 3}), frozenset({1, 2}), frozenset({1, 5}), frozenset({2, 3}), frozenset({3, 5}), frozenset({2, 5})] ### 第五步： 找出满足最小支持度的频繁项集。 '''找出数据集中支持度不小于最小支持度的候选项集以及它们的支持度即频繁项集。 算法思想：首先构建集合C1，然后扫描数据集来判断这些只有一个元素的项集是否满足最小支持度。满足最小支持度要求的项集构成集合L1。然后L1 中的元素相互组合成C2，C2再进一步过滤变成L2，以此类推，直到C_n的长度为0时结束，即可找出所有频繁项集的支持度。 返回：L 频繁项集的全集 supportData 所有元素和支持度的全集 ''' def apriori(dataSet, minSupport=0.5): # C1即对dataSet去重排序，然后转换所有的元素为frozenset C1 = createC1(dataSet) # 对每一行进行 set 转换，然后存放到集合中 D = list(map(set, dataSet)) # 计算候选数据集C1在数据集D中的支持度，并返回支持度大于minSupport 的数据 L1, supportData = scanD(D, C1, minSupport) # L 加了一层 list, L一共 2 层 list L = [L1];k = 2 # 判断L第k-2项的数据长度是否>0即频繁项集第一项。第一次执行时 L 为 [[frozenset([1]), frozenset([3]), frozenset([2]), frozenset([5])]]。L[k-2]=L[0]=[frozenset([1]), frozenset([3]), frozenset([2]), frozenset([5])]，最后面 k += 1 while (len(L[k-2]) > 0): Ck = aprioriGen(L[k-2], k) # 例如: 以 {0},{1},{2} 为输入且 k = 2 则输出 {0,1}, {0,2}, {1,2}. 以 {0,1},{0,2},{1,2} 为输入且 k = 3 则输出 {0,1,2} # 返回候选数据集CK在数据集D中的支持度大于最小支持度的数据 Lk, supK = scanD(D, Ck, minSupport) # 保存所有候选项集的支持度，如果字典没有就追加元素，如果有就更新元素 supportData.update(supK) if len(Lk) == 0: break # Lk 表示满足频繁子项的集合，L 元素在增加，例如: # l=[[set(1), set(2), set(3)]] # l=[[set(1), set(2), set(3)], [set(1, 2), set(2, 3)]] L.append(Lk) k += 1 return L, supportData 我们写个测试以上代码 '''测试频繁项集生产''' def testApriori(): # 加载测试数据集 dataSet = loadDataSet() print ('dataSet: ', dataSet) # Apriori 算法生成频繁项集以及它们的支持度 L1, supportData1 = apriori(dataSet, minSupport=0.7) print ('L(0.7): ', L1) print ('supportData(0.7): ', supportData1) print ('->->->->->->->->->->->->->->->->->->->->->->->->->->->->') # Apriori 算法生成频繁项集以及它们的支持度 L2, supportData2 = apriori(dataSet, minSupport=0.5) print ('L(0.5): ', L2) print ('supportData(0.5): ', supportData2) 运行结果如下： dataSet: [[1, 3, 4], [2, 3, 5], [1, 2, 3, 5], [2, 5]] L(0.7): [[frozenset({3}), frozenset({2}), frozenset({5})], [frozenset({2, 5})]] supportData(0.7): {frozenset({5}): 0.75, frozenset({3}): 0.75, frozenset({3, 5}): 0.5, frozenset({4}): 0.25, frozenset({2, 3}): 0.5, frozenset({2, 5}): 0.75, frozenset({1}): 0.5, frozenset({2}): 0.75} -&gt;-&gt;-&gt;-&gt;-&gt;-&gt;-&gt;-&gt;-&gt;-&gt;-&gt;-&gt;-&gt;-&gt;-&gt;-&gt;-&gt;-&gt;-&gt;-&gt;-&gt;-&gt;-&gt;-&gt;-&gt;-&gt;-&gt;-&gt; L(0.5): [[frozenset({1}), frozenset({3}), frozenset({2}), frozenset({5})], [frozenset({3, 5}), frozenset({1, 3}), frozenset({2, 5}), frozenset({2, 3})], [frozenset({2, 3, 5})]] supportData(0.5): {frozenset({5}): 0.75, frozenset({3}): 0.75, frozenset({2, 3, 5}): 0.5, frozenset({1, 2}): 0.25, frozenset({1, 5}): 0.25, frozenset({3, 5}): 0.5, frozenset({4}): 0.25, frozenset({2, 3}): 0.5, frozenset({2, 5}): 0.75, frozenset({1}): 0.5, frozenset({1, 3}): 0.5, frozenset({2}): 0.75} 到这一步，我们就找出我们所需要的 频繁项集 和他们的 支持度 了，接下来再找出关联规则即可！ 第六步从频繁项集中挖掘关联规则集合中的元素是不重复的，但我们想知道基于这些元素能否获得其它内容。 某个元素或某个元素集合可能会推导出另一个元素。 从先前 杂货店 的例子可以得到，如果有一个频繁项集 {豆奶,莴苣}，那么就可能有一条关联规则 “豆奶 -&gt; 莴苣”。 这意味着如果有人买了豆奶，那么在统计上他会购买莴苣的概率比较大。 但是，这一条件反过来并不总是成立。 也就是说 “豆奶 -&gt; 莴苣” 统计上显著，那么 “莴苣 -&gt; 豆奶” 也不一定成立。 前面我们给出了 频繁项集 的量化定义，即它满足最小支持度要求。对于 关联规则，我们也有类似的量化方法，这种量化指标称之为 可信度。一条规则 A -&gt; B 的可信度定义为 support(A | B) / support(A)。（注意: 在 python 中 | 表示集合的并操作，而数学书集合并的符号是 U）。A | B 是指所有出现在集合 A 或者集合 B 中的元素。由于我们先前已经计算出所有 频繁项集 的支持度了，现在我们要做的只不过是提取这些数据做一次除法运算即可。 一个频繁项集可以产生多少条关联规则呢？ 如下图所示，给出的是项集 {0,1,2,3} 产生的所有关联规则: 与我们前面的 频繁项集 生成一样，我们可以为每个频繁项集产生许多关联规则。如果能减少规则的数目来确保问题的可解析，那么计算起来就会好很多。通过观察，我们可以知道，如果某条规则并不满足 最小可信度 要求，那么该规则的所有子集也不会满足 最小可信度 的要求。如上图所示，假设 123 -&gt; 3 并不满足最小可信度要求，那么就知道任何左部为 {0,1,2} 子集的规则也不会满足 最小可信度 的要求。 即 12 -&gt; 03 , 02 -&gt; 13 , 01 -&gt; 23 , 2 -&gt; 013, 1 -&gt; 023, 0 -&gt; 123 都不满足 最小可信度 要求。可以利用关联规则的上述性质属性来减少需要测试的规则数目，跟先前 Apriori 算法的套路一样。以下是一些辅助函数: 计算可信度 ‘’’计算可信度（confidence）Args: freqSet 频繁项集中的元素，例如: frozenset([1, 3]) H 频繁项集中的元素的集合，例如: [frozenset([1]), frozenset([3])] supportData 所有元素的支持度的字典 brl 关联规则列表的空数组 minConf 最小可信度Returns: prunedH 记录 可信度大于阈值的集合‘’’def calcConf(freqSet, H, supportData, brl, minConf=0.7): # 记录可信度大于最小可信度（minConf）的集合 prunedH = [] for conseq in H: # 假设 freqSet = frozenset([1, 3]), H = [frozenset([1]), frozenset([3])]，那么现在需要求出 frozenset([1]) -&gt; frozenset([3]) 的可信度和 frozenset([3]) -&gt; frozenset([1]) 的可信度 conf = supportData[freqSet]/supportData[freqSet-conseq] # 支持度定义: a -&gt; b = support(a | b) / support(a). 假设 freqSet = frozenset([1, 3]), conseq = [frozenset([1])]，那么 frozenset([1]) 至 frozenset([3]) 的可信度为 = support(a | b) / support(a) = supportData[freqSet]/supportData[freqSet-conseq] = supportData[frozenset([1, 3])] / supportData[frozenset([1])] if conf &gt;= minConf: # 只要买了 freqSet-conseq 集合，一定会买 conseq 集合（freqSet-conseq 集合和 conseq集合 是全集） print (freqSet-conseq, &#39;--&gt;&#39;, conseq, &#39;conf:&#39;, conf) brl.append((freqSet-conseq, conseq, conf)) prunedH.append(conseq) return prunedH &lt;/pre&gt;递归计算频繁项集的规则 “””递归计算频繁项集的规则 Args: freqSet 频繁项集中的元素，例如: frozenset([2, 3, 5]) H 频繁项集中的元素的集合，例如: [frozenset([2]), frozenset([3]), frozenset([5])] supportData 所有元素的支持度的字典 brl 关联规则列表的数组 minConf 最小可信度“””def rulesFromConseq(freqSet, H, supportData, brl, minConf=0.7): # H[0] 是 freqSet 的元素组合的第一个元素，并且 H 中所有元素的长度都一样，长度由 aprioriGen(H, m+1) 这里的 m + 1 来控制 # 该函数递归时，H[0] 的长度从 1 开始增长 1 2 3 ... # 假设 freqSet = frozenset([2, 3, 5]), H = [frozenset([2]), frozenset([3]), frozenset([5])] # 那么 m = len(H[0]) 的递归的值依次为 1 2 # 在 m = 2 时, 跳出该递归。假设再递归一次，那么 H[0] = frozenset([2, 3, 5])，freqSet = frozenset([2, 3, 5]) ，没必要再计算 freqSet 与 H[0] 的关联规则了。 m = len(H[0]) if (len(freqSet) &gt; (m + 1)): # 生成 m+1 个长度的所有可能的 H 中的组合，假设 H = [frozenset([2]), frozenset([3]), frozenset([5])] # 第一次递归调用时生成 [frozenset([2, 3]), frozenset([2, 5]), frozenset([3, 5])] # 第二次 。。。没有第二次，递归条件判断时已经退出了 Hmp1 = aprioriGen(H, m+1) # 返回可信度大于最小可信度的集合 Hmp1 = calcConf(freqSet, Hmp1, supportData, brl, minConf) # print (&#39;Hmp1=&#39;, Hmp1) # print (&#39;len(Hmp1)=&#39;, len(Hmp1), &#39;len(freqSet)=&#39;, len(freqSet)) # 计算可信度后，还有数据大于最小可信度的话，那么继续递归调用，否则跳出递归 if (len(Hmp1) &gt; 1): # print &#39;----------------------&#39;, Hmp1 # print len(freqSet), len(Hmp1[0]) + 1 rulesFromConseq(freqSet, Hmp1, supportData, brl, minConf) &lt;/pre&gt;生成关联规则 ‘’’生成关联规则 Args: L 频繁项集列表 supportData 频繁项集支持度的字典 minConf 最小置信度 Returns: bigRuleList 可信度规则列表（关于 (A-&gt;B+置信度) 3个字段的组合）‘’’def generateRules(L, supportData, minConf=0.7): bigRuleList = [] for i in range(1, len(L)): # 获取频繁项集中每个组合的所有元素 for freqSet in L[i]: # 组合总的元素并遍历子元素，转化为 frozenset集合存放到 list 列表中 H1 = [frozenset([item]) for item in freqSet] # print(H1) # 2 个的组合else, 2 个以上的组合 if if (i &gt; 1): rulesFromConseq(freqSet, H1, supportData, bigRuleList, minConf) else: calcConf(freqSet, H1, supportData, bigRuleList, minConf) return bigRuleList &lt;/pre&gt;到这里为止，通过调用 generateRules 函数即可得出我们所需的 关联规则。测试下结果： def testGenerateRules(): # 加载测试数据集 dataSet = loadDataSet() print (&#39;dataSet: &#39;, dataSet) # Apriori 算法生成频繁项集以及它们的支持度 L1, supportData1 = apriori(dataSet, minSupport=0.5) print (&#39;L(0.7): &#39;, L1) print (&#39;supportData(0.7): &#39;, supportData1) # 生成关联规则 rules = generateRules(L1, supportData1, minConf=0.5) print (&#39;rules: &#39;, rules) &lt;/pre&gt;运行结果如下： dataSet: [[1, 3, 4], [2, 3, 5], [1, 2, 3, 5], [2, 5]] L(0.7): [[frozenset({1}), frozenset({3}), frozenset({2}), frozenset({5})], [frozenset({3, 5}), frozenset({1, 3}), frozenset({2, 5}), frozenset({2, 3})], [frozenset({2, 3, 5})]] supportData(0.7): {frozenset({5}): 0.75, frozenset({3}): 0.75, frozenset({2, 3, 5}): 0.5, frozenset({1, 2}): 0.25, frozenset({1, 5}): 0.25, frozenset({3, 5}): 0.5, frozenset({4}): 0.25, frozenset({2, 3}): 0.5, frozenset({2, 5}): 0.75, frozenset({1}): 0.5, frozenset({1, 3}): 0.5, frozenset({2}): 0.75} frozenset({5}) --&gt; frozenset({3}) conf: 0.6666666666666666 frozenset({3}) --&gt; frozenset({5}) conf: 0.6666666666666666 frozenset({3}) --&gt; frozenset({1}) conf: 0.6666666666666666 frozenset({1}) --&gt; frozenset({3}) conf: 1.0 frozenset({5}) --&gt; frozenset({2}) conf: 1.0 frozenset({2}) --&gt; frozenset({5}) conf: 1.0 frozenset({3}) --&gt; frozenset({2}) conf: 0.6666666666666666 frozenset({2}) --&gt; frozenset({3}) conf: 0.6666666666666666 frozenset({5}) --&gt; frozenset({2, 3}) conf: 0.6666666666666666 frozenset({3}) --&gt; frozenset({2, 5}) conf: 0.6666666666666666 frozenset({2}) --&gt; frozenset({3, 5}) conf: 0.6666666666666666 rules: [(frozenset({5}), frozenset({3}), 0.6666666666666666), (frozenset({3}), frozenset({5}), 0.6666666666666666), (frozenset({3}), frozenset({1}), 0.6666666666666666), (frozenset({1}), frozenset({3}), 1.0), (frozenset({5}), frozenset({2}), 1.0), (frozenset({2}), frozenset({5}), 1.0), (frozenset({3}), frozenset({2}), 0.6666666666666666), (frozenset({2}), frozenset({3}), 0.6666666666666666), (frozenset({5}), frozenset({2, 3}), 0.6666666666666666), (frozenset({3}), frozenset({2, 5}), 0.6666666666666666), (frozenset({2}), frozenset({3, 5}), 0.6666666666666666)] 实际应用：发现毒蘑菇的相似特性实际需求菌类蘑菇食用对人体有益，现在市场上很受欢迎。假设你在一个山林里，遇到很多蘑菇，有些可以食用有些有毒。此刻，你或许会询问山中常驻居民，居民非常友好的告诉你伞菇上有彩色花斑的，样式好看的等等有毒。他会通过判断蘑菇的大小，高度，颜色，形状等23个特征决定蘑菇有毒，我把将居民的经验收集在mushromm.dat里面，以下是部分数据： 1 3 9 13 23 25 34 36 38 40 52 54 59 63 67 76 85 86 90 93 98 107 113 2 3 9 14 23 26 34 36 39 40 52 55 59 63 67 76 85 86 90 93 99 108 114 2 4 9 15 23 27 34 36 39 41 52 55 59 63 67 76 85 86 90 93 99 108 115 其中第一列1代表可以食用2代表有毒。其他各列代表不同特征。实际中，我们不可能对比23个特征，我们只需要找出毒蘑菇特有的几个特征即可，比如颜色彩色，形状方形等。我们自然语言描述很容易，就是看到蘑菇，对比下毒蘑菇的几个特征，不具备就可以采摘食用了。 到目前为止，我们清楚的采用毒蘑菇共同特征判断，那么如何知道毒蘑菇共同特征呢？我们就可以使用本节学习的先验算法Apriori进行关联规则找出毒蘑菇的共同特性。 算法实现 得到数据集 dataSet = [line.split() for line in open(&quot;./mushroom.dat&quot;).readlines()] 利用我们的先验算法计算L频繁项集和所有元素支持度的全集 L, supportData = apriori(dataSet, minSupport=0.4) 找出关于2的频繁子项，就知道如果是毒蘑菇，那么出现频繁的也可能是毒蘑菇 for item in L[2]: if item.intersection(&#39;2&#39;): print (item) 毒蘑菇的相似特性运行结果frozenset({&#39;59&#39;, &#39;39&#39;, &#39;2&#39;}) frozenset({&#39;59&#39;, &#39;85&#39;, &#39;2&#39;}) frozenset({&#39;34&#39;, &#39;39&#39;, &#39;2&#39;}) frozenset({&#39;90&#39;, &#39;86&#39;, &#39;2&#39;}) frozenset({&#39;34&#39;, &#39;90&#39;, &#39;2&#39;}) frozenset({&#39;39&#39;, &#39;86&#39;, &#39;2&#39;}) frozenset({&#39;85&#39;, &#39;28&#39;, &#39;2&#39;}) frozenset({&#39;59&#39;, &#39;86&#39;, &#39;2&#39;}) frozenset({&#39;34&#39;, &#39;85&#39;, &#39;2&#39;}) frozenset({&#39;90&#39;, &#39;39&#39;, &#39;2&#39;}) frozenset({&#39;39&#39;, &#39;85&#39;, &#39;2&#39;}) frozenset({&#39;34&#39;, &#39;59&#39;, &#39;2&#39;}) frozenset({&#39;34&#39;, &#39;86&#39;, &#39;2&#39;}) frozenset({&#39;90&#39;, &#39;59&#39;, &#39;2&#39;}) frozenset({&#39;85&#39;, &#39;86&#39;, &#39;2&#39;}) frozenset({&#39;90&#39;, &#39;85&#39;, &#39;2&#39;}) frozenset({&#39;63&#39;, &#39;85&#39;, &#39;2&#39;}) 如上结果显示，遇到如上特征就很可能是毒蘑菇不能食用的啦。我们上面实验设置的2-频繁项集，根据实际需要可以调整k-频繁项集。 参考文献 数据挖掘十大算法：https://wizardforcel.gitbooks.io/dm-algo-top10/content/apriori.html 中文维基百科：https://zh.wikipedia.org/wiki/%E5%85%88%E9%AA%8C%E7%AE%97%E6%B3%95 GitHub：https://github.com/BaiNingchao/MachineLearning-1 图书：《机器学习实战》 图书：《自然语言处理理论与实战》 完整代码下载 源码请进【机器学习和自然语言QQ群：436303759】文件下载： 作者声明 本文版权归作者所有，旨在技术交流使用。未经作者同意禁止转载，转载后需在文章页面明显位置给出原文连接，否则相关责任自行承担。]]></content>
      <categories>
        <category>机器学习</category>
        <category>Apriori</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>机器学习算法</tag>
        <tag>文本聚类</tag>
        <tag>Apriori</tag>
        <tag>关联规则</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sublime+Anaconda开发环境部署教程]]></title>
    <url>%2F2018%2F09%2F20%2FSublime-Anaconda%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[摘要：随着人工智能的快速发展，深度学习、机器学习、自然语言处理和数据挖掘等技术的应用愈加广泛。然而身为初学者，要想快速入门这些前沿技术总是存在着各种各样的困难。古语说“工欲善其事，必先利其器”，本课程的“器”就是开发环境部署，本文主要介绍Sublime的安装部署与使用。最后，我们将用一个简单的实战案例让读者亲身领略编程之美。（本文原创编著，转载注明出处.） Sublime Text和Anaconda介绍 Sublime Text简介 Sublime Text是一套跨平台的文本编辑器，支持基于Python的插件。Sublime Text 是专有软件，可通过包Package扩充本身的功能。大多数的包使用自由软件授权发布，并由社区建设维护。Sublime Text是由程序员Jon Skinner于2008年1月份所开发出来，它最初被设计为一个具有丰富扩展功能的Vim。其具有漂亮的用户界面和强大的功能，例如代码缩略图，Python的插件，代码段等。还可自定义键绑定，菜单和工具栏。Sublime Text 的主要功能包括：拼写检查，书签，完整的 Python API ， Goto 功能，即时项目切换，多选择，多窗口等等。Sublime Text 是一个跨平台的编辑器，同时支持Windows、Linux、Mac OS X等操作系统。 Sublime Text 支持众多编程语言，并支持语法上色。内置支持的编程语言包含：ActionScript、AppleScript、ASP、batch files、C、C++、C#、Clojure、CSS、D、Diff、Erlang、Go、Graphviz (DOT)、Groovy、Haskell、HTML、Java、JSP、JavaScript、JSON、LaTeX、Lisp、Lua、Makefiles、Markdown、MATLAB、Objective-C、OCaml、Perl、PHP、Python、R、Rails、Regular Expressions、reStructuredText、Ruby、Scala、shell scripts (Bash)、SQL、Tcl、Textile、XML、XSL 和 YAML。用户可通过下载外挂支持更多的编程语言。 Sublime Text优点 主流前端开发编辑器 体积较小，运行速度快 文本功能强大 支持编译功能且可在控制台看到输出 内嵌python解释器支持插件开发以达到可扩展目的 Package Control：ST支持的大量插件可通过其进行管理 Anaconda简介 Anaconda是一个用于科学计算的Python发行版，支持 Linux, Mac, Windows系统，提供了包管理与环境管理的功能，可以很方便地解决多版本python并存、切换以及各种第三方包安装问题。Anaconda利用工具/命令conda来进行package和environment的管理，并且已经包含了Python和相关的配套工具。这里先解释下conda、anaconda这些概念的差别。conda可以理解为一个工具，也是一个可执行命令，其核心功能是包管理与环境管理。包管理与pip的使用类似，环境管理则允许用户方便地安装不同版本的python并可以快速切换。Anaconda则是一个打包的集合，里面预装好了conda、某个版本的python、众多packages、科学计算工具等等，所以也称为Python的一种发行版。其实还有Miniconda，顾名思义，它只包含最基本的内容——python与conda，以及相关的必须依赖项，对于空间要求严格的用户，Miniconda是一个不错的选择。 开发环境安装与配置 工具包的准备 本文主要介绍Windows下的安装配置，关于Linux和MacOS下的安装，后文给出了扩展文章，需要的读者可自行在线下载。Sublime Text3安装包下载地址（http://www.sublimetext.com/3）。点击该网址进入Sublime主页，根据本机操作系统选择相关工具包下载。本文示范下载Windows64 bit。具体详见下图： Anaconda安装包下载地址（https://www.anaconda.com/download/）进入下载页面显示Python3.0以上版本和Python2.0以上版本。关于Python3和Python2的区别请访问（http://www.runoob.com/python/python-2x-3x.html）查看，在此不做赘述。一般推荐下载Python3.0以上版本。具体详见图所示： Anaconda安装 (1) 安装Anaconda集成环境，将下载后的Anaconda包双击打开如图所示： (2) 然后一直“Next”下去，直到完成配置。（环境变量自动配置），配置完成后，查看是否成功。打开主菜单-&gt;所有应用查看安装，如图所示： (3) 打开cmd进入dos命令下，输入conda list 查看集成的python包。如图所示： (4) 如果想添加新的python包，打开Anaconda官网：https://anaconda.org/search进行查找，比如想找到机器学习工具包scikit-learn如图所示： 至此我们就完成了Anaconda安装配置工作，以及对包文件的自定义下载安装。需要注明的是Anaconda自身集成了Python、pip、nltk、numpy、matplotlib等一系列常用包。现在，我们已经可以对python进行操作了。考虑到熟悉python开发的人员，常用Pycharm开发工具，熟悉java的开发人员常用Eclipse开发工具，熟悉C#的开发人员常用VS开发工具。然后我们将Anaconda集成到PyDev、Pycharm、Eclipse、VS等编译环境即可，诸如此类就不一一列举了。考虑到新学一种语言要重新学习一种编程环境，这样极其不方便。那么能不能找到一款编程工具可以通用以上语言？或许这样还不够，如果它还能跨Linux、Windows、MacOS那就更好了。本书强烈推荐的Sublime跨平台跨语言编辑器事实上就是这样一款强大的工具。我们接下来唯一要做的，就是将Anaconda集成到sublime中就可以了。扩展：linux和MacOS安装教程请访问（https://docs.continuum.io/anaconda/install/）。 Sublime Text3 安装 (1) 将下载好的Sublime Text3工具包双击到如下界面：如图所示： (2) 一直执行“Next”一路安装即可，中间保存路径可以自定义。最终安装成功将如图所示： (3) 安装插件Package Control。打开（ https://packagecontrol.io/installation）复制Sublime Text3中的代码如图所示： (4) 点击“Ctrl+`”，将3中文本代码内容复制粘贴到文本框中，按Enter即可。如图所示： (5) 成功安装后，在Sublime Text3中同时按住“Ctrl+Shift+P”键盘。最终安装成功：如图示： (6) 点击“Packeage Control:Install Package”进入查找python环境配置插件“SublimeREPL”，下载安装完成后，点击“Preferences-&gt;Browse Package…”查看安装的包如图所示： 手动安装安装插件Package Control sublime使用Package Control不能正常使用的解决办法 解决方法：Package Control.sublime-settings]修改方法：Preferences &gt; Package Settings &gt; Package Control &gt; Settings - User "channels": [ "http://cst.stu.126.net/u/json/cms/channel_v3.json", //"https://packagecontrol.io/channel_v3.json", //"https://web.archive.org/web/20160103232808/https://packagecontrol.io/channel_v3.json", //"https://gist.githubusercontent.com/nick1m/660ed046a096dae0b0ab/raw/e6e9e23a0bb48b44537f61025fbc359f8d586eb4/channel_v3.json" ] 激活版本：Help >Enter LICENSE ----- BEGIN LICENSE ----- sgbteam Single User License EA7E-1153259 8891CBB9 F1513E4F 1A3405C1 A865D53F 115F202E 7B91AB2D 0D2A40ED 352B269B 76E84F0B CD69BFC7 59F2DFEF E267328F 215652A3 E88F9D8F 4C38E3BA 5B2DAAE4 969624E7 DC9CD4D5 717FB40C 1B9738CF 20B3C4F1 E917B5B3 87C38D9C ACCE7DD8 5F7EF854 86B9743C FADC04AA FB0DA5C0 F913BE58 42FEA319 F954EFDD AE881E0B ------ END LICENSE ------ (7) 自定义快捷键盘配置：打开Preferences下Key Bindings输入如下代码，F5运行程序，F6切换IDEL工具，Ctrl+D自定义删除行，其他快捷键是通用的，网上有很多快捷键的资料，这里不赘述。 [ { "keys": ["f5"], "caption": "SublimeREPL: Python - RUN current file", "command": "run_existing_window_command", "args": { "id": "repl_python_run", "file": "config/Python/Main.sublime-menu" } }, { "keys": ["f6"], "caption": "SublimeREPL: Python", "command": "run_existing_window_command", "args": { "id": "repl_python", "file": "config/Python/Main.sublime-menu" } },{ "keys": ["ctrl+d"], "command":"run_macro_file", "args": {"file":"res://Packages/Default/Delete Line.sublime-macro"} } ] 至此完成了Sublime Text3安装配置工作，详细插件安装参考网址（http://www.open-open.com/news/view/26d731），快捷键使用请查看（https://segmentfault.com/a/1190000004463984）。还有问题的读者可以自行上网检索，由于资料较多且比较容易实现，不再详写。 实战：第一个小程序的诞生实例介绍 编写一个可以智能数据计算的小程序，用户输入公式如“10/(2+3)”，自动提示计算结果。 源码实现本实例设计思路如下： 如下①中采用def定义函数名，python不采用花括号，而是用冒号代替代码块，形参中param是一个自动识别类型的参数。 如下②中基本的计算公式，记住结尾没有分号 如下③是对结果的输入。 如下④类似于C、C#、Java中的主函数，后面章节会项目介绍。 如下⑤是对函数名的调用，并且直接传递列表参数，暂时不理解也没有关系，详见第二章。 源码如下（源码下载见https://github.com/BaiNingchao/NLP&amp;ML/01chapter/1.1py）： pythondef countNum(param): ① result = param[0]/(param[1]+param[2]) ② print(“this count: “+str(result)) ③ if name==”main“: ④ countNum([10,2,3]) ⑤&lt;/pre&gt;运行结果如图所示： 参考文献 Anaconda使用总结 官网anaconda配置教程 简单⾼效地进⾏科学计算 : Python和Anaconda 图书：《自然语言处理理论与实战》 完整代码下载 源码请进【机器学习和自然语言QQ群：436303759】文件下载： 作者声明 本文版权归作者所有，旨在技术交流使用。未经作者同意禁止转载，转载后需在文章页面明显位置给出原文连接，否则相关责任自行承担。]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Sublime</tag>
        <tag>Logistic regression</tag>
        <tag>Anaconda</tag>
        <tag>开发环境</tag>
        <tag>自然语言处理</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何自定义文件格式转换]]></title>
    <url>%2F2018%2F09%2F19%2F%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%EF%BC%9A%E9%87%8F%E8%BA%AB%E6%89%93%E9%80%A0%E8%87%AA%E5%AE%9A%E4%B9%89%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F%E8%BD%AC%E6%8D%A2%2F</url>
    <content type="text"><![CDATA[摘要：随着大数据的快速发展，自然语言处理、数据挖掘、机器学习技术应用愈加广泛。针对大数据的预处理工作是一项庞杂、棘手的工作。首先数据采集和存储，尤其高质量数据采集往往不是那么简单。采集后的信息文件格式不一，诸如pdf，doc，docx，Excel，ppt等多种形式。然而最常见便是txt、pdf和word类型的文档。本文主要对pdf和word文档进行文本格式转换成txt。格式一致化以后再进行后续预处理工作。笔者采用一些工具转换效果都不理想，于是才出现本系统的研究与实现。（本文原创，转载必须注明出处.） 本文概述背景介绍 为什么要文件格式转换？ 无论读者现在是做数据挖掘、数据分析、自然语言处理、智能对话系统、商品推荐系统等等，都不可避免的涉及语料的问题即大数据。数据来源无非分为结构化数据、半结构化数据和非结构化数据。其中结构化数据以规范的文档、数据库文件等等为代表；半结构化数据以网页、json文件等为代表；非结构化数据以自由文本为主，诸如随想录、中医病症记录等等。遗憾的是现实生活中半结构化和非结构化数据居多，而且往往还需要自己去收集。读者试想以下情况： 你的技术主管交给你一堆数据文件，让你做数据分析工作。你打开一看文件格式繁杂，诸如pdf、doc、docx、txt、excel等。更悲催的是有些pdf文件还是加密的，或者是图片格式的等复杂情况。此刻你采用什么方法做数据分析与预处理工作呢？ 上面情况算你幸运，隔几天技术主管直接给你一堆网站，让你自己去采集信息。你或许会惊喜的说的，那不简单，使用爬虫技术不就可以啦？恭喜你思路完全正确，可是爬取过程中遇到一些网页是pdf格式的情况，你不能直接抓取页面了。你此刻如何去采集信息呢？ 现有工具的转换效果如何 针对以上典型的情况，自定义插件PDFMiner、win2com等将派上用场（本文主要讲述文件格式转化，网络爬虫解析读者自行研究）。首先我们看看常规方式的处理，比如我下载个格式转化软件或者在线格式转化软件，具体如下所示：在线格式转换工具1页面效果如下： pdf格式转化为txt后的效果如下： 上面转换效果读者是否满意？是否因为某一个在线转换工具不完备，那我们再尝试一个,在线格式转换工具2页面效果如下： pdf格式转化为txt后的效果如下： 继续我们的格式转换工作，我们这次采用offic软件内带的pdf另存为效果如下： 总结 通过上面现有常规的方法，我们总结出以下问题： 1、 格式转换后，识别乱码较多。 2、 不支持或者限制支持批量处理。 3、 格式转换后的txt文件存在编码问题。 4、 生成目标文件的标题跟原标题不一致。 5、 操作不够灵活便捷。 基于自定义格式转换介绍 预期效果 1、 将带有嵌套的目录放在一个根目录文件下，只需要传入文件名即可自动转化。 2、 自动过滤掉不符合指定格式的文件。 3、 对处理的pdf文件不能识别的（加密文件等）给出日志记录其路径。 4、 生成目标文件的标题跟原文件目录标题保持一致。 5、 生成的文件按照统一的utf-8编码格式保存。 6、 支持默认保存路径与自定义保存路径。 预期效果展示 待处理语料数据如下： 处理后默认自动保存的结果（支持自定义指定保存目录）： 基于自定义插件的文本转化效果： 基于pdfminer插件的运行效果 基础配置工作基础准备工作 运行环境 1、windows7以上64bit操作系统 2、sublime运行环境 3、python3.0+ 需要插件 1、 pdfminer插件： 链接: https://pan.baidu.com/s/1p7X430bvBpjJ-qGNO-Fmcg 密码: v5th 或者：pip install pdfminer3k 2、 win2com 插件：链接: https://pan.baidu.com/s/1-2BsiTs8XjMIe5Gnh_GFjw 密码: 7j3t pip install pypiwin32 类库重构 算法基础类库重构 重构又称高度代码封装，旨在代码重用和面向对象编程。本文将相关基本方法封装在一个类库中供外部类调用，提高代码复用性和可读性。具体重构文件结构如下： 重构文件名：BaseClass.py ''' 功能描述：遍历目录，对子文件单独处理 参数描述： 1 rootdir：待处理的目录路径 2 deffun： 方法参数，默认为空 3 savepath: 保存路径 ''' class TraversalFun(): TraversalDir：遍历目录文件方法 creat_savepath：支持默认和自定义保存目录方法 AllFiles：递归遍历所有文件，并提供具体文件操作功能 TranType：通过指定关键字操作，检查文件类型并转化目标类型 filelogs：记录文件处理日志方法 cleardir：清空目录文件方法 writeFile：文件的写操作方法 readFile：文件的读操作方法 mkdir：创建目录方法 ''' 功能描述：提供全局变量类 作 者：白宁超 时 间：2017年10月24日15:07:38 ''' class Global(object):提高各个公共全局变量 ''' 功能描述：测试类 作 者：白宁超 时 间：2017年10月24日15:07:38 ''' def TestMethod(filepath,newpath):方法测试类 > 核心方法详解 1 TraversalFun类方法： def __init__(self,rootdir,deffun=None,savedir=""): self.rootdir = rootdir # 目录路径 self.deffun = deffun # 参数方法 self.savedir = savedir # 保存路径 ''' 遍历目录文件''' def TraversalDir(self,defpar='newpath'): try: # 支持默认和自定义保存目录 newdir = TraversalFun.creat_savepath(self,defpar) # 递归遍历word文件并将其转化txt文件 TraversalFun.AllFiles(self,self.rootdir,newdir) except Exception as e: raise e '''支持默认和自定义保存目录''' # @staticmethod def creat_savepath(self,defpar): # 文件路径切分为上级路径和文件名('F:\\kjxm\\kjt', '1.txt') prapath,filename = os.path.split(self.rootdir) newdir = "" if self.savedir=="": newdir = os.path.abspath(os.path.join(prapath,filename+"_"+defpar)) else: newdir = self.savedir print("保存目录路径：\n"+newdir) if not os.path.exists(newdir): os.mkdir(newdir) return newdir '''递归遍历所有文件，并提供具体文件操作功能。''' def AllFiles(self,rootdir,newdir=''): # 返回指定目录包含的文件或文件夹的名字的列表 for lists in os.listdir(rootdir): # 待处理文件夹名字集合 path = os.path.join(rootdir, lists) # 核心算法，对文件具体操作 if os.path.isfile(path): self.deffun(path,newdir) # 具体方法实现功能 # TraversalFun.filelogs(rootdir) # 日志文件 # 递归遍历文件目录 if os.path.isdir(path): newpath = os.path.join(newdir, lists) if not os.path.exists(newpath): os.mkdir(newpath) TraversalFun.AllFiles(self,path,newpath) ''' 通过指定关键字操作，检查文件类型并转化目标类型''' def TranType(filename,typename): # print("本方法支持文件类型处理格式：pdf2txt，代表pdf转化为txt；word2txt，代表word转化txt；word2pdf，代表word转化pdf。") # 新的文件名称 new_name = "" if typename == "pdf2txt" : #如果不是pdf文件，或者是pdf临时文件退出 if not fnmatch.fnmatch(filename, '*.pdf') or not fnmatch.fnmatch(filename, '*.PDF') or fnmatch.fnmatch(filename, '~$*'): return # 如果是pdf文件，修改文件名 if fnmatch.fnmatch(filename, '*.pdf') or fnmatch.fnmatch(filename, '*.PDF'): new_name = filename[:-4]+'.txt' # 截取".pdf"之前的文件名 if typename == "word2txt" : #如果是word文件： if fnmatch.fnmatch(filename, '*.doc') : new_name = filename[:-4]+'.txt' print(new_name) if fnmatch.fnmatch(filename, '*.docx'): new_name = filename[:-5]+'.txt' # 如果不是word文件，或者是word临时文件退出 else: return if typename == "word2pdf" : #如果是word文件： if fnmatch.fnmatch(filename, '*.doc'): new_name = filename[:-4]+'.pdf' if fnmatch.fnmatch(filename, '*.docx'): new_name = filename[:-5]+'.pdf' #如果不是word文件：继续 else: return return new_name '''记录文件处理日志''' def filelogs(rootdir): prapath,filename = os.path.split(rootdir) # 创建日志目录 dirpath = prapath+r"/"+filename+"_logs" TraversalFun.mkdir(dirpath) # 错误文件路径 errorpath = dirpath+r"/errorlogs.txt" # 限制文件路径 limitpath = dirpath+r"/limitlogs.txt" # 错误文件日志写入 TraversalFun.writeFile(errorpath,'\n'.join(Global.error_file_list)) # # 限制文件日志写入 TraversalFun.writeFile(limitpath,'\n'.join(Global.limit_file_list)) '''清空目录文件''' def cleardir(dirpath): if not os.path.exists(dirpath): TraversalFun.mkdir(dirpath) else: shutil.rmtree(dirpath) TraversalFun.mkdir(dirpath) ''' 文件的写操作''' def writeFile(filepath,strs): #encoding="utf-8" with open(filepath,'wb') as f: f.write(strs.encode()) ''' 文件的读操作''' def readFile(filepath): isfile = os.path.exists(filepath) readstr = "" if isfile: with open(filepath,"r",encoding="utf-8") as f: readstr = f.read() else: return return readstr ''' 创建目录 ''' def mkdir(dirpath): # 判断路径是否存在 isExists=os.path.exists(dirpath) # 判断结果 if not isExists: os.makedirs(dirpath) print(dirpath+' 创建成功') else: pass ``` 2 TestMethod测试类 ```python def TestMethod(filepath,newpath): if os.path.isfile(filepath) : print("this is file name:"+filepath) else: pass 3 利用测试类方法运行方法参数效果图 方法的调用：传达参数分别是跟目录和测试类中的方法参数 t1=time.time() # 根目录文件路径 rootDir = r"../../Corpus/DataSet" tra=TraversalFun(rootDir,TestMethod) # 默认方法参数打印所有文件路径 tra.TraversalDir() # 遍历文件并进行相关操作 t2=time.time() totalTime=Decimal(str(t2-t1)).quantize(Decimal('0.0000')) print("耗时："+str(totalTime)+" s"+"\n") input() 运行结果如图所示： ![](https://i.imgur.com/qN4fpJm.png) # 基于pdfminer插件的pdf批量格式转换代码实现 > pdfminer原理介绍 ![](https://i.imgur.com/i0WdEma.png) 由于解析PDF是一件非常耗时和内存的工作，因此PDFMiner使用了一种称作lazy parsing的策略，只在需要的时候才去解析，以减少时间和内存的使用。要解析PDF至少需要两个类：PDFParser 和 PDFDocument，PDFParser 从文件中提取数据，PDFDocument保存数据。另外还需要PDFPageInterpreter去处理页面内容，PDFDevice将其转换为我们所需要的。PDFResourceManager用于保存共享内容例如字体或图片。 ![](https://i.imgur.com/HuhjYMi.png) 1. LTPage :表示整个页。可能会含有LTTextBox，LTFigure，LTImage，LTRect，LTCurve和LTLine子对象。 1. LTTextBox:表示一组文本块可能包含在一个矩形区域。注意此box是由几何分析中创建，并且不一定表示该文本的一个逻辑边界。它包含TTextLine对象的列表。使用 get_text（）方法返回的文本内容。 1. LTTextLine :包含表示单个文本行LTChar对象的列表。字符对齐要么 水平或垂直，取决于文本的写入模式。 1. get_text（）方法返回的文本内容。 1. LTAnno:在文本中实际的字母表示为Unicode字符串（？）。需要注意的是，虽然一个LTChar对象具有实际边界，LTAnno对象没有，因为这些是“虚拟”的字符，根据两个字符间的关系（例如，一个空格）由布局分析后插入。 1. LTImage:表示一个图像对象。嵌入式图像可以是JPEG或其它格式，但是目前PDFMiner没有放置太多精力在图形对象。 1. LTLine:代表一条直线。可用于分离文本或附图。 1. LTRect:表示矩形。可用于框架的另一图片或数字。 1. LTCurve:表示一个通用的 Bezier曲线 > pdfminer学习文献 英文官方：https://euske.github.io/pdfminer/index.html 中文：https://blog.csdn.net/robolinux/article/details/43318229 > pdfminer代码实现 # pdfminer库的地址 https://pypi.python.org/pypi/pdfminer3k # 下载后，用cmd执行命令 setup.py install from pdfminer.pdfparser import PDFParser,PDFDocument from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter from pdfminer.converter import PDFPageAggregator from pdfminer.layout import LTTextBoxHorizontal,LAParams from pdfminer.pdfinterp import PDFTextExtractionNotAllowed from decimal import Decimal import time,fnmatch,os,re,sys from BaseClass import * #全局变量 # from BaseClass import TraversalFun # 文件遍历处理基类函数 # 清除警告 import logging logging.Logger.propagate = False logging.getLogger().setLevel(logging.ERROR) '''pdf文件格式转换为txt''' def PdfToText(filepath,newdir=''): # 文件路径切分为上级路径和文件名 prapath,filename = os.path.split(filepath) new_txt_name=TraversalFun.TranType(filename,"pdf2txt") # 更改文件名 if new_txt_name ==None: return newpath = os.path.join(newdir,new_txt_name) # 文件保存路径 print ("->格式转换后保留路径：\n"+newpath) try: praser = PDFParser(open(filepath, 'rb')) # 创建一个pdf文档分析器 doc = PDFDocument() # 创建一个PDF文档 praser.set_document(doc) # 连接分析器 与文档对象 doc.set_parser(praser) doc.initialize() # 提供初始化密码，如果没有密码 就创建一个空的字符串 # 检测文档是否提供txt转换，不提供就忽略 if not doc.is_extractable: Global.error_file_list.append(filepath) return rsrcmgr = PDFResourceManager() # 创建PDf 资源管理器管理共享资源 laparams = LAParams() # 创建一个PDF设备对象 device = PDFPageAggregator(rsrcmgr, laparams=laparams) interpreter = PDFPageInterpreter(rsrcmgr, device) # 创建一个PDF解释器对象 pdfStr = "" # 存储解析后的提取内容 # 循环遍历列表，每次处理一个page的内容 for page in doc.get_pages(): # doc.get_pages()获取page列表 interpreter.process_page(page) layout = device.get_result() # 接受该页面的LTPage对象 # 这里layout是一个LTPage对象 里面存放着 这个page解析出的各种对象 一般包括LTTextBox, LTFigure, LTImage, LTTextBoxHorizontal 等等 想要获取文本就获得对象的text属性， for x in layout: if (isinstance(x, LTTextBoxHorizontal)): pdfStr = pdfStr + x.get_text() TraversalFun.writeFile(newpath,pdfStr) # 写文件 # 限制文件列表 filesize = os.path.getsize(newpath) if filesize < Global.limit_file_size : Global.limit_file_list.append(newpath+"\t"+ str(Decimal(filesize/1024).quantize(Decimal('0.00'))) +"KB") os.remove(newpath) else : Global.all_FileNum+=1 except Exception as e: Global.error_file_list.append(filepath) return if __name__ == '__main__': t1=time.time() rootDir = r"../../Corpus/DataSet" # 默认处理路径 TraversalFun.cleardir(r'../../Corpus/DataSet_newpath') # 每次加载清空目录 print ('【批量生成的文件:】') tra=TraversalFun(rootDir,PdfToText) # 默认方法参数打印所有文件路径 tra.TraversalDir() # 写入日志文件 TraversalFun.filelogs(rootDir) print ('共处理文档数目：'+str(Global.all_FileNum+len(Global.error_file_list)+len(Global.limit_file_list))+' 个,其中:\n \ 1) 筛选文件(可用)'+str(Global.all_FileNum)+'个.\n \ 2) 错误文件(不能识别)'+ str(len(Global.error_file_list)) +'个.\n \ 3) 限制文件(]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>自然语言处理</tag>
        <tag>数据预处理</tag>
        <tag>NLP</tag>
        <tag>数据准备</tag>
        <tag>pdf2txt</tag>
        <tag>格式转化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一步步教你轻松学决策树模型算法]]></title>
    <url>%2F2018%2F09%2F19%2F%E4%B8%80%E6%AD%A5%E6%AD%A5%E6%95%99%E4%BD%A0%E8%BD%BB%E6%9D%BE%E5%AD%A6%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[摘要：决策树算法是一种基本的分类与回归方法，是最经常使用的算法之一。决策树模型呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程。它可以认为是基于规则的集合。本文首先介绍决策树定义、工作原理、算法流程、优缺点等，然后结合案例进行分析。（本文原创，转载必须注明出处.） 理论介绍什么是决策树 维基百科：决策树（Decision Tree）是一个预测模型；他代表的是对象属性与对象值之间的一种映射关系。树中每个节点表示某个对象，而每个分叉路径则代表某个可能的属性值，而每个叶节点则对应从根节点到该叶节点所经历的路径所表示的对象的值。数据挖掘中决策树是一种经常要用到的技术，可以用于分析数据，同样也可以用来作预测。从数据产生决策树的机器学习技术叫做决策树学习,通俗说就是决策树。 分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点（node）和有向边（directed edge）组成。结点有两种类型：内部结点（internal node）和叶结点（leaf node）。内部结点表示一个特征或属性(features)，叶结点表示一个类(labels)。 用决策树对需要测试的实例进行分类：从根节点开始，对实例的某一特征进行测试，根据测试结果，将实例分配到其子结点；这时，每一个子结点对应着该特征的一个取值。如此递归地对实例进行测试并分配，直至达到叶结点。最后将实例分配到叶结点的类中。 什么是信息熵和信息增益 熵（entropy）： 熵指的是体系的混乱的程度，在不同的学科中也有引申出的更为具体的定义，是各领域十分重要的参量。 信息论（information theory）中的熵（香农熵）： 是一种信息的度量方式，表示信息的混乱程度，也就是说：信息越有序，信息熵越低。例如：火柴有序放在火柴盒里，熵值很低，相反，熵值很高。 信息增益（information gain）： 在划分数据集前后信息发生的变化称为信息增益，信息增益越大，确定性越强。 决策树工作原理12345678910111213'''决策树工作原理：基于迭代的思想。'''def createBranch(): 检测数据集中的所有数据的分类标签是否相同: If so return 类标签 Else: 寻找划分数据集的最好特征（划分之后信息熵最小，也就是信息增益最大的特征） 划分数据集 创建分支节点 for 每个划分的子集 调用函数 createBranch （创建分支的函数）并增加返回结果到分支节点中 return 分支节点 决策树算法流程收集数据：可以使用任何方法。 准备数据：树构造算法 (这里使用的是ID3算法，只适用于标称型数据，这就是为什么数值型数据必须离散化。 还有其他的树构造算法，比如CART) 分析数据：可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期。 训练算法：构造树的数据结构。 测试算法：使用训练好的树计算错误率。 使用算法：此步骤可以适用于任何监督学习任务，而使用决策树可以更好地理解数据的内在含义。 决策树优缺点相对于其他数据挖掘算法，决策树在以下几个方面拥有优势： 1 决策树易于理解和实现.人们在通过解释后都有能力去理解决策树所表达的意义。 2 对于决策树，数据的准备往往是简单或者是不必要的.其他的技术往往要求先把数据一般化，比如去掉多余的或者空白的属性。 3 能够同时处理数据型和常规型属性。其他的技术往往要求数据属性的单一。 4 是一个白盒模型如果给定一个观察的模型，那么根据所产生的决策树很容易推出相应的逻辑表达式。 5 易于通过静态测试来对模型进行评测。表示有可能测量该模型的可信度。 6 在相对短的时间内能够对大型数据源做出可行且效果良好的结果。 7 计算复杂度不高，输出结果易于理解，数据有缺失也能跑，可以处理不相关特征。 缺点： 1 容易过拟合。 2 对于那些各类别样本数量不一致的数据，在决策树当中信息增益的结果偏向于那些具有更多数值的特征。 适用数据类型：数值型和标称型。 1 数值型：数值型目标变量则可以从无限的数值集合中取值，如0.100，42.001等 (数值型目标变量主要用于回归分析) 2 标称型：标称型目标变量的结果只在有限目标集中取值，如真与假(标称型目标变量主要用于分类) 案例描述：加深决策树理解案例描述小王是一家著名高尔夫俱乐部的经理。但是他被雇员数量问题搞得心情十分不好。某些天好像所有人都来玩高尔夫，以至于所有员工都忙的团团转还是应付不过来，而有些天不知道什么原因却一个人也不来，俱乐部为雇员数量浪费了不少资金。小王的目的是通过下周天气预报寻找什么时候人们会打高尔夫，以适时调整雇员数量。因此首先他必须了解人们决定是否打球的原因。 数据采集在2周时间内我们得到以下记录： 天气状况有晴，云和雨；气温用华氏温度表示；相对湿度用百分比；还有有无风。当然还有顾客是不是在这些日子光顾俱乐部。最终他得到了14行5列的数据表格。 构建决策树决策树模型就被建起来用于解决问题。 结果分析决策树是一个有向无环图。根结点代表所有数据。分类树算法可以通过变量outlook，找出最好地解释非独立变量play（打高尔夫的人）的方法。变量outlook的范畴被划分为以下三个组：晴天，多云天和雨天。 我们得出第一个结论：如果天气是多云，人们总是选择玩高尔夫，而只有少数很着迷的甚至在雨天也会玩。 接下来我们把晴天组的分为两部分，我们发现顾客不喜欢湿度高于70%的天气。最终我们还发现，如果雨天还有风的话，就不会有人打了。 这就通过分类树给出了一个解决方案。小王（老板）在晴天，潮湿的天气或者刮风的雨天解雇了大部分员工，因为这种天气不会有人打高尔夫。而其他的天气会有很多人打高尔夫，因此可以雇用一些临时员工来工作。 决策树算法实现与分析案例: 判定鱼类和非鱼类 案例需求描述 我们采集海洋生物数据信息，选择其中5条如下表所示，从诸多特征中选择2个最主要特征，以及判定是否属于鱼类（此处我们选择二分类法即只考虑鱼类和非鱼类）。根据这些信息如何创建一个决策树进行分类并可视化展示？ 收集数据 部分数据采集信息 序号 不浮出水面是否可以生存 是否有脚蹼 属于鱼类 1 是 是 是 2 是 是 是 3 是 否 否 4 否 是 否 5 否 是 否 我们将自然语言数据转化为计算机输入数据，代码实现如下：123456789'''创建数据集，返回数据集和标签'''def createDataSet(): dataSet = [[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']] labels = ['no surfacing', 'flippers'] return dataSet, labels 运行查看数据集的特征向量和分类标签： # 1 打印数据集和标签 dataset,label=createDataSet() print(dataset) print(label) 运行结果： [[1, 1, &#39;yes&#39;], [1, 1, &#39;yes&#39;], [1, 0, &#39;no&#39;], [0, 1, &#39;no&#39;], [0, 1, &#39;no&#39;]] [&#39;no surfacing&#39;, &#39;flippers&#39;] 准备数据 由于我们输入的数据已经是数据预处理后的数据，这一步不需要进行。 分析数据我们得到数据之后，到底是按照第一个特征即(不浮出水面是否可以生存)还是第二个特征即（是否有脚蹼）进行数据划分呢？这里面就需要找到一种量化的方法判断特征的选择。在介绍具体数据划分方法之前，我们首先明白划分数据集的最大原则是：将无序的数据变得更加有序 1948 年，香农引入信息熵，将其定义为离散随机事件的出现概率。一个系统越有序，信息熵就越低；反之，一个系统越混乱，信息熵就越高。所以说，信息熵可以被认为是系统有序化程度的一个度量。 这里就要用的信息熵的概念，熵越高表示混合数据越多，度量数据集无序程度。我们看下信息熵的数学描述（具体请自行查找熵相关知识）： 计算数据集的香农熵(信息期望值) 根据公式比较容易理解的实现方法1如下：123456789101112131415161718'''计算数据集的香农熵(信息期望值):熵越高表示混合数据越多，度量数据集无序程度'''def calcShannonEnt(dataSet): numEntries = len(dataSet) # 计算数据集中实例总数 labelCounts = &#123;&#125; # 创建字典，计算分类标签label出现的次数 for featVec in dataSet: currentLabel = featVec[-1] # 记录当前实例的标签 if currentLabel not in labelCounts.keys():# 为所有可能的分类创建字典 labelCounts[currentLabel] = 0 labelCounts[currentLabel] += 1 # print(featVec, labelCounts) # 打印特征向量和字典的键值对 # 对于label标签的占比，求出label标签的香农熵 shannonEnt = 0.0 for key in labelCounts: prob = float(labelCounts[key])/numEntries # 计算类别出现的概率。 shannonEnt -= prob * log(prob, 2) # 计算香农熵，以 2 为底求对数 print(Decimal(shannonEnt).quantize(Decimal('0.00000'))) return shannonEnt 更高级的实现方法2如下：12345678'''计算数据集的香农熵(信息期望值):熵越高表示混合数据越多，度量数据集无序程度'''def calcShannonEnt(dataSet): # 需要对 list 中的大量计数时,可以直接使用Counter,不用新建字典来计数 label_count = Counter(data[-1] for data in dataSet) # 统计标签出现的次数 probs = [p[1] / len(dataSet) for p in label_count.items()] # 计算概率 shannonEnt = sum([-p * log(p, 2) for p in probs]) # 计算香农熵 print(Decimal(shannonEnt).quantize(Decimal('0.00000'))) return shannonEnt 调用运行如下： # 2 计算数据集的熵 calcShannonEnt(dataset) 按照给定的特征划分数据集 我们根据信息熵度量出来的特征，进行数据集划分方法1如下：123456789101112131415161718'''划分数据集:按照特征划分'''def splitDataSet(dataSet, index, value): retDataSet = [] for featVec in dataSet: if featVec[index] == value:# 判断index列的值是否为value reducedFeatVec = featVec[:index] # [:index]表示取前index个特征 reducedFeatVec.extend(featVec[index+1:]) # 取接下来的数据 retDataSet.append(reducedFeatVec) print(retDataSet) return retDataSet``` 我们根据信息熵度量出来的特征，进行数据集划分方法2如下：```python '''划分数据集:按照特征划分'''def splitDataSet(dataSet, index, value): retDataSet = [data for data in dataSet for i, v in enumerate(data) if i == index and v == value] print(retDataSet) return retDataSet 指定特征的数据集划分方法调用 #3 划分数据集 splitDataSet(dataset,0,1) 运行结果如下： [[1, 1, &#39;yes&#39;], [1, 1, &#39;yes&#39;], [1, 0, &#39;no&#39;]] 选择最好的数据集划分方式 选择最好的数据集划分方式：特征选择，划分数据集、计算最好的划分数据集特征，方法1如下：1234567891011121314151617181920212223'''注意：一是数据集列表元素具备相同数据长度，二是最后一列是标签列'''def chooseBestFeatureToSplit(dataSet): numFeatures = len(dataSet[0]) - 1 # 特征总个数, 最后一列是标签 baseEntropy = calcShannonEnt(dataSet) # 计算数据集的信息熵 bestInfoGain, bestFeature = 0.0, -1 # 最优的信息增益值, 和最优的Featurn编号 for i in range(numFeatures): featList = [example[i] for example in dataSet] # 获取各实例第i+1个特征 uniqueVals = set(featList) # 获取去重后的集合 newEntropy = 0.0 # 创建一个新的信息熵 for value in uniqueVals: subDataSet = splitDataSet(dataSet, i, value) prob = len(subDataSet)/float(len(dataSet)) newEntropy += prob * calcShannonEnt(subDataSet) # 比较所有特征中的信息增益，返回最好特征划分的索引值。 infoGain = baseEntropy - newEntropy print('infoGain=', infoGain, 'bestFeature=', i, baseEntropy, newEntropy) if (infoGain &gt; bestInfoGain): bestInfoGain = infoGain bestFeature = i # print(bestFeature) return bestFeature 选择最好的数据集划分方式：特征选择，划分数据集、计算最好的划分数据集特征，方法2如下：123456789101112131415161718192021'''注意：一是数据集列表元素具备相同数据长度，二是最后一列是标签列'''def chooseBestFeatureToSplit(dataSet): base_entropy = calcShannonEnt(dataSet) # 计算初始香农熵 best_info_gain = 0 best_feature = -1 # 遍历每一个特征 for i in range(len(dataSet[0]) - 1): # 对当前特征进行统计 feature_count = Counter([data[i] for data in dataSet]) # 计算分割后的香农熵 new_entropy = sum(feature[1] / float(len(dataSet)) * calcShannonEnt(splitDataSet(dataSet, i, feature[0])) for feature in feature_count.items()) # 更新值 info_gain = base_entropy - new_entropy # print('No. &#123;0&#125; feature info gain is &#123;1:.3f&#125;'.format(i, info_gain)) if info_gain &gt; best_info_gain: best_info_gain = info_gain best_feature = i # print(best_feature) return best_feature 选择最好的数据集划分方法调用 # 4 选择最好的数据集划分方式 chooseBestFeatureToSplit(dataset)) 运行结果如下： infoGain= 0.4199730940219749 bestFeature= 0 0.9709505944546686 0.5509775004326937 infoGain= 0.17095059445466854 bestFeature= 1 0.9709505944546686 0.8 选择：0 训练算法：构造树的数据结构创建树的函数代码如下：1234567891011121314151617181920212223242526272829303132'''创建决策树'''def createTree(dataSet, labels): classList = [example[-1] for example in dataSet] # 如果数据集的最后一列的第一个值出现的次数=整个集合的数量，也就说只有一个类别，就只直接返回结果就行 # 第一个停止条件：所有的类标签完全相同，则直接返回该类标签。 # count() 函数是统计括号中的值在list中出现的次数 if classList.count(classList[0]) == len(classList): return classList[0] # 如果数据集只有1列，那么最初出现label次数最多的一类，作为结果 # 第二个停止条件：使用完了所有特征，仍然不能将数据集划分成仅包含唯一类别的分组。 if len(dataSet[0]) == 1: return majorityCnt(classList) # 选择最优的列，得到最优列对应的label含义 bestFeat = chooseBestFeatureToSplit(dataSet) # 获取label的名称 bestFeatLabel = labels[bestFeat] # 初始化myTree myTree = &#123;bestFeatLabel: &#123;&#125;&#125; # 所以这行代码导致函数外的同名变量被删除了元素，造成例句无法执行，提示'no surfacing' is not in list # del(labels[bestFeat]) # 取出最优列，然后它的branch做分类 featValues = [example[bestFeat] for example in dataSet] uniqueVals = set(featValues) for value in uniqueVals: # 求出剩余的标签label subLabels = labels[:] # 遍历当前选择特征包含的所有属性值，在每个数据集划分上递归调用函数createTree() myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value), subLabels) # print('myTree', value, myTree) print(myTree) return myTree 其中多数表决方法决定叶子节点的分类实现如下：123456789101112131415161718'''多数表决方法决定叶子节点的分类：选择出现次数最多的一个结果'''def majorityCnt(classList): # -----------多数表决实现的方式一-------------- # classCount = &#123;&#125; # 标签字典，用于统计类别频率 # for vote in classList: # classList标签的列表集合 # if vote not in classCount.keys(): # classCount[vote] = 0 # classCount[vote] += 1 # # 取出结果（yes/no），即出现次数最多的结果 # sortedClassCount = sorted(classCount.items(), key=operator.itemgetter(1), reverse=True) # print('sortedClassCount:', sortedClassCount) # return sortedClassCount[0][0] # -----------多数表决实现的方式二----------------- major_label = Counter(classList).most_common(1)[0] print('sortedClassCount:', major_label[0]) return major_label[0] 调用方法： # 6创建决策树 createTree(dataset, label) 运行结果： {&#39;no surfacing&#39;: {0: &#39;no&#39;, 1: {&#39;flippers&#39;: {0: &#39;no&#39;, 1: &#39;yes&#39;}}}} 结果分析：此时，每次生成决策树数据都需要大量的计算，并且耗时，最好是每次直接调用生成结果。这里就需要使用Python模块pickle序列化对象，其存储决策树读取决策树代码实现如下：1234567891011121314151617'''使用pickle模块存储决策树'''def storeTree(inputTree, filename): import pickle # -------------- 第一种方法 -------------- fw = open(filename, 'wb') pickle.dump(inputTree, fw) fw.close() # -------------- 第二种方法 -------------- with open(filename, 'wb') as fw: pickle.dump(inputTree, fw)def grabTree(filename): import pickle fr = open(filename,'rb') return pickle.load(fr) 测试算法：使用决策树执行分类用决策树进行鱼类属于分类实现如下：1234567891011121314'''用决策树分类函数'''def classify(inputTree, featLabels, testVec): firstStr = list(inputTree.keys())[0] # 获取tree的根节点对于的key值 secondDict = inputTree[firstStr] # 通过key得到根节点对应的value # 判断根节点名称获取根节点在label中的先后顺序，这样就知道输入的testVec怎么开始对照树来做分类 featIndex = featLabels.index(firstStr) for key in secondDict.keys(): if testVec[featIndex] == key: if type(secondDict[key]).__name__ == 'dict': classLabel = classify(secondDict[key], featLabels, testVec) else: classLabel = secondDict[key] print(classLabel) return classLabel 调用方法： # 7 用决策树分类函数 myTree = treePlotter.retrieveTree(0) # print(myTree) classify(myTree,label,[1,0]) 运行结果： 分类结果：no surfacing 决策树分类器实现使用算法此步骤可以适用于任何监督学习任务，而使用决策树可以更好地理解数据的内在含义。1234567891011121314151617181920212223'''决策树判断是否是鱼'''def fishTest(): # 1.创建数据和结果标签 myDat, labels = createDataSet() # 计算label分类标签的香农熵 calcShannonEnt(myDat) # 求第0列 为 1/0的列的数据集【排除第0列】 print('1---', splitDataSet(myDat, 0, 1)) print('0---', splitDataSet(myDat, 0, 0)) # 计算最好的信息增益的列 print(chooseBestFeatureToSplit(myDat)) import copy myTree = createTree(myDat, copy.deepcopy(labels)) print(myTree) # [1, 1]表示要取的分支上的节点位置，对应的结果值 print(classify(myTree, labels, [1, 1])) # 画图可视化展现 treePlotter.createPlot(myTree) 调用决策树分类方法： # 9 决策树判断是否是鱼 fishTest() 运行结果如下： 1--- [[1, 1, &#39;yes&#39;], [1, 1, &#39;yes&#39;], [1, 0, &#39;no&#39;]] 0--- [[0, 1, &#39;no&#39;], [0, 1, &#39;no&#39;]] {&#39;no surfacing&#39;: {0: &#39;no&#39;, 1: {&#39;flippers&#39;: {0: &#39;no&#39;, 1: &#39;yes&#39;}}}} yes 可视化结果 决策树实际应用:预测隐形眼镜的测试代码项目概述隐形眼镜类型包括硬材质、软材质以及不适合佩戴隐形眼镜。我们需要使用决策树预测患者需要佩戴的隐形眼镜类型。 开发流程收集数据: 提供的文本文件。 解析数据: 解析 tab 键分隔的数据行 分析数据: 快速检查数据，确保正确地解析数据内容，使用 createPlot() 函数绘制最终的树形图。 训练算法: 使用 createTree() 函数。 测试算法: 编写测试函数验证决策树可以正确分类给定的数据实例。 使用算法: 存储树的数据结构，以便下次使用时无需重新构造树。 收集数据：提供的文本文件 数据读取 文本文件数据格式如下： young myope no reduced no lenses young myope no normal soft young myope yes reduced no lenses young myope yes normal hard young hyper no reduced no lenses young hyper no normal soft young hyper yes reduced no lenses young hyper yes normal hard pre myope no reduced no lenses pre myope no normal soft pre myope yes reduced no lenses pre myope yes normal hard pre hyper no reduced no lenses pre hyper no normal soft pre hyper yes reduced no lenses pre hyper yes normal no lenses presbyopic myope no reduced no lenses presbyopic myope no normal no lenses presbyopic myope yes reduced no lenses presbyopic myope yes normal hard presbyopic hyper no reduced no lenses presbyopic hyper no normal soft presbyopic hyper yes reduced no lenses presbyopic hyper yes normal no lenses 代码实现: 编写测试函数验证决策树可以正确分类给定的数据实例。12345678910111213'''预测隐形眼镜的测试代码'''def ContactLensesTest(): # 加载隐形眼镜相关的 文本文件 数据 fr = open('lenses.txt') # 解析数据，获得 features 数据 lenses = [inst.strip().split(' ') for inst in fr.readlines()] # 得到数据的对应的 Labels lensesLabels = ['age', 'prescript', 'astigmatic', 'tearRate'] # 使用上面的创建决策树的代码，构造预测隐形眼镜的决策树 lensesTree = createTree(lenses, lensesLabels) print(lensesTree) # 画图可视化展现 treePlotter.createPlot(lensesTree) 运行结果 调用方法 # 10 预测隐形眼镜类型 ContactLensesTest() 运行结果 {&#39;tearRate&#39;: {&#39;reduced&#39;: &#39;no lenses&#39;, &#39;normal&#39;: {&#39;astigmatic&#39;: {&#39;no&#39;: {&#39;age&#39;: {&#39;young&#39;: &#39;soft&#39;, &#39;pre&#39;: &#39;soft&#39;, &#39;presbyopic&#39;: {&#39;prescript&#39;: {&#39;myope&#39;: &#39;no lenses&#39;, &#39;hyper&#39;: &#39;soft&#39;}}}}, &#39;yes&#39;: {&#39;prescript&#39;: {&#39;myope&#39;: &#39;hard&#39;, &#39;hyper&#39;: {&#39;age&#39;: {&#39;young&#39;: &#39;hard&#39;, &#39;pre&#39;: &#39;no lenses&#39;, &#39;presbyopic&#39;: &#39;no lenses&#39;}}}}}}}} 决策树可视化 完整代码下载 源码请进QQ群文件下载： 作者声明 本文版权归作者所有，旨在技术交流使用。未经作者同意禁止转载，转载后需在文章页面明显位置给出原文连接，否则相关责任自行承担。]]></content>
      <categories>
        <category>机器学习</category>
        <category>决策树</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>机器学习算法</tag>
        <tag>sklean</tag>
        <tag>决策树</tag>
        <tag>文本聚类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一步步教你轻松学KNN模型算法]]></title>
    <url>%2F2018%2F09%2F19%2F%E4%B8%80%E6%AD%A5%E6%AD%A5%E6%95%99%E4%BD%A0%E8%BD%BB%E6%9D%BE%E5%AD%A6KNN%E6%A8%A1%E5%9E%8B%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[摘要：机器学习算法中KNN属于比较简单的典型算法，既可以做聚类又可以做分类使用。本文通过一个模拟的实际案例进行讲解。整个流程包括：采集数据、数据格式化处理、数据分析、数据归一化处理、构造算法模型、评估算法模型和算法模型的应用。（本文原创，转载必须注明出处.） 1 理论介绍 什么是KNN？ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;k-近邻（kNN,k-NearestNeighbor）算法是一种基本分类与回归方法，我们这里只讨论分类问题中的 k-近邻算法。k-近邻算法的输入为实例的特征向量，对应于特征空间的点；输出为实例的类别，可以取多类。k-邻算法假设给定一个训练数据集，其中的实例类别已定。分类时，对新的实例，根据其 k 个最近邻的训练实例的类别，通过多数表决等方式进行预测。因此，k近邻算法不具有显式的学习过程即属于有监督学习范畴。k近邻算法实际上利用训练数据集对特征向量空间进行划分，并作为其分类的“模型”。k值的选择、距离度量以及分类决策规则是k近邻算法的三个基本要素。 KNN算法思想 1 计算已知类别中数据集的点与当前点的距离。[即计算所有样本点跟待分类样本之间的距离] 2 按照距离递增次序排序。[计算完样本距离进行排序] 3 选取与当前点距离最小的k个点。[选取距离样本最近的k个点] 4 确定前k个点所在类别的出现频率。[针对这k个点，统计下各个类别分别有多少个] 5 返回前k个点出现频率最高的类别作为当前点的预测分类。[k个点中某个类别最多，就将样本划归改点] KNN工作原理 1 假设有一个带有标签的样本数据集（训练样本集），其中包含每条数据与所属分类的对应关系。 2 输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较。 3 计算新数据与样本数据集中每条数据的距离。 4 对求得的所有距离进行排序（从小到大，越小表示越相似）。 5 取前 k （k 一般小于等于 20 ）个样本数据对应的分类标签。 6 求 k 个数据中出现次数最多的分类标签作为新数据的分类。 KNN算法流程 1 搜集数据：数据采集过程，其分为非结构化数据，半结构化数据和数据化数据。诸如：网络爬取，数据库，文件等。 2 准备数据：格式化处理，对不同类别的数据进行统一的格式化处理。诸如：将pdf，word，excel，sql等等统一转化为txt文本。 3 分析数据：主要看看数据特点，有没有缺失值，数据连续性还是离散型，进而选择不同模型。诸如：可视化数据分析 4 训练数据：不适用于KNN，但是在其他一些监督学习中会经常遇到，诸如：朴素贝叶斯分类等。 5 测试算法：评价指标，如计算错误率，准确率，召回率，F度量值等。 6 应用算法：针对完善的模型进行封装重构，然后进行实际应用。 KNN优缺点 优点：精度高、对异常值不敏感、无数据输入假定 缺点：计算复杂度高、空间复杂度好 适用数据范围：数值型和标称型 2 KNN算法实现与分析2.1 数据准备 创建模拟数据集 描述：现在你来了一个新的任务，任务其实非常简单，就是根据吃冰淇淋和喝水的数量判断成都天气冷热程度。你现在要做的就是去成都春熙路街头采访记录一些游客吃了多少冰淇淋，又喝了几瓶水，他觉得成都天气怎么样（这里只考虑二分类问题，假设只有‘非常热’和‘一般热’）。其中特征向量包括两个分别是冰激凌数t1和喝水数t2，标签类别分别是非常热A和一般热B。 现在我们开始行动，随机采访4个游客（暂时不考虑样本数量问题），询问每个游客吃多少冰淇淋和喝多少水（两个整型的特征向量，不考虑特征重要程度），并记录下他们口中的成都天气感受（非常热A与一般热B）。然后通过采访数据训练一个KNN分类器，新的游客只需要说出特征向量自动判别成都天气冷热程度。创建模拟数据集代码如下： '''KNN创建数据源，返回数据集和标签''' def create_dataset(): group = array(random.randint(0,10,size=(4,2))) # 数据集 labels = ['A','A','B','B'] # 标签 return group,labels 运行查看数据集的特征向量和分类标签： '''1 KNN模拟数据分类算法''' dataset,labels = create_dataset() print('特征集：\n'+str(dataset)) print('标签集：\n'+str(labels)) 运行结果： 特征集： [[8 4] [7 1] [1 4] [3 0]] 标签集： ['A', 'A', 'B', 'B'] 分析解读： 本段代码没有实际意义，只是帮助读者理解特征向量和分类标签。可以这么理解，A代表非常热，B代表一般热，属性1代表吃冰淇淋数量，属性2代表喝水的数量。那么样本数据可以解读为： 游客 冰淇淋 喝水 冷热程度 判断描述 小王 8 4 A 小王吃了8个冰淇淋喝了4瓶水，成都天气非常热 小张 7 1 A 小张吃了7个冰淇淋喝了1瓶水，成都天气非常热 小李 1 4 B 小王吃了1个冰淇淋喝了4瓶水，成都天气一般热 小赵 3 0 B 小王吃了3个冰淇淋喝了0瓶水，成都天气一般热 思考： 计算机是不能直接处理自然语言，往往需要将自然语言转化成特征向量，再通过计算机处理。比如这里不是吃喝看天气情况了，而是垃圾邮件自动识别，我们就需要对邮件转化成数值型特征向量再输入计算机进行处理。 > 规范文件数据集处理 如下是一个规范文件的数据集（已经经过数采集、数据格式化、数据预处理等），特征向量包括3个，样本属于一个多分类的情况。即我们通过周飞行里程数、玩游戏占比、吃冰激凌数量判断一个人的优秀程度。假设1代表普通，2代表比较优秀，3代表非常优秀。（ps：一个人一周都在飞机上度过忙碌的工作，又不太玩游戏，关键还注意饮食，说明优秀是有道理的。） 周飞行里程数（km） 周玩游戏占比（%） 周消耗冰激凌（公升） 样本分类 40920 8.326976 0.953952 3 14488 7.153469 1.673904 2 26052 1.441871 0.805124 1 ... ... ... ... 75136 13.147394 0.428964 1 38344 1.669788 0.134296 1 72993 10.141740 1.032955 1 上面是处理好保存在txt文本的数据，计算机如何去识别并处理这些数据呢？这里我们分别提取特征向量和标签向量。数据集处理代码如下： '''对文件进行格式处理，便于分类器可以理解''' def file_matrix(filename): f = open(filename) arrayLines = f.readlines() returnMat = zeros((len(arrayLines),3)) # 数据集 classLabelVactor = [] # 标签集 index = 0 for line in arrayLines: listFromLine = line.strip().split(' ') # 分析数据，空格处理 returnMat[index,:] = listFromLine[0:3] classLabelVactor.append(int(listFromLine[-1])) index +=1 return returnMat,classLabelVactor 代码说明： 1 zeros(Y,X)：填充矩阵，需要导入NumPy包。Y向量代表样本行数，X向量代表样本特征数即列数。 2 returnMat[index,:]：遍历样本特征向量 运行查看数据集的特征向量和分类标签： ''' KNN针对文件的分类算法''' filename = os.path.abspath(r'./datasource/datingTestSet2.txt') dataset,labels = file_matrix(filename) print('特征集：\n'+str(dataset)) print('标签集：\n'+str(labels)) 运行结果： 特征集： [[4.0920000e+04 8.3269760e+00 9.5395200e-01] [1.4488000e+04 7.1534690e+00 1.6739040e+00] [2.6052000e+04 1.4418710e+00 8.0512400e-01] ... [2.6575000e+04 1.0650102e+01 8.6662700e-01] [4.8111000e+04 9.1345280e+00 7.2804500e-01] [4.3757000e+04 7.8826010e+00 1.3324460e+00]] 标签集： [3, 2, 1, 1, 1, 1, 3, 3, 1, 3, 1, 1, 2, 1, 1, 1, 1, 1, 2, 3, 2, 1, 2, 3, 2, 3, 2, 3, 2, 1, 3, 1, 3, 1, 2, 1, 1, 2, 3, 3, 1, 2, 3, 3, 3, ... 3, 3, 1, 2, 3, 1, 3, 1, 2, 2, 1, 1, 1, 1, 1] > 不规范数据集处理 这里我们只是提供一个思路。比如做文本分类算法，如何将一篇篇新闻转化为规范的数值型数据集呢。假设Z政治新闻100篇，T体育新闻100篇，Y娱乐新闻100篇，K科技新闻100篇。我们进行分类： 1 遍历所有文本转化统一的txt格式文件（2.2节会讲到） 2 对全部ZTYK文本进行分词和停用词处理。 3 统计全部样本词频尺寸（可以采用TF-IDF或者深度学习方法处理）。 4 每个样本进行词频统计 最终模拟效果如下： 样本 词1 词2 词3 词4 ... 词n 标签 p1 200 0 100 50 ... 20 Z p2 100 0 80 40 ... 10 Z p3 0 100 5 5 ... 200 T p4 6 230 40 12 ... 670 T p5 0 2 110 57 ... 234 Y ... ... ... ... ... ... ... ... pn 123 45 0 580 ... 24 K ### 2.2 数据格式化 > 数据文件转化 自然语言处理、数据挖掘、机器学习技术应用愈加广泛。针对大数据的预处理工作是一项庞杂、棘手的工作。首先数据采集和存储，尤其高质量数据采集往往不是那么简单。采集后的信息文件格式不一，诸如pdf，doc，docx，Excel，ppt等多种形式。然而最常见便是txt、pdf和word类型的文档。这里所谓格式转化主要对pdf和word文档进行文本格式转换成txt。格式一致化以后再进行后续预处理工作。具体详情请参照之前写的数据分析：基于Python的自定义文件格式转换系统一文。 > 文件格式化处理 这里可以采用多种方式，诸如上文提到的矩阵填充法，当然也可以采用现成的工具。比如百度的Echarts中表格数据转换工具。其支持纯数组的转换，数组+对象，地理坐标等方式，还支持json数据的转化，这对使用百度EChart可视化是非常友好的，也有助于可视化数据分析。文本数据格式效果如下图： ![](https://i.imgur.com/8hDPG00.png) [ ['40920 8.326976 0.953952 3'], ['14488 7.153469 1.673904 2'], ['26052 1.441871 0.805124 1'], ['75136 13.147394 0.428964 1'], ['38344 1.669788 0.134296 1'], ['72993 10.141740 1.032955 1'], ... ['35948 6.830792 1.213192 3'], ['42666 13.276369 0.543880 3'], ['67497 8.631577 0.749278 1'], ['35483 12.273169 1.508053 3'], ['50242 3.723498 0.831917 1'], ['63275 8.385879 1.669485 1'], ['5569 4.875435 0.728658 2'], ['15669 0.000000 1.250185 2'], ['28488 10.528555 1.304844 3'], ['6487 3.540265 0.822483 2'], ['37708 2.991551 0.833920 1'] ] ### 2.3 数据归一化 > 数据归一化 机器学习、数据挖掘、自然语言处理等数据科学工作中，数据前期准备、数据预处理过程、特征提取等几个步骤比较花费时间。同时，数据预处理的效果也直接影响了后续模型能否有效的工作。然而，目前的很多研究主要集中在模型的构建、优化等方面，对数据预处理的理论研究甚少，很多数据预处理工作仍然是靠工程师的经验进行的。也不是所有数据都需要归一化，诸如 1. 数据类型一致且分布均匀。 2. 概率模型可以不做归一化，如决策树。 > 数据归一化优点 1. 归一化后加快了梯度下降求最优解的速度; 2. 归一化有可能提高精度; > 归一化方法 1 sklearn线性归一化 # 线性函数将原始数据线性化的方法转换到[0, 1]的范围 min_max_scaler = preprocessing.MinMaxScaler() X_train_minmax = min_max_scaler.fit_transform(X_train) X_test_minmax = min_max_scaler.transform(X_test) 2 标准差标准化 # 经过处理的数据符合标准正态分布，即均值为0，标准差为1，其转化函数为： scaler = preprocessing.StandardScaler().fit(X_train) scaler.transform(X_test) 3 非线性归一化 经常用在数据分化比较大的场景，有些数值很大，有些很小。通过一些数学函数，将原始值进行映射。该方法包括 log、指数，正切等。需要根据数据分布的情况，决定非线性函数的曲线，比如log(V, 2)还是log(V, 10)等。 > 线性归一化方法代码实现 '''数值归一化：特征值转化为0-1之间：newValue = (oldValue-min)/(max-min)''' def norm_dataset(dataset): minVals = dataset.min(0) # 参数0是取得列表中的最小值，而不是行中最小值 maxVals = dataset.max(0) ranges = maxVals - minVals normdataset = zeros(shape(dataset)) # 生成原矩阵一样大小的0矩阵 m = dataset.shape[0] # tile:复制同样大小的矩阵 molecular = dataset - tile(minVals,(m,1)) # 分子： (oldValue-min) Denominator = tile(ranges,(m,1)) # 分母：(max-min) normdataset = molecular/Denominator # 归一化结果。 return normdataset,ranges,minVals 数据归一化前： 归一化的数据结果： [[4.0920000e+04 8.3269760e+00 9.5395200e-01] [1.4488000e+04 7.1534690e+00 1.6739040e+00] [2.6052000e+04 1.4418710e+00 8.0512400e-01] ... [2.6575000e+04 1.0650102e+01 8.6662700e-01] [4.8111000e+04 9.1345280e+00 7.2804500e-01] [4.3757000e+04 7.8826010e+00 1.3324460e+00]] 展开第一条信息“40920 8.326976 0.953952 3”，其中里程40000多，而公升数才0.9.两者根本不在同一个数量级上面，也就是说，如果特征属性相同的情况下，公升数即使变动100倍对里程数的影响也微乎其微。而里程数轻微变化就直接影响公升数的结果。所以我们将其放在同一尺度下进行处理，也就是本文采用的线性缩放方，数据归一化后结果如下： 归一化的数据结果： [[0.44832535 0.39805139 0.56233353] [0.15873259 0.34195467 0.98724416] [0.28542943 0.06892523 0.47449629] ... [0.29115949 0.50910294 0.51079493] [0.52711097 0.43665451 0.4290048 ] [0.47940793 0.3768091 0.78571804]] 分析： 经过上述归一化处理后，各个特征指标都是0-1这样一个范畴中进行比较。当然实际工作中不同特征的权重不同，这个可以通过增加权重方法处理即可，本文不在进行深入讨论。 ### 2.4 数据分析 > 基于matplotlib的可视化分析 我们对数据处理后，很不容易进行数据分析。毕竟密密麻麻的数字略显冰冷无趣。我们可以将其可视化展示出来，进而查看数据稀疏程度，离散程度等等。我们查看'玩游戏所耗时间百分比','每周消耗在冰淇淋的公升数'两个属性的散点图，实现代码如下： ''' 散列表分析数据： dataset：数据集 datingLabels：标签集 Title:列表，标题、横坐标标题、纵坐标标题。 ''' def analyze_data_plot(dataset,datingLabels,Title): fig = plt.figure() # 将画布划分为1行1列1块 ax = fig.add_subplot(111) ax.scatter(dataset[:,1],dataset[:,2],15.0*array(datingLabels),15.0*array(datingLabels)) # 设置散点图标题和横纵坐标标题 plt.title(Title[0],fontsize=25,fontname='宋体',fontproperties=myfont) plt.xlabel(Title[1],fontsize=15,fontname='宋体',fontproperties=myfont) plt.ylabel(Title[2],fontsize=15,fontname='宋体',fontproperties=myfont) # 设置刻度标记大小,axis='both'参数影响横纵坐标，labelsize刻度大小 plt.tick_params(axis='both',which='major',labelsize=10) # 设置每个坐标轴取值范围 # plt.axis([-1,25,-1,2.0]) # 截图保存图片 # plt.savefig('datasets_plot.png',bbox_inches='tight') # 显示图形 plt.show() 这里注意一个问题，横纵坐标是乱码显示，解决这个问题，添加如下代码： #加入中文显示 import matplotlib.font_manager as fm # 解决中文乱码，本案例使用宋体字 myfont=fm.FontProperties(fname=r"C:\\Windows\\Fonts\\simsun.ttc") 调用可视化数据分析方法如下： ''' 文件数据图形化分析数据 ''' dataset,labels = file_matrix(filename) noredataset = norm_dataset(dataset)[0] # 数据归一化 title = ['约会数据游戏和饮食散列点','玩游戏所耗时间百分比','每周消耗在冰淇淋的公升数'] visualplot.analyze_data_plot(noredataset,labels,title) 游戏占比与冰淇淋公升数关系散点图可视化： ![](https://i.imgur.com/KPIb9fs.png) 折线图代码实现如下： '''折线图''' def line_chart(xvalues,yvalues): # 绘制折线图,c颜色设置，alpha透明度 plt.plot(xvalues,yvalues,linewidth=0.5,alpha=0.5,c='red') # num_squares数据值，linewidth设置线条粗细 # 设置折线图标题和横纵坐标标题 plt.title("Python绘制折线图",fontsize=30,fontname='宋体',fontproperties=myfont) plt.xlabel('横坐标',fontsize=20,fontname='宋体',fontproperties=myfont) plt.ylabel('纵坐标',fontsize=20,fontname='宋体',fontproperties=myfont) # 设置刻度标记大小,axis='both'参数影响横纵坐标，labelsize刻度大小 plt.tick_params(axis='both',labelsize=14) # 显示图形 plt.show() 游戏占比与冰淇淋公升数关系折线图可视化：（此处折线图明显不合适，只是突出另一种分析方式。） ![](https://i.imgur.com/m1b96kb.png) 扩展： 更多matplotlib可视化实现效果图参考文章 70个注意的Python小Notes:完整的matplotlib可视化 > 基于Echart的可视化分析 我们上面采用的matplotlib可视化效果，采用该方式主要是结合python数据分析绑定比较方便。有些时候我们为了取得更加漂亮的可视化效果，可以选择百度echart进行分析，百度Echart使用简单且文档规范容易上手。我们对原数据进行分析并转化为json代码： '''array数据转化json''' def norm_Json(dataset): noredataset = norm_dataset(dataset)[0] # 数据归一化 number1 = np.around(noredataset[:,1], decimals=4) # 获取数据集第二列 number2 = np.around(noredataset[:,2], decimals=4) # 获取数据集第三列 returnMat=zeros((dataset.shape[0],2)) # 二维矩阵 returnMat[:,0] = number1 returnMat[:,1] = number2 file_path = os.path.abspath(r"./datasource/test.json") json.dump(returnMat.tolist(), codecs.open(file_path, 'w', encoding='utf-8'), separators=(',', ':'), sort_keys=True, indent=4) 生成json数据保存在指定文件中，打开文件查看数据如下： [ [ 0.3981, 0.5623 ], [ 0.342, 0.9872 ], [ 0.0689, 0.4745 ], [ 0.6285, 0.2525 ] ... [ 0.4367, 0.429 ], [ 0.3768, 0.7857 ] ] 从百度Echart实例中选择一种散点图并绑定json文件，其html代码如下： 图案例 //初始化echarts实例 var myChart = echarts.init(document.getElementById('chartmain')); $.getJSON('game-food.json', function (data) { var option = { title: { text: '玩游戏时间占比和饮食数据描述', left: 'center', top: 0 }, visualMap: { min: 15202, max: 159980, dimension: 1, orient: 'vertical', right: 10, top: 'center', text: ['优秀', '一般'], calculable: true, inRange: { color: ['#f2c31a', '#24b7f2'] } }, tooltip: { trigger: 'item', axisPointer: { type: 'cross' } }, xAxis: [{ type: 'value' }], yAxis: [{ type: 'value' }], series: [{ name: 'price-area', type: 'scatter', symbolSize: 5, data: data }] }; myChart.setOption(option); }); json文件读取需要在web运行环境中，单纯的运行效果如下图所示： ![](https://i.imgur.com/CE8vS7x.png) > 数据转化工具 本文采用自己构建的方式进行json文件生成，此外我们也可以采用现有的数据转化工具进行处理。比如百度的表格数据转化工具（2.2节已经介绍了）。 另一个便是在线json验证格式工具:http://www.bejson.com/ ### 2.5 KNN分类器实现 通过数据分析，我们查看数据样本是否偏态分别，数据规模情况等等。针对性进行数据预处理后，编写具体算法模型。本文主要是KNN分类器，其代码如下： ''' 构造KNN分类器 vecX:输入向量，待测数据 filename: 特征集文件路径 isnorm:是否进行归一化处理 k:k值的选择，默认选择3 ''' def knn_classifier(vecX,dataset,labels,isnorm='Y',k=3): # 距离计算（方法1） if isnorm == 'Y': normMat,ranges,minVals = norm_dataset(dataset) # 对数据进行归一化处理 normvecX = norm_dataset(vecX) else: normMat = dataset normvecX = vecX m = normMat.shape[0] # tile方法是在列向量vecX，datasetSize次，行向量vecX1次叠加 diffMat = tile(normvecX,(m,1)) - normMat sqDiffMat = diffMat ** 2 sqDistances = sqDiffMat.sum(axis=1) # axis=0 是列相加,axis=1是行相加 distances = sqDistances**0.5 # print('vecX向量到数据集各点距离：\n'+str(distances)) sortedDistIndexs = distances.argsort(axis=0) # 距离排序，升序 # print(sortedDistIndicies) classCount = {} # 统计前k个类别出现频率 for i in range(k): votelabel = labels[sortedDistIndexs[i]] classCount[votelabel] = classCount.get(votelabel,0) + 1 #统计键值 # 类别频率出现最高的点,itemgetter(0)按照key排序，itemgetter(1)按照value排序 sortedClassCount = sorted(classCount.items(),key=operator.itemgetter(1),reverse=True) print(str(vecX)+'KNN的投票决策结果：\n'+str(sortedClassCount[0][0])) return sortedClassCount[0][0] 3 KNN算法模型评估3.1 评价指标介绍 基本知识 混淆矩阵：正元组和负元组的合计 评估度量：（其中P:正样本数 N：负样本数 TP：真正例 TN：真负例 FP：假正例 FN：假负例） 注意：学习器的准确率最好在检验集上估计，检验集的由训练集模型时未使用的含有标记的元组组成数据。 各参数描述如下： TP（真正例/真阳性）：是指被学习器正确学习的正元组，令TP为真正例的个数。 TN（真负例/真阴性）：是指被学习器正确学习的负元组，令TN为真负例的个数。 FP（假正例/假阳性）：是被错误的标记为正元组的负元组。令FP为假正例的个数。 FN（假负例/假阴性）：是被错误的标记为负元组的正元组。令FN为假负例的个数。 准确率：正确识别的元组所占的比例。 评价指标优点 一般采用精确率和召回率作为度量的方法具有以下几个优点： (1) 准确率数值对于小数据不是特别敏感，而精确率和召回率对于这样数据比较敏感。(2) 在相同实验环境下，F度量这种倾向和我们直观感觉是一致的，我们对目标事件很敏感，甚至返回一些垃圾数据也在所不惜。(3) 通过精确率和找回来衡量目标事件和垃圾事件的差异。 模型评估拓展 参见《自然语言处理理论与实战》一书第13章模型评估。 3.2 评估算法模型实现本文只是对错误率进行评估，其也是knn分类器核心指标，实现代码如下： ‘’’测试评估算法模型’’’def test_knn_perfor(filename): hoRatio = 0.1 dataset,label = file_matrix(filename) # 获取训练数据和标签 normMat,ranges,minVals = norm_dataset(dataset) # 对数据进行归一化处理 m = normMat.shape[0] numTestVecs = int(m*hoRatio) # 10%的测试数据条数 errorCount = 0.0 # 统计错误分类数 for i in range(numTestVecs): classifierResult = knn_classifier(normMat[i,:],normMat[numTestVecs:m,:],label[numTestVecs:m],3) # 此处分类器可以替换不同分类模型 print(&#39;分类结果:&#39;+str(classifierResult)+&#39;\t\t准确结果:&#39;+str(label[i])) if classifierResult != label[i]: errorCount += 1.0 Global.all_FileNum += 1 print(&#39;总的错误率为：&#39;+str(errorCount/float(numTestVecs))+&quot;\n总测试数据量: &quot;+str(Global.all_FileNum)) &lt;/pre&gt;运行效果如下： [0.44832535 0.39805139 0.56233353]KNN的投票决策结果： 分类结果:3 准确结果:3 [0.15873259 0.34195467 0.98724416]KNN的投票决策结果： 分类结果:2 准确结果:2 ... 分类结果:3 准确结果:3 [0.19385799 0.30474213 0.01919426]KNN的投票决策结果： 分类结果:2 准确结果:2 [0.24463971 0.10813023 0.60259472]KNN的投票决策结果： 分类结果:1 准确结果:1 [0.51022756 0.27138082 0.41804137]KNN的投票决策结果： 分类结果:3 准确结果:1 总的错误率为：0.05 总测试数据量: 100 耗时：0.0300 s 评估结果分析： 本文采用封闭评估的方法，前100条数据作为测试集，后900条数据作为训练集。如上结果最后一条信息表明knn分类器的结果是3，而标准结果是1.knn分类存在错误。将所有错误占比分析出来即错误率。本文错误率5%，即准确率95%. 读者可以选取200:800、300:700等等数据进行测试查看错误率。 4 KNN算法模型的实际应用 knn分类器应用 经过如上的改进最终形成实际应用的算法模型API开发给外部程序使用，调用knn算法代码如下： ‘’’调用可用算法’’’def show_classifyPerson(filename): resultlist = [‘不喜欢’,’还可以’,’特别喜欢’] ffMiles = float(input(‘每年飞行的历程多少公里？\n’)) percentTats = float(input(‘玩游戏时间占百分比多少？\n’)) # [751,13,0.4][40920 ,8.326976,0.953952] iceCream = float(input(‘每周消费冰淇淋多少公升？\n’)) dataset,labels = file_matrix(filename) # 数据格式化处理 inArr = array([ffMiles,percentTats,iceCream]) classifierResult = knn_classifier(inArr,dataset,labels,3) # 数据归一化并进行分类 print(&#39;预测的约会结果是：&#39;+resultlist[classifierResult-1]) &lt;/pre&gt;运行结果如下： 每年飞行的历程多少公里？ 10000 玩游戏时间占百分比多少？ 10 每周消费冰淇淋多少公升？ 0.5 KNN的投票决策结果： 2 预测的约会结果是：还可以 展望 我们还可以采用knn分类器进行实际应用，比如新闻分类系统。大致思路如下： 1 采集数据：选用复旦大学的文本分类新闻语料2 准备数据：数据格式化、分词、停用词处理等3 分析数据：看看数据特点，有没有缺失值，数据连续性还是离散型，进而选择不同模型。诸如：可视化数据分析4 数据转化：采用IF-IDF或者神经网络的方法对词频进行处理，最终转化为机器可以处理的数值型矩阵。5 构建模型：KNN新闻分类器模型构建。6 测试算法：评价指标，如计算错误率，准确率，召回率，F度量值等。7 应用算法：针对完善的模型进行封装重构，然后进行实际的新闻分类应用。 参考文献 归一化学习：https://blog.csdn.net/hyq3235356/article/details/78472307 归一化方法：https://blog.csdn.net/zxd1754771465/article/details/73558103 百度Echart转化工具：http://echarts.baidu.com/spreadsheet.html 在线json验证格式工具:http://www.bejson.com/ 模型评估：http://www.cnblogs.com/baiboy/p/mxpg2.html 完整代码下载源码请进QQ群文件下载： 作者声明 本文版权归作者所有，旨在技术交流使用。未经作者同意禁止转载，转载后需在文章页面明显位置给出原文连接，否则相关责任自行承担。]]></content>
      <categories>
        <category>机器学习</category>
        <category>KNN</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>机器学习算法</tag>
        <tag>sklean</tag>
        <tag>文本聚类</tag>
        <tag>K近邻算法</tag>
        <tag>kNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一步步教你轻松学K-means聚类算法]]></title>
    <url>%2F2018%2F09%2F19%2F%E4%B8%80%E6%AD%A5%E6%AD%A5%E6%95%99%E4%BD%A0%E8%BD%BB%E6%9D%BE%E5%AD%A6K-means%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[摘要：k-均值算法（英文：k-means clustering），属于比较常用的算法之一，文本首先介绍聚类的理论知识包括什么是聚类、聚类的应用、聚类思想、聚类优缺点等等；然后通过k-均值聚类案例实现及其可视化有一个直观的感受，针对算法模型进行分析和结果优化提出了二分k-means算法。最后我们调用机器学习库函数，很短的代码完成聚类算法。（本文原创，转载必须注明出处.） 理论介绍聚类 什么是聚类 统计数据分析的一门技术，在许多领域受到广泛应用，包括机器学习，数据挖掘，模式识别，图像分析以及生物信息。聚类是把相似的对象通过静态分类的方法分成不同的组别或者更多的子集（subset），这样让在同一个子集中的成员对象都有相似的一些属性，常见的包括在坐标系中更加短的空间距离等。 聚类的应用 在商务上，聚类能帮助市场分析人员从客户基本库中发现不同的客户群，并且用购买模式来刻画不同的客户群的特征。在生物学上，聚类能用于推导植物和动物的分类，对基因进行分类，获得对种群中固有结构的认识。聚类在地球观测数据库中相似地区的确定，汽车保险单持有者的分组，及根据房子的类型、价值和地理位置对一个城市中房屋的分组上也可以发挥作用。聚类也能用于对Web上的文档进行分类，以发现信息。诸如此类，聚类有着广泛的实际应用。 K-means（k均值）聚类算法 什么是k-means聚类算法 k-平均算法（英文：k-means clustering）源于信号处理中的一种向量量化方法，现在则更多地作为一种聚类分析方法流行于数据挖掘领域。k-平均聚类的目的是：把 n个点划分到k个聚类中，使得每个点都属于离他最近的均值（此即聚类中心）对应的聚类，以之作为聚类的标准。k-平均聚类与k-近邻之间没有任何关系（后者是另一流行的机器学习技术）。 K-Means 是发现给定数据集的 K 个簇的聚类算法, 之所以称之为 K-均值 是因为它可以发现 K 个不同的簇, 且每个簇的中心采用簇中所含值的均值计算而成.簇个数 K 是用户指定的, 每一个簇通过其质心（centroid）, 即簇中所有点的中心来描述.聚类与分类算法的最大区别在于, 分类的目标类别已知, 而聚类的目标类别是未知的. 发展历史 虽然其思想能够追溯到1957年的Hugo Steinhaus，术语“k-均值”于1967年才被James MacQueen 首次使用。标准算法则是在1957年被Stuart Lloyd作为一种脉冲码调制的技术所提出，但直到1982年才被贝尔实验室公开出版。在1965年，E.W.Forgy发表了本质上相同的方法，所以这一算法有时被称为Lloyd-Forgy方法。更高效的版本则被Hartigan and Wong提出。 算法描述 已知观测集\((x_1,x_2,…,x_n)\)，其中每个观测都是一个 d-维实向量，k-平均聚类要把这 n个观测划分到k个集合中(k≤n),使得组内平方和最小。换句话说，它的目标是找到使得下式满足的聚类\( S_i\)， 其中 \( \mu_i\) 是\(S_i\) 中所有点的均值。 k-means术语 簇: 所有数据的点集合，簇中的对象是相似的。 质心: 簇中所有点的中心（计算所有点的均值而来）. SSE: Sum of Sqared Error（误差平方和）, 它被用来评估模型的好坏，SSE 值越小，表示越接近它们的质心. 聚类效果越 好。由于对误差取了平方，因此更加注重那些远离中心的点（一般为边界点或离群点）。详情见kmeans的评价标准。有关 簇 和 质心 术语更形象的介绍, 请参考下图: k-means应用场景 kmeans，用于数据集内种类属性不明晰，希望能够通过数据挖掘出或自动归类出有相似特点的对象的场景。其商业界的应用场景一般为挖掘出具有相似特点的潜在客户群体以便公司能够重点研究、对症下药。 例如，在2000年和2004年的美国总统大选中，候选人的得票数比较接近或者说非常接近。任一候选人得到的普选票数的最大百分比为50.7%而最小百分比为47.9% 如果1%的选民将手中的选票投向另外的候选人，那么选举结果就会截然不同。 实际上，如果妥善加以引导与吸引，少部分选民就会转换立场。尽管这类选举者占的比例较低，但当候选人的选票接近时，这些人的立场无疑会对选举结果产生非常大的影响。如何找出这类选民，以及如何在有限的预算下采取措施来吸引他们？ 答案就是聚类（Clustering)。 那么，具体如何实施呢？首先，收集用户的信息，可以同时收集用户满意或不满意的信息，这是因为任何对用户重要的内容都可能影响用户的投票结果。然后，将这些信息输入到某个聚类算法中。接着，对聚类结果中的每一个簇（最好选择最大簇 ）， 精心构造能够吸引该簇选民的消息。最后， 开展竞选活动并观察上述做法是否有效。 另一个例子就是产品部门的市场调研了。为了更好的了解自己的用户，产品部门可以采用聚类的方法得到不同特征的用户群体，然后针对不同的用户群体可以对症下药，为他们提供更加精准有效的服务。 k-means算法思想 先随机选取K个对象作为初始的聚类中心。然后计算每个对象与各个种子聚类中心之间的距离，把每个对象分配给距离它最近的聚类中心。聚类中心以及分配给它们的对象就代表一个聚类。一旦全部对象都被分配了，每个聚类的聚类中心会根据聚类中现有的对象被重新计算。这个过程将不断重复直到满足某个终止条件。终止条件可以是以下任何一个： 没有（或最小数目）对象被重新分配给不同的聚类。 没有（或最小数目）聚类中心再发生变化。 误差平方和局部最小。 得到相互分离的球状聚类，在这些聚类中，均值点趋向收敛于聚类中心。 一般会希望得到的聚类大小大致相当，这样把每个观测都分配到离它最近的聚类中心（即均值点）就是比较正确的分配方案。 k-means工作流程 创建 k 个点作为起始质心（通常是随机选择） 当任意一个点的簇分配结果发生改变时（不改变时算法结束） 对数据集中的每个数据点 对每个质心 计算质心与数据点之间的距离 将数据点分配到距其最近的簇 对每一个簇, 计算簇中所有点的均值并将均值作为质心 k-means开发流程 收集数据：使用任意方法 准备数据：需要数值型数据类计算距离, 也可以将标称型数据映射为二值型数据再用于距离计算 分析数据：使用任意方法 训练算法：不适用于无监督学习，即无监督学习不需要训练步骤 测试算法：应用聚类算法、观察结果.可以使用量化的误差指标如误差平方和（后面会介绍）来评价算法的结果. 使用算法：可以用于所希望的任何应用.通常情况下, 簇质心可以代表整个簇的数据来做出决策. k-means评价标准 k-means算法因为手动选取k值和初始化随机质心的缘故，每一次的结果不会完全一样，而且由于手动选取k值，我们需要知道我们选取的k值是否合理，聚类效果好不好，那么如何来评价某一次的聚类效果呢？也许将它们画在图上直接观察是最好的办法，但现实是，我们的数据不会仅仅只有两个特征，一般来说都有十几个特征，而观察十几维的空间对我们来说是一个无法完成的任务。因此，我们需要一个公式来帮助我们判断聚类的性能，这个公式就是SSE (Sum of Squared Error, 误差平方和 ），它其实就是每一个点到其簇内质心的距离的平方值的总和，这个数值对应kmeans函数中clusterAssment矩阵的第一列之和。 SSE值越小表示数据点越接近于它们的质心，聚类效果也越好。 因为对误差取了平方，因此更加重视那些远离中心的点。一种肯定可以降低SSE值的方法是增加簇的个数，但这违背了聚类的目标。聚类的目标是在保持簇数目不变的情况下提高簇的质量。 k-means优缺点 优点: 属于无监督学习，无须准备训练集 原理简单，实现起来较为容易 结果可解释性较好 缺点: 聚类数目k是一个输入参数。选择不恰当的k值可能会导致糟糕的聚类结果。这也是为什么要进行特征检查来决定数据集的聚类数目了。 可能收敛到局部最小值, 在大规模数据集上收敛较慢 对于异常点、离群点敏感 使用数据类型 : 数值型数据 k-means聚类算法实现案例描述我们假设这样的一个案例需求：某公司发布一批新型手机，根据客户热衷度进行投放。公司市场人员收集其中四个地区用户对手机的满意程度（由两个特征决定的）。分析哪个区域对手机产品比较热衷，对应的进行市场销售工作。这里就用到k-means聚类算法。 从文件加载数据集上文中我们收集好四个地区用户对产品满意的特征数据值，转化为向量预先保存到文本中（关于词向量转化及其词袋模型问题，参考：决策树算法模型研究与案例分析一文）。我们加载文件并以数据矩阵形式返回数据集，代码实现如下： '''加载数据集''' def loadDataSet(fileName): dataSet = [] # 初始化一个空列表 fr = open(fileName) for line in fr.readlines(): # 切割每一行的数据 curLine = line.strip().split('\t') # 将数据追加到dataMat，映射所有的元素为 float类型 fltLine = list(map(float,curLine)) dataSet.append(fltLine) return mat(dataSet) 我们打印看下结果： ![](https://i.imgur.com/zItcW74.png) ## 计算两个向量的欧氏距离 上文在k均值算法思想和工作流程都提到过，我们一个重要的方法就是随机设置质心，然后比较每条数据（可以理解为单一客户的特征数据）与质心之间的距离。这里距离公式包括很多，本文采用的是欧式距离计算，其代码实现如下： '''欧氏距离计算函数''' def distEclud(vecA, vecB): return sqrt(sum(power(vecA - vecB, 2))) 构建一个包含 K 个随机质心的集合接下来，我们构建随机质心（中心点），这里的K值是经过数据观察随机设置的值，假如k=3，代表我们将数据集分为3个簇，也就是说分为3个部分。我们随机质心在整个数据集的边界之内，这可以通过找到数据集每一维的最小和最大值来完成,然后生成0到1.0之间的随机数并通过取值范围和最小值,以便确保随机点在数据的边界之内。 ‘’’随机质心‘’’def randCent(dataMat, k): # 获取样本数与特征值 m, n = shape(dataMat) # 初始化质心,创建(k,n)个以零填充的矩阵 centroids = mat(zeros((k, n))) # 循环遍历特征值 for j in range(n): # 计算每一列的最小值 minJ = min(dataMat[:, j]) # 计算每一列的范围值 rangeJ = float(max(dataMat[:, j]) - minJ) # 计算每一列的质心,并将值赋给centroids centroids[:, j] = mat(minJ + rangeJ * random.rand(k, 1)) # 返回质心 return centroids &lt;/pre&gt;我们测试下k=3的随机质心结果： K-Means 聚类算法我们基于以上算法构建k均值算法，该算法会创建k个质心，然后将每个点分配到最近的质心，再重新计算质心。这个过程重复数次，直到数据点的簇分配结果不再改变位置。返回类质心与点分配结果（多次运行结果可能会不一样，可以试试，原因为随机质心的影响，但总的结果是对的，因为数据足够相似，也可能会陷入局部最小值），代码实现如下： ‘’’创建K个质心,然后将每个点分配到最近的质心,再重新计算质心。这个过程重复数次,直到数据点的簇分配结果不再改变为止‘’’def kMeans(dataMat, k, distMeas=distEclud, createCent=randCent): # 获取样本数和特征数 m, n = shape(dataMat) # 初始化一个矩阵来存储每个点的簇分配结果 # clusterAssment包含两个列:一列记录簇索引值,第二列存储误差(误差是指当前点到簇质心的距离,后面会使用该误差来评价聚类的效果) clusterAssment = mat(zeros((m, 2))) # 创建质心,随机K个质心 centroids = createCent(dataMat, k) # 初始化标志变量,用于判断迭代是否继续,如果True,则继续迭代 clusterChanged = True while clusterChanged: clusterChanged = False # 遍历所有数据找到距离每个点最近的质心, # 可以通过对每个点遍历所有质心并计算点到每个质心的距离来完成 for i in range(m): minDist = inf # 正无穷 minIndex = -1 for j in range(k): # 计算数据点到质心的距离 # 计算距离是使用distMeas参数给出的距离公式,默认距离函数是distEclud distJI = distMeas(centroids[j, :], dataMat[i, :]) # 如果距离比minDist(最小距离)还小,更新minDist(最小距离)和最小质心的index(索引) if distJI &lt; minDist: minDist = distJI minIndex = j # 如果任一点的簇分配结果发生改变,则更新clusterChanged标志 if clusterAssment[i, 0] != minIndex: # print(clusterAssment[i, 0],minIndex) clusterChanged = True # 更新簇分配结果为最小质心的index(索引),minDist(最小距离)的平方 clusterAssment[i, :] = minIndex, minDist ** 2 # print(centroids) # 遍历所有质心并更新它们的取值 for cent in range(k): # 通过数据过滤来获得给定簇的所有点 ptsInClust = dataMat[nonzero(clusterAssment[:, 0].A == cent)[0]] # 计算所有点的均值,axis=0表示沿矩阵的列方向进行均值计算 centroids[cent, :] = mean(ptsInClust, axis=0)# axis=0列方向 # 返回所有的类质心与点分配结果 return centroids, clusterAssment &lt;/pre&gt;测试查看下运行结果： 分析数据：聚类可视化通过上文返回的数据结果，似乎我们还不能直观感受，接下来我们采用可视化分析方法直观感受下，代码实现如下： ''' 可视化展示 ''' def kmeanShow(dataMat,centers,clusterAssment): plt.scatter(np.array(dataMat)[:, 0], np.array(dataMat)[:, 1], c=np.array(clusterAssment)[:, 0].T) plt.scatter(centers[:, 0].tolist(), centers[:, 1].tolist(), c="r") plt.show() 测试查看可视化结果： 结果讨论与分析 局部最小值（局部最优的结果，但不是全局最优的结果） 上文可视化结果显示，其中两个簇聚集在一起，也就说说没有达到我们预期的效果。出现这个问题有很多原因，可能是k值取的不合适，可能是距离函数不合适，可能是最初随机选取的质心靠的太近，也可能是数据本身分布的问题。 为了解决这个问题，我们可以对生成的簇进行后处理，一种方法是将具有最大SSE值的簇划分成两个簇。具体实现时可以将最大簇包含的点过滤出来并在这些点上运行K-均值算法，令k设为2。 为了保持簇总数不变，可以将某两个簇进行合并。从上图中很明显就可以看出，应该将上图下部两个出错的簇质心进行合并。那么问题来了，我们可以很容易对二维数据上的聚类进行可视化， 但是如果遇到40维的数据应该如何去做？ 有两种可以量化的办法：合并最近的质心，或者合并两个使得SSE增幅最小的质心。 第一种思路通过计算所有质心之间的距离， 然后合并距离最近的两个点来实现。第二种方法需要合并两个簇然后计算总SSE值。必须在所有可能的两个簇上重复上述处理过程，直到找到合并最佳的两个簇为止。 因为上述后处理过程实在是有些繁琐，所以有更厉害的大佬提出了另一个称之为二分K-均值（bisecting K-Means）的算法. 二分 K-Means 聚类算法算法描述该算法首先将所有点作为一个簇，然后将该簇一分为二。之后选择其中一个簇继续进行划分，选择哪一个簇进行划分取决于对其划分时候可以最大程度降低 SSE（平方和误差）的值。上述基于 SSE 的划分过程不断重复，直到得到用户指定的簇数目为止。 二分 K-Means 聚类算法伪代码将所有点看成一个簇 当簇数目小于 k 时 对于每一个簇 计算总误差 在给定的簇上面进行 KMeans 聚类（k=2） 计算将该簇一分为二之后的总误差 选择使得误差最小的那个簇进行划分操作 另一种做法是选择 SSE 最大的簇进行划分，直到簇数目达到用户指定的数目位置。 二分 K-Means 聚类算法代码根据算法思想，我们基于k均值算法做了少许的改动，代码实现如下： ‘’’在给定数据集,所期望的簇数目和距离计算方法的条件下,函数返回聚类结果’’’def biKmeans(dataMat, k, distMeas=distEclud): m, n = shape(dataMat) # 创建一个矩阵来存储数据集中每个点的簇分配结果及平方误差 clusterAssment = mat(zeros((m, 2))) # 计算整个数据集的质心,并使用一个列表来保留所有的质心 centroid0 = mean(dataMat, axis=0).tolist()[0] centList = [centroid0] # [-0.15772275000000002, 1.2253301166666664] # 遍历数据集中所有点来计算每个点到质心的误差值 for j in range(m): clusterAssment[j, 1] = distMeas(mat(centroid0), dataMat[j, :]) ** 2 # 对簇不停的进行划分,直到得到想要的簇数目为止 while (len(centList) &lt; k): # 初始化最小SSE为无穷大,用于比较划分前后的SSE lowestSSE = inf # 通过考察簇列表中的值来获得当前簇的数目,遍历所有的簇来决定最佳的簇进行划分 for i in range(len(centList)): # 对每一个簇,将该簇中的所有点堪称一个小的数据集 ptsInCurrCluster = dataMat[nonzero(clusterAssment[:, 0].A == i)[0], :] # 将ptsInCurrCluster输入到函数kMeans中进行处理,k=2, # kMeans会生成两个质心(簇),同时给出每个簇的误差值 centroidMat, splitClustAss = kMeans(ptsInCurrCluster, 2, distMeas) # 将误差值与剩余数据集的误差之和作为本次划分的误差 sseSplit = sum(splitClustAss[:, 1]) sseNotSplit = sum(clusterAssment[nonzero(clusterAssment[:, 0].A != i)[0], 1]) print(&#39;sseSplit, and notSplit: &#39;, sseSplit, sseNotSplit) # 如果本次划分的SSE值最小,则本次划分被保存 if (sseSplit + sseNotSplit) &lt; lowestSSE: bestCentToSplit = i bestNewCents = centroidMat bestClustAss = splitClustAss.copy() lowestSSE = sseSplit + sseNotSplit # 找出最好的簇分配结果 # 调用kmeans函数并且指定簇数为2时,会得到两个编号分别为0和1的结果簇 bestClustAss[nonzero(bestClustAss[:, 0].A == 1)[0], 0] = len(centList) # 更新为最佳质心 bestClustAss[nonzero(bestClustAss[:, 0].A == 0)[0], 0] = bestCentToSplit print(&#39;the bestCentToSplit is: &#39;, bestCentToSplit) print(&#39;the len of bestClustAss is: &#39;, len(bestClustAss)) # 更新质心列表 # 更新原质心list中的第i个质心为使用二分kMeans后bestNewCents的第一个质心 centList[bestCentToSplit] = bestNewCents[0, :].tolist()[0] # 添加bestNewCents的第二个质心 centList.append(bestNewCents[1, :].tolist()[0]) # 重新分配最好簇下的数据(质心)以及SSE clusterAssment[nonzero(clusterAssment[:, 0].A == bestCentToSplit)[0], :] = bestClustAss return mat(centList), clusterAssment &lt;/pre&gt; 测试二分 KMeans 聚类算法经过改进后，我们运行biKmeans函数得到可视化结果如下： 总结：如此我们得到预想的结果，解决了局部最优的问题，聚类会收敛到全局最小值。而原始的 kMeans() 函数偶尔会陷入局部最小值。 调用机器学习库sklearn实现k-means 聚类加载数据集 加载数据集dataMat = []fr = open(“./testSet2.txt”) # 注意，这个是相对路径for line in fr.readlines(): curLine = line.strip().split(‘\t’) fltLine = list(map(float,curLine)) # 映射所有的元素为 float（浮点数）类型 dataMat.append(fltLine)&lt;/pre&gt; 训练k-means算法模型 km = KMeans(n_clusters=3) # 初始化 km.fit(dataMat) # 拟合 km_pred = km.predict(dataMat) # 预测 centers = km.cluster_centers_ # 质心 可视化结果 plt.scatter(np.array(dataMat)[:, 1], np.array(dataMat)[:, 0], c=km_pred) plt.scatter(centers[:, 1], centers[:, 0], c="r") plt.show() 聚类结果 参考文献 scikit中文社区：http://sklearn.apachecn.org/cn/0.19.0/ 中文维基百科：https://zh.wikipedia.org/wiki/K-%E5%B9%B3%E5%9D%87%E7%AE%97%E6%B3%95 GitHub：https://github.com/BaiNingchao/MachineLearning-1 图书：《机器学习实战》 图书：《自然语言处理理论与实战》 完整代码下载 源码请进【机器学习和自然语言QQ群：436303759】文件下载： 作者声明 本文版权归作者所有，旨在技术交流使用。未经作者同意禁止转载，转载后需在文章页面明显位置给出原文连接，否则相关责任自行承担。]]></content>
      <categories>
        <category>机器学习</category>
        <category>K-Means</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>文本分类</tag>
        <tag>机器学习算法</tag>
        <tag>sklean</tag>
        <tag>K-均值算法</tag>
        <tag>k-means</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一步步教你轻松学逻辑回归模型算法]]></title>
    <url>%2F2018%2F09%2F19%2F%E4%B8%80%E6%AD%A5%E6%AD%A5%E6%95%99%E4%BD%A0%E8%BD%BB%E6%9D%BE%E5%AD%A6%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[摘要：逻辑回归（Logistic regression）即逻辑模型，属于常见的一种分类算法。本文将从理论介绍开始，搞清楚什么是逻辑回归、回归系数、算法思想、工作原理及其优缺点等。进一步通过两个实际案例深化理解逻辑回归，以及在工程应用进行实现。（本文原创，转载必须注明出处.） 理论介绍逻辑回归和Sigmoid 函数 逻辑回归 回归：假设现在有一些数据点，我们用一条直线对这些点进行拟合（这条直线称为最佳拟合直线），这个拟合的过程就叫做回归。 逻辑回归（Logistic Regression）是一种用于解决二分类（0 or 1）问题的机器学习方法，用于估计某种事物的可能性。比如某用户购买某商品的可能性，某病人患有某种疾病的可能性，以及某广告被用户点击的可能性等。 注意，这里用的是“可能性”，而非数学上的“概率”，logisitc回归的结果并非数学定义中的概率值，不可以直接当做概率值来用。该结果往往用于和其他特征值加权求和，而非直接相乘。 Sigmoid 函数 Sigmoid函数是一个常见的S型数学函数，在信息科学中，由于其单增以及反函数单增等性质，Sigmoid函数常被用作神经网络的阈值函数，将变量映射到0,1之间。在逻辑回归、人工神经网络中有着广泛的应用。Sigmoid函数的数学形式是： z=W^TX+b (其中W为权重，b为偏移量)对x求导可以推出如下结论： 下图给出了 Sigmoid 函数在不同坐标尺度下的两条曲线图。当 x 为 0 时，Sigmoid 函数值为 0.5 。随着 x 的增大，对应的 Sigmoid 值将逼近于 1 ; 而随着 x 的减小， Sigmoid 值将逼近于 0 。如果横坐标刻度足够大， Sigmoid 函数看起来很像一个阶跃函数。 因此，为了实现 Logistic 回归分类器，我们可以在每个特征上都乘以一个回归系数，然后把所有结果值相加，将这个总和代入 Sigmoid 函数中，进而得到一个范围在 0~1 之间的数值。任何大于 0.5 的数据被分入 1 类，小于 0.5 即被归入 0 类。所以，Logistic 回归也是一种概率估计，比如这里Sigmoid 函数得出的值为0.5，可以理解为给定数据和参数，数据被分入 1 类的概率为0.5。(注意：针对二分类问题，0.5不是唯一确定分类的值，你可以根据需求调整这个概率值。) 逻辑回归与线性回归的关系 逻辑回归（Logistic Regression）与线性回归（Linear Regression）都是一种广义线性模型（generalized linear model）。逻辑回归假设因变量 y 服从伯努利分布，而线性回归假设因变量 y 服从高斯分布。 因此与线性回归有很多相同之处，去除Sigmoid映射函数的话，逻辑回归算法就是一个线性回归。可以说，逻辑回归是以线性回归为理论支持的，但是逻辑回归通过Sigmoid函数引入了非线性因素，因此可以轻松处理0/1分类问题。 最优化方法的回归系数Sigmoid 函数的输入记为 z ，由下面公式得到: 如果采用向量的写法，上述公式可以写成 Sigmoid 函数计算公式向量形式\( z=w^Tx\)，它表示将这两个数值向量对应元素相乘然后全部加起来即得到 z 值。其中的向量 x 是分类器的输入数据，向量 w 也就是我们要找到的最佳参数（系数），从而使得分类器尽可能地精确。为了寻找该最佳参数，需要用到最优化理论的一些知识。我们这里使用的是——梯度上升法（Gradient Ascent）。 梯度上升与梯度下降 梯度 对于梯度这个词一些人比较陌生，我们先看看维基百科的解释：在向量微积分中，标量场（向量场）中某一点的梯度指向在这点标量场增长最快的方向（当然要比较的话必须固定方向的长度），梯度的绝对值是长度为1的方向中函数最大的增加率，也就是说\( |\nabla f|=\max_{|v|=1} {\nabla_v f}\)，其中 \( \nabla_v\) 代表方向导数。 在单变量的实值函数的情况，梯度只是导数，或者，对于一个线性函数，也就是线的斜率。 梯度一词有时用于斜度，也就是一个曲面沿着给定方向的倾斜程度。可以通过取向量梯度和所研究的方向的内积来得到斜度。梯度的数值有时也被称为梯度。（更多梯度相关知识参照维基百科词条） 梯度形式化描述 考虑一座高度在 (x, y)点是 H(x, y)的山。H这一点的梯度是在该点坡度（或者说斜度）最陡的方向。梯度的大小告诉我们坡度到底有多陡。这个现象可以如下数学的表示。山的高度函数 H的梯度点积一个单位向量给出了表面在该向量的方向上的斜率。这称为方向导数。 理解梯度 为了大家更容易理解什么是梯度，我们介意向量的概念，向量是一个矢量具有大小和方向的。同样，梯度也可以类比为具备大小和方向的这么一个概念。其两者比较如下：（这里严格意义上讲是不成立的，便于大家理解。） 向量 = 值 + 方向 梯度 = 向量 梯度 = 梯度值 + 梯度方向 梯度上升 要找到某函数的最大值，最好的方法是沿着该函数的梯度方向探寻。如果梯度记为 ▽ ，则函数 f(x, y) 的梯度由下式表示: \nabla f(x,y) = \begin{pmatrix} {\frac{\partial f(x,y)}{\partial x}}, {\frac{\partial f(x,y)}{\partial y}} \end{pmatrix}这个梯度意味着要沿 x 的方向移动\( {\frac{\partial f(x,y)}{\partial x}} \)，沿 y 的方向移动\({\frac{\partial f(x,y)}{\partial x}}\)。其中，函数f(x, y) 必须要在待计算的点上有定义并且可微。下图是一个具体的例子。 上图展示的，梯度上升算法到达每个点后都会重新估计移动的方向。从 P0 开始，计算完该点的梯度，函数就根据梯度移动到下一点 P1。在 P1 点，梯度再次被重新计算，并沿着新的梯度方向移动到 P2 。如此循环迭代，直到满足停止条件。迭代过程中，梯度算子总是保证我们能选取到最佳的移动方向。 上图中的梯度上升算法沿梯度方向移动了一步。可以看到，梯度算子总是指向函数值增长最快的方向。这里所说的是移动方向，而未提到移动量的大小。该量值称为步长，记作 α 。用向量来表示的话，梯度上升算法的迭代公式如下: 例如：y = w0 + w1x1 + w2x2 + ... + wnxn 梯度：参考上图的例子，二维图像，x方向代表第一个系数，也就是 w1，y方向代表第二个系数也就是 w2，这样的向量就是梯度。 α：上面的梯度算法的迭代公式中的阿尔法，这个代表的是移动步长（step length）。移动步长会影响最终结果的拟合程度，最好的方法就是随着迭代次数更改移动步长。步长通俗的理解，100米，如果我一步走10米，我需要走10步；如果一步走20米，我只需要走5步。这里的一步走多少米就是步长的意思。 ▽f(w)：代表沿着梯度变化的方向，也可以理解该方向求导。 该公式将一直被迭代执行，直至达到某个停止条件为止，比如迭代次数达到某个指定值或者算法达到某个可以允许的误差范围。 梯度上升与梯度下降的区别 梯度下降是大家听的最多的，本质上梯度下降与梯度上升算法是一样的，只是公司中加法变减法，梯度下降的公式如下： 在求极值的问题中，有梯度上升和梯度下降两个最优化方法。梯度上升用于求最大值，梯度下降用于求最小值。如logistic回归的目标函数：代表的是概率，我们想求概率最大值，即对数似然函数的最大值，所以使用梯度上升算法。而线性回归的代价函数：代表的则是误差，我们想求误差最小值，所以用梯度下降算法。 逻辑回归分类核心思想根据现有数据对分类边界建立回归公司，以此进行分类。回归即最佳拟合。 逻辑回归工作原理每个回归系数初始化为 1 重复 R 次: 计算整个数据集的梯度 使用 步长 x 梯度 更新回归系数的向量 返回回归系数 逻辑回归算法流程收集数据: 采用任意方法收集数据 准备数据: 由于需要进行距离计算，因此要求数据类型为数值型。另外，结构化数据格式则最佳。 分析数据: 采用任意方法对数据进行分析。 训练算法: 大部分时间将用于训练，训练的目的是为了找到最佳的分类回归系数。 测试算法: 一旦训练步骤完成，分类将会很快。 使用算法: 首先，我们需要输入一些数据，并将其转换成对应的结构化数值；接着，基于训练好的回归系数就可以对这些数值进行简单的回归计算，判定它们属于哪个类别； 逻辑回归优缺点优点: 计算代价不高，易于理解和实现。 缺点: 容易欠拟合，分类精度可能不高。 适用数据类型: 数值型和标称型数据。 案例分析1：Logistic回归在简单数据集上的分类案例描述在一个简单的数据集上，采用梯度上升法找到 Logistic 回归分类器在此数据集上的最佳回归系数 开发流程 收集数据: 可以使用任何方法 准备数据: 由于需要进行距离计算，因此要求数据类型为数值型。另外，结构化数据格式则最佳 分析数据: 画出决策边界 训练算法: 使用梯度上升找到最佳参数 测试算法: 使用 Logistic 回归进行分类 使用算法: 对简单数据集中数据进行分类 数据采集本文采用100行的测试集文本。其中前两列是特征1，和特征2，第三类是对应的标签。（这里特征1，特征2作为测试使用没有实际意义，你可以理解为特征1 是水里游的，特征2是有鱼鳞。类别判断是否为鱼类。） 读取文本文件，加载数据集和类标签，这里将特征集第一列加1，便于后续回归系数的计算： '''加载数据集和类标签''' def loadDataSet(file_name): # dataMat为原始数据， labelMat为原始数据的标签 dataMat,labelMat = [],[] fr = open(file_name) for line in fr.readlines(): lineArr = line.strip().split(',') if len(lineArr) == 1: continue # 这里如果就一个空的元素，则跳过本次循环 # 为了方便计算，我们将每一行的开头添加一个 1.0 作为 X0 dataMat.append([1.0, float(lineArr[0]), float(lineArr[1])]) labelMat.append(int(lineArr[2])) return dataMat, labelMat ## 梯度上升训练算法模型 > 梯度上升算法 使用梯度上升训练算法模型，其代码实现如下： ''' 正常的梯度上升法，得到的最佳回归系数 ''' def gradAscent(dataMatIn, classLabels): dataMatrix = mat(dataMatIn) # 转换为 NumPy 矩阵 # 转化为矩阵[[0,1,0,1,0,1.....]]，并转制[[0],[1],[0].....] # transpose() 行列转置函数 # 将行向量转化为列向量 => 矩阵的转置 labelMat = mat(classLabels).transpose() # 首先将数组转换为 NumPy 矩阵，然后再将行向量转置为列向量 # m->数据量，样本数 n->特征数 m, n = shape(dataMatrix) # 矩阵的行数和列数 # print(m,n) alpha = 0.001 # alpha代表向目标移动的步长 maxCycles = 500 # 迭代次数 weights = ones((n, 1)) # 代表回归系数,ones((n,1)) 长度和特征数相同矩阵全是1 for k in range(maxCycles): h = sigmoid(dataMatrix * weights) # 矩阵乘法 # labelMat是实际值 error = (labelMat - h) # 向量相减 # 0.001* (3*m)*(m*1) 表示在每一个列上的一个误差情况，最后得出 x1,x2,xn的系数的偏移量 weights = weights + alpha * dataMatrix.transpose() * error # 矩阵乘法，最后得到回归系数 return array(weights) 其中sigmoid函数实现如下： ''' sigmoid跳跃函数 ''' def sigmoid(ZVar): return 1.0 / (1 + exp(-ZVar)) 代码分析：函数的两个参数是数据加载返回的特征集和标签类集合。对数据集进行mat矩阵话转化，而类标签集进行矩阵之后转置，便于行列式的计算。然后设定步长，和迭代次数。整个特征矩阵与回归系数乘积求sigmoid值，最后返回回归系数的值。运行结果如下： [[ 4.12414349] [ 0.48007329] [-0.6168482 ]] **思考？步长和迭代次数的初始值如何设定？** > 随机梯度上升算法 梯度上升算法在每次更新回归系数时都需要遍历整个数据集，该方法在处理 100 个左右的数据集时尚可，但如果有数十亿样本和成千上万的特征，那么该方法的计算复杂度就太高了。一种改进方法是一次仅用一个样本点来更新回归系数，该方法称为 随机梯度上升算法。由于可以在新样本到来时对分类器进行增量式更新，因而随机梯度上升算法是一个在线学习(online learning)算法。与 “在线学习” 相对应，一次处理所有数据被称作是 “批处理” （batch） 。其伪代码是： 所有回归系数初始化为 1 对数据集中每个样本 计算该样本的梯度 使用 alpha x gradient 更新回归系数值 返回回归系数值 随机梯度上升算法的代码实现如下： ''' 随机梯度上升''' # 梯度上升与随机梯度上升的区别？梯度下降在每次更新数据集时都需要遍历整个数据集，计算复杂都较高；随机梯度下降一次只用一个样本点来更新回归系数 def stocGradAscent0(dataMatrix, classLabels): m, n = shape(dataMatrix) alpha = 0.01 weights = ones(n) # 初始化长度为n的数组，元素全部为 1 for i in range(m): # sum(dataMatrix[i]*weights)为了求 f(x)的值，f(x)=a1*x1+b2*x2+..+nn*xn h = sigmoid(sum(dataMatrix[i] * weights)) # 计算真实类别与预测类别之间的差值，然后按照该差值调整回归系数 error = classLabels[i] - h # 0.01*(1*1)*(1*n) weights = array(weights) + alpha * error * array(mat(dataMatrix[i])) return array(weights.transpose()) 可以看到，随机梯度上升算法与梯度上升算法在代码上很相似，但也有一些区别: 第一，后者的变量 h 和误差 error 都是向量，而前者则全是数值；第二，前者没有矩阵的转换过程，所有变量的数据类型都是 NumPy 数组。 判断优化算法优劣的可靠方法是看它是否收敛，也就是说参数是否达到了稳定值，是否还会不断地变化？下图展示了随机梯度上升算法在 200 次迭代过程中回归系数的变化情况。其中的系数2，也就是 X2 只经过了 50 次迭代就达到了稳定值，但系数 1 和 0 则需要更多次的迭代。如下图所示: ![](https://i.imgur.com/0kXiZWs.png) 针对波动问题，我们改进了之前的随机梯度上升算法，具体代码实现如下: ''' 改进版的随机梯度上升，使用随机的一个样本来更新回归系数''' def stocGradAscent1(dataMatrix, classLabels, numIter=150): m, n = shape(dataMatrix) weights = ones(n) # 创建与列数相同的矩阵的系数矩阵 # 随机梯度, 循环150,观察是否收敛 for j in range(numIter): dataIndex = list(range(m)) # [0, 1, 2 .. m-1] for i in range(m): # i和j的不断增大，导致alpha的值不断减少，但是不为0 alpha = 4 / (1.0 + j + i) + 0.0001 # alpha随着迭代不断减小非0 # random.uniform(x, y) 随机生成下一个实数，它在[x,y]范围内 Index = int(random.uniform(0, len(dataIndex))) # sum(dataMatrix[i]*weights)为了求 f(x)的值， f(x)=a1*x1+b2*x2+..+nn*xn h = sigmoid(sum(dataMatrix[dataIndex[Index]] * weights)) error = classLabels[dataIndex[Index]] - h weights = weights + alpha * error *array(mat(dataMatrix[dataIndex[Index]])) del (dataIndex[Index]) # print(weights.transpose()) return weights.transpose() 上面的改进版随机梯度上升算法改了两处代码。 - 改进为 alpha 的值。alpha 在每次迭代的时候都会调整，这回缓解上面波动图的数据波动或者高频波动。另外，虽然 alpha 会随着迭代次数不断减少，但永远不会减小到 0，因为我们在计算公式中添加了一个常数项。 - 修改为 randIndex 更新，这里通过随机选取样本拉来更新回归系数。这种方法将减少周期性的波动。这种方法每次随机从列表中选出一个值，然后从列表中删掉该值（再进行下一次迭代）。 ## 分析数据：画出决策边界 边界可视化的代码实现如下： ''' 数据可视化展示 ''' def plotBestFit(dataArr, labelMat, weights): n = shape(dataArr)[0] xcord1,xcord2,ycord1,ycord2 = [],[],[],[] for i in range(n): if int(labelMat[i]) == 1: xcord1.append(dataArr[i, 1]) ycord1.append(dataArr[i, 2]) else: xcord2.append(dataArr[i, 1]) ycord2.append(dataArr[i, 2]) fig = plt.figure() ax = fig.add_subplot(111) ax.scatter(xcord1, ycord1, s=30, c='red', marker='s') ax.scatter(xcord2, ycord2, s=30, c='green') x = arange(-3.0, 3.0, 0.1) """ dataMat.append([1.0, float(lineArr[0]), float(lineArr[1])]) w0*x0+w1*x1+w2*x2=f(x) x0最开始就设置为1， x2就是我们画图的y值，而f(x)被我们磨合误差给算到w0,w1,w2身上去了 所以： w0+w1*x+w2*y=0 => y = (-w0-w1*x)/w2 """ y = (-weights[0] - weights[1] * x) / weights[2] ax.plot(x, y) plt.xlabel('X') plt.ylabel('Y') plt.show() 运行结果分别是： 梯度上升算法可视化结果图1-1： ![](https://i.imgur.com/PFWw6FI.png) 随机梯度上升算法可视化结果： ![](https://i.imgur.com/vkDN1aJ.png) 优化随机梯度上升算法可视化结果： ![](https://i.imgur.com/I7lHYOk.png) 结果分析： 图1-1的梯度上升算法在每次更新回归系数时都需要遍历整个数据集，虽然分类结果还不错该方法的计算复杂度就太高了。图1-2的随机梯度上升算法虽然分类效果不是很好（分类1/3左右），但是其迭代次数远远小于图1-1迭代次数（500次）。整体性能有所改进，但是其存在局部波动现象。基于此改进后的图1-3效果显示好很多。 ## 测试算法: 使用Logistic回归进行分类 代码实现如下： '''数据集决策可视化''' def simpleTest(file_name): # 1.收集并准备数据 dataMat, labelMat = loadDataSet(file_name) # 2.训练模型， f(x)=a1*x1+b2*x2+..+nn*xn中 (a1,b2, .., nn).T的矩阵值 dataArr = array(dataMat) weights = stocGradAscent1(dataArr, labelMat) # 数据可视化 plotBestFit(dataArr, labelMat, weights) # 案例分析2：从病毒性流感预测病人的死亡情况 ## 案例描述 使用 Logistic 回归来预测病毒性流感预测病人的死亡问题。这个数据集中包含了医院检测病毒性流感的一些指标，有的指标比较主观，有的指标难以测量，例如人的疼痛级别。 **开发流程** 收集数据: 给定数据文件 准备数据: 用 Python 解析文本文件并填充缺失值 分析数据: 可视化并观察数据 训练算法: 使用优化算法，找到最佳的系数 测试算法: 为了量化回归的效果，需要观察错误率。根据错误率决定是否回退到训练阶段， 通过改变迭代的次数和步长的参数来得到更好的回归系数 使用算法: 实现一个简单的命令行程序来收集马的症状并输出预测结果并非难事， 这可以作为留给大家的一道习题 ## 收集数据: 给定数据文件 训练数据已经给出，这里对文件处理即可，代码如下： '''加载数据集和类标签2''' def loadDataSet2(file_name): frTrain = open(file_name) trainingSet,trainingLabels = [],[] for line in frTrain.readlines(): currLine = line.strip().split(',') # print(len(currLine)) lineArr = [] for i in range(len(currLine)-1): lineArr.append(float(currLine[i])) trainingSet.append(lineArr) trainingLabels.append(float(currLine[len(currLine)-1])) return trainingSet,trainingLabels 准备数据: 用 Python 解析文本文件并填充缺失值 处理数据中的缺失值 假设有100个样本和20个特征，这些数据都是机器收集回来的。若机器上的某个传感器损坏导致一个特征无效时该怎么办？此时是否要扔掉整个数据？这种情况下，另外19个特征怎么办？ 它们是否还可以用？答案是肯定的。因为有时候数据相当昂贵，扔掉和重新获取都是不可取的，所以必须采用一些方法来解决这个问题。下面给出了一些可选的做法： 使用可用特征的均值来填补缺失值； 使用特殊值来填补缺失值，如 -1； 忽略有缺失值的样本； 使用有相似样本的均值添补缺失值； 使用另外的机器学习算法预测缺失值。 现在，我们对下一节要用的数据集进行预处理，使其可以顺利地使用分类算法。在预处理需要做两件事:所有的缺失值必须用一个实数值来替换，因为我们使用的 NumPy 数据类型不允许包含缺失值。我们这里选择实数 0 来替换所有缺失值，恰好能适用于 Logistic 回归。这样做的直觉在于，我们需要的是一个在更新时不会影响系数的值。回归系数的更新公式如下:weights = weights + alpha * error * dataMatrix[dataIndex[randIndex]]如果 dataMatrix 的某个特征对应值为 0，那么该特征的系数将不做更新，即:weights = weights另外，由于 Sigmoid(0) = 0.5 ，即它对结果的预测不具有任何倾向性，因此我们上述做法也不会对误差造成任何影响。基于上述原因，将缺失值用 0 代替既可以保留现有数据，也不需要对优化算法进行修改。此外，该数据集中的特征取值一般不为 0，因此在某种意义上说它也满足 “特殊值” 这个要求。如果在测试数据集中发现了一条数据的类别标签已经缺失，那么我们的简单做法是将该条数据丢弃。这是因为类别标签与特征不同，很难确定采用某个合适的值来替换。采用 Logistic 回归进行分类时这种做法是合理的，而如果采用类似 kNN 的方法，则保留该条数据显得更加合理。 训练算法: 使用优化算法，找到最佳的系数训练算法模型代码如下： '''测试Logistic算法分类''' def testClassier(): # 使用改进后的随机梯度上升算法 求得在此数据集上的最佳回归系数 trainWeights file_name = './HorseColicTraining.txt' trainingSet,trainingLabels = loadDataSet2(file_name) trainWeights = stocGradAscent1(array(trainingSet), trainingLabels, 500) # 根据特征向量预测结果 teststr = '2.000000,1.000000,38.300000,40.000000,24.000000,1.000000,1.000000,3.000000,1.000000,3.000000,3.000000,1.000000,0.000000,0.000000,0.000000,1.000000,1.000000,33.000000,6.700000,0.000000,0.000000' currLine = teststr.strip().split(',') lineArr = [] for i in range(len(currLine)): lineArr.append(float(currLine[i])) res = classifyVector(array(lineArr), trainWeights) # 打印预测结果 reslut = ['死亡','存活'] print('预测结果是：',int(res)) 分类函数代码如下： '''分类函数，根据回归系数和特征向量来计算 Sigmoid的值,大于0.5函数返回1，否则返回0''' def classifyVector(featuresV, weights): prob = sigmoid(sum(featuresV * weights)) print(prob) if prob > 0.9: return 1.0 else: return 0.0 测试算法：使用决策树执行分类为了量化回归的效果，需要观察错误率。根据错误率决定是否回退到训练阶段，通过改变迭代的次数和步长的参数来得到更好的回归系数 ‘’’打开测试集和训练集,并对数据进行格式化处理’’’def colicTest(): file_name = ‘./HorseColicTraining.txt’ trainingSet,trainingLabels = loadDataSet2(file_name) # 使用改进后的随机梯度上升算法 求得在此数据集上的最佳回归系数 trainWeights trainWeights = stocGradAscent1(array(trainingSet), trainingLabels, 500) frTest = open(&#39;./HorseColicTest.txt&#39;) errorCount = 0 ; numTestVec = 0.0 # 读取 测试数据集 进行测试，计算分类错误的样本条数和最终的错误率 for line in frTest.readlines(): numTestVec += 1.0 currLine = line.strip().split(&#39;,&#39;) lineArr = [] for i in range(21): lineArr.append(float(currLine[i])) if int(classifyVector(array(lineArr), trainWeights)) != int( currLine[21]): errorCount += 1 errorRate = (float(errorCount) / numTestVec) print(&quot;逻辑回归算法测试集的错误率为: %f&quot; % errorRate) return errorRate 调用 colicTest() 10次并求结果的平均值def multiTest(): numTests = 10;errorSum = 0.0 for k in range(numTests): errorSum += colicTest() print(“迭代 %d 次后的平均错误率是: %f” % (numTests, errorSum / float(numTests)))&lt;/pre&gt;其运行结果如下： 逻辑回归算法测试集的错误率为: 0.298507 参考文献 scikit中文社区：http://sklearn.apachecn.org/cn/0.19.0/ 中文维基百科：https://zh.wikipedia.org/wiki/%E9%82%8F%E8%BC%AF%E8%BF%B4%E6%AD%B8 GitHub：https://github.com/BaiNingchao/MachineLearning-1 图书：《机器学习实战》 图书：《自然语言处理理论与实战》 完整代码下载 源码请进【机器学习和自然语言QQ群：436303759】文件下载： 作者声明 本文版权归作者所有，旨在技术交流使用。未经作者同意禁止转载，转载后需在文章页面明显位置给出原文连接，否则相关责任自行承担。]]></content>
      <categories>
        <category>机器学习</category>
        <category>逻辑回归</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Logistic regression</tag>
        <tag>文本分类</tag>
        <tag>机器学习算法</tag>
        <tag>逻辑回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一步步教你轻松学朴素贝叶斯模型算法Sklearn深度篇3]]></title>
    <url>%2F2018%2F09%2F19%2F%E4%B8%80%E6%AD%A5%E6%AD%A5%E6%95%99%E4%BD%A0%E8%BD%BB%E6%9D%BE%E5%AD%A6%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%A8%A1%E5%9E%8B%E7%AE%97%E6%B3%95Sklearn%E6%B7%B1%E5%BA%A6%E7%AF%873%2F</url>
    <content type="text"><![CDATA[摘要：朴素贝叶斯模型是机器学习常用的模型算法之一，其在文本分类方面简单易行，且取得不错的分类效果。所以很受欢迎，对于朴素贝叶斯的学习，本文首先介绍理论知识即朴素贝叶斯相关概念和公式推导，为了加深理解，采用一个维基百科上面性别分类例子进行形式化描述。然后通过编程实现朴素贝叶斯分类算法，并在屏蔽社区言论、垃圾邮件、个人广告中获取区域倾向等几个方面进行应用，包括创建数据集、数据预处理、词集模型和词袋模型、朴素贝叶斯模型训练和优化等。然后结合复旦大学新闻语料进行朴素贝叶斯的应用。最后，大家熟悉其原理和实现之后，采用机器学习sklearn包进行实现和优化。由于篇幅较长，采用理论理解、案例实现、sklearn优化三个部分进行学习。（本文原创，转载必须注明出处.） 复旦新闻语料：朴素贝叶斯中文文本分类项目概述本节介绍朴素贝叶斯分类算法模型在中文领域中的应用。我们对新闻语料进行多文本分类操作，本文选择艺术、文学、教育、哲学、历史五个类别的训练文本，然后采用新的测试语料进行分类预测。 收集数据数据集是从复旦新闻语料库中抽取出来的，考虑学习使用，样本选择并不大。主要抽选艺术、文学、教育、哲学、历史五个类别各10篇文章。全部数据文档50篇。具体情况不同对收集数据要求不同，你也可以选择网络爬取，数据库导出等。这文档读取时候可能会遇到gbk，utf-8等格式共存的情况，这里建议采用BatUTF8Conv.exe（点击下载）工具，进行utf-8格式批量转化。 准备数据创建数据集代码如下： '''创建数据集和类标签''' def loadDataSet(): docList = [];classList = [] # 文档列表、类别列表 dirlist = ['C3-Art','C4-Literature','C5-Education','C6-Philosophy','C7-History'] for j in range(5): for i in range(1, 11): # 总共10个文档 # 切分，解析数据，并归类为 1 类别 wordList = textParse(open('./fudan/%s/%d.txt' % (dirlist[j],i),encoding='UTF-8').read()) docList.append(wordList) classList.append(j) # print(i,'\t','./fudan/%s/%d.txt' % (dirlist[j],i),'\t',j) return docList,classList ''' 利用jieba对文本进行分词，返回切词后的list ''' def textParse(str_doc): # 正则过滤掉特殊符号、标点、英文、数字等。 import re r1 = '[a-zA-Z0-9’!"#$%&\'()*+,-./:;?@，。?★、…【】《》？“”‘’！[\\]^_`{|}~]+' str_doc=re.sub(r1, '', str_doc) # 创建停用词列表 stwlist = set([line.strip() for line in open('./stopwords.txt', 'r', encoding='utf-8').readlines()]) sent_list = str_doc.split('\n') # word_2dlist = [rm_tokens(jieba.cut(part), stwlist) for part in sent_list] # 分词并去停用词 word_2dlist = [rm_tokens([word+"/"+flag+" " for word, flag in pseg.cut(part) if flag in ['n','v','a','ns','nr','nt']], stwlist) for part in sent_list] # 带词性分词并去停用词 word_list = list(itertools.chain(*word_2dlist)) # 合并列表 return word_list ''' 去掉一些停用词、数字、特殊符号 ''' def rm_tokens(words, stwlist): words_list = list(words) for i in range(words_list.__len__())[::-1]: word = words_list[i] if word in stwlist: # 去除停用词 words_list.pop(i) elif len(word) == 1: # 去除单个字符 words_list.pop(i) elif word == " ": # 去除空字符 words_list.pop(i) return words_list 代码分析：loadDataSet()方法是遍历读取文件夹，并对每篇文档进行处理，最后返回全部文档集的列表和类标签。textParse()方法是对每篇文档字符串进行数据预处理，我们首选使用正则方法保留文本数据，然后进行带有词性的中文分词和词性选择，rm_tokens()是去掉一些停用词、数字、特殊符号。最终返回相对干净的数据集和标签集。 ### 分析数据 前面两篇文章都介绍了，我们需要把文档进行向量化表示，首先构建全部文章的单词集合，实现代码如下： '''获取所有文档单词的集合''' def createVocabList(dataSet): vocabSet = set([]) for document in dataSet: vocabSet = vocabSet | set(document) # 操作符 | 用于求两个集合的并集 # print(len(vocabSet),len(set(vocabSet))) return list(vocabSet) 基于文档模型的基础上，我们将特征向量转化为数据矩阵向量，这里使用的词袋模型，构造与实现方法如下： '''文档词袋模型，创建矩阵数据''' def bagOfWords2VecMN(vocabList, inputSet): returnVec = [0] * len(vocabList) for word in inputSet: if word in vocabList: returnVec[vocabList.index(word)] += 1 return returnVec 对矩阵数据可以采用可视化分析方法或者结合NLTK进行数据分析，检查数据分布情况和特征向量构成情况及其特征选择作为参考。 ### 训练算法 我们在前面两篇文章介绍了朴素贝叶斯模型训练方法，我们在该方法下稍微改动就得到如下实现： '''朴素贝叶斯模型训练数据优化''' def trainNB0(trainMatrix, trainCategory): numTrainDocs = len(trainMatrix) # 总文件数 numWords = len(trainMatrix[0]) # 总单词数 p1Num=p2Num=p3Num=p4Num=p5Num = ones(numWords) # 各类为1的矩阵 p1Denom=p2Denom=p3Denom=p4Denom=p5Denom = 2.0 # 各类特征和 num1=num2=num3=num4=num5 = 0 # 各类文档数目 pNumlist=[p1Num,p2Num,p3Num,p4Num,p5Num] pDenomlist =[p1Denom,p2Denom,p3Denom,p4Denom,p5Denom] Numlist = [num1,num2,num3,num4,num5] for i in range(numTrainDocs): # 遍历每篇训练文档 for j in range(5): # 遍历每个类别 if trainCategory[i] == j: # 如果在类别下的文档 pNumlist[j] += trainMatrix[i] # 增加词条计数值 pDenomlist[j] += sum(trainMatrix[i]) # 增加该类下所有词条计数值 Numlist[j] +=1 # 该类文档数目加1 pVect,pi = [],[] for index in range(5): pVect.append(log(pNumlist[index] / pDenomlist[index])) pi.append(Numlist[index] / float(numTrainDocs)) return pVect, pi 构建分类函数，其优化后的代码实现如下： '''朴素贝叶斯分类函数,将乘法转换为加法''' def classifyNB(vec2Classify, pVect,pi): # 计算公式 log(P(F1|C))+log(P(F2|C))+....+log(P(Fn|C))+log(P(C)) bnpi = [] # 文档分类到各类的概率值列表 for x in range(5): bnpi.append(sum(vec2Classify * pVect[x]) + log(pi[x])) # print([bnp for bnp in bnpi]) # 分类集合 reslist = ['Art','Literature','Education','Philosophy','History'] # 根据最大概率，选择索引值 index = [bnpi.index(res) for res in bnpi if res==max(bnpi)] return reslist[index[0]] # 返回分类值 测试算法我们加载构建的数据集方法，然后创建单词集合，集合词袋模型进行特征向量化，构建训练模型和分类方法，最终我们从复旦新闻语料中选择一篇未加入训练集的教育类文档，进行开放测试，具体代码如下： '''朴素贝叶斯新闻分类应用''' def testingNB(): # 1. 加载数据集 dataSet,Classlabels = loadDataSet() # 2. 创建单词集合 myVocabList = createVocabList(dataSet) # 3. 计算单词是否出现并创建数据矩阵 trainMat = [] for postinDoc in dataSet: trainMat.append(bagOfWords2VecMN(myVocabList, postinDoc)) with open('./word-bag.txt','w') as f: for i in trainMat: f.write(str(i)+'\r\n') # 4. 训练数据 pVect,pi= trainNB0(array(trainMat), array(Classlabels)) # 5. 测试数据 testEntry = textParse(open('./fudan/test/C5-1.txt',encoding='UTF-8').read()) thisDoc = array(bagOfWords2VecMN(myVocabList, testEntry)) print(testEntry[:10], '分类结果是: ', classifyNB(thisDoc, pVect,pi)) 实现结果如下： Building prefix dict from the default dictionary ... Loading model from cache C:\Users\ADMINI~1\AppData\Local\Temp\jieba.cache Loading model cost 0.892 seconds. Prefix dict has been built succesfully. ['全国/n ', '举办/v ', '电影/n ', '新华社/nt ', '北京/ns ', '国家教委/nt ', '广播电影电视部/nt ', '文化部/n ', '联合/v ', '决定/v '] 分类结果是: Literature 耗时：29.4882 s 结果分析：我们运行分类器得出结果易知，预测结果是文化类，且运行时间为29s。首先分析为什么预测错误，这里面主要是训练集样本比较少和特征选择的原因。运行时间是由于将特征矩阵存储本地后，后面直接读取文本，相当于加载缓存，大大缩短运行时间。但是这里还有值得优化的地方，比如每次运行都会加载训练模型，大大消耗时间，我们能不能训练模型加载一次，多次调用呢？当然是可以的，这个问题下文继续优化。我们重点关注下特征选择问题 ### 特征选择问题讨论 - 做文本分类的时候，遇到特征矩阵1.5w。在测试篇幅小的文章总是分类错误？这个时候如何做特征选择？是不是说去掉特征集中频率极高和极低的一部分，对结果有所提升？ 答：你说的这个情况是很普遍的现象，篇幅小的文章，特征小，所以模型更容易判断出错！去掉高频和低频通常是可以使得训练的模型泛化能力变强 - 比如：艺术，文化，历史，教育。界限本来就不明显，比如测试数据“我爱艺术，艺术是我的全部”。结果会分类为文化。其实这个里面还有就是不同特征词的权重问题，采用tf-idf优化下应该会好一些？ 答：我个人觉得做文本特征提取，还是需要自己去分析文本本身内容的文字特点，你可以把每一类的文本的实体提取出来，然后统计一下每个词在每一类上的数量，看看数量分布，也许可以发现一些数据特点 - 我就是按照这个思路做的，还有改进时候的停用词，其实可以分析特征文本，针对不同业务，使用自定义的停用词要比通用的好 还有提前各类见最具表征性的词汇加权，凸显本类的权重是吧？ 答：比如，艺术类文章中，哪些词出现较多，哪些词出现少，再观察这些词的词性主要是哪些，这样可能会对你制定提取特征规则方式的时候提供一定的思路参考，我可以告诉你的是，有些词绝对会某一类文章出出现多，然后在其他类文章出现很少，这一类的词就是文章的特征词 - 那样的思路可以是：对某类文章单独构建类内的词汇表再进行选择。最后对类间词汇表叠加就ok了。 答：词汇表有个缺点就是，不能很好的适应新词 - 改进思路呢 答：我给你一个改进思路：你只提取每个文本中的名词、动词、形容词、地名，用这些词的作为文本的特征来训练试一试，用文本分类用主题模型（LDA）来向量化文本，再训练模型试一试。如果效果还是不够好，再将文本向量用PCA进行一次特征降维，然后再训练模型试一试，按常理来说，效果应该会有提高 - 还有我之前个人写的程序分类效果不理想，后来改用sklearn内置BN运行依旧不理想。适当改进了特征提取，还是不理想。估计每类10篇文章的训练数据太少了 答：文本本身特征提取就相对难一些，再加上训练数据少，训练出来的模型效果可想而已，正常的 ## sklearn：朴素贝叶斯分类调用 ### 数据准备和数据预处理 > 加载文档数据集和分类集 数据准备和数据预处理上文已经介绍了，本节增加了一个全局变量存储词汇表，目的是写入到本地文本里，本地读取词汇集，避免每次都做特征向量时加载训练集，提高运行时间。 myVocabList = [] # 设置词汇表的全局变量 '''创建数据集和类标签''' def loadDataSet(): docList = [];classList = [] # 文档列表、类别列表、文本特征 dirlist = ['C3-Art','C4-Literature','C5-Education','C6-Philosophy','C7-History'] for j in range(5): for i in range(1, 11): # 总共10个文档 # 切分，解析数据，并归类为 1 类别 wordList = textParse(open('./fudan/%s/%d.txt' % (dirlist[j],i),encoding='UTF-8').read()) docList.append(wordList) classList.append(j) # print(i,'\t','./fudan/%s/%d.txt' % (dirlist[j],i),'\t',j) # print(len(docList),len(classList),len(fullText)) global myVocabList myVocabList = createVocabList(docList) # 创建单词集合 return docList,classList,myVocabList ''' 利用jieba对文本进行分词，返回切词后的list ''' def textParse(str_doc): #与上文方法一致 ''' 去掉一些停用词、数字、特殊符号 ''' def rm_tokens(words, stwlist): #与上文方法一致 > 文档数据集和分类集在本地读写操作 # 本地存储数据集和标签 def storedata(): # 3. 计算单词是否出现并创建数据矩阵 # trainMat =[[0,1,2,3],[2,3,1,5],[0,1,4,2]] # 训练集 # classList = [0,1,2] #类标签 docList,classList,myVocabList = loadDataSet() # 计算单词是否出现并创建数据矩阵 trainMat = [] for postinDoc in docList: trainMat.append(bagOfWords2VecMN(myVocabList, postinDoc)) res = "" for i in range(len(trainMat)): res +=' '.join([str(x) for x in trainMat[i]])+' '+str(classList[i])+'\n' # print(res[:-1]) # 删除最后一个换行符 with open('./word-bag.txt','w') as fw: fw.write(res[:-1]) with open('./wordset.txt','w') as fw: fw.write(' '.join([str(v) for v in myVocabList])) # 读取本地数据集和标签 def grabdata(): f = open('./word-bag.txt') # 读取本地文件 arrayLines = f.readlines() # 行向量 tzsize = len(arrayLines[0].split(' '))-1 # 列向量，特征个数减1即数据集 returnMat = zeros((len(arrayLines),tzsize)) # 0矩阵数据集 classLabelVactor = [] # 标签集，特征最后一列 index = 0 for line in arrayLines: # 逐行读取 listFromLine = line.strip().split(' ') # 分析数据，空格处理 # print(listFromLine) returnMat[index,:] = listFromLine[0:tzsize] # 数据集 classLabelVactor.append(int(listFromLine[-1])) # 类别标签集 index +=1 # print(returnMat,classLabelVactor) myVocabList=writewordset() return returnMat,classLabelVactor,myVocabList def writewordset(): f1 = open('./wordset.txt') myVocabList =f1.readline().split(' ') for w in myVocabList: if w=='': myVocabList.remove(w) return myVocabList > 获取文档集合和构建词袋模型 '''获取所有文档单词的集合''' def createVocabList(dataSet): vocabSet = set([]) for document in dataSet: vocabSet = vocabSet | set(document) # 操作符 | 用于求两个集合的并集 # print(len(vocabSet),len(set(vocabSet))) return list(vocabSet) '''文档词袋模型，创建矩阵数据''' def bagOfWords2VecMN(vocabList, inputSet): returnVec = [0] * len(vocabList) for word in inputSet: if word in vocabList: returnVec[vocabList.index(word)] += 1 return returnVec ### 高斯朴素贝叶斯 GaussianNB 实现了运用于分类的高斯朴素贝叶斯算法。特征的可能性(即概率)假设为高斯分布: 参数\\(\sigma_y ,\mu_y\\)使用最大似然法估计。 高斯朴素贝叶斯实现方法代码： '''高斯朴素贝叶斯''' def MyGaussianNB(trainMat='',Classlabels='',testDoc=''): # -----sklearn GaussianNB------- # 训练数据 X = np.array(trainMat) Y = np.array(Classlabels) # 高斯分布 clf = GaussianNB() clf.fit(X, Y) # 测试预测结果 index = clf.predict(testDoc) # 返回索引 reslist = ['Art','Literature','Education','Philosophy','History'] print(reslist[index[0]]) ### 多项朴素贝叶斯 MultinomialNB 实现了服从多项分布数据的朴素贝叶斯算法，也是用于文本分类(这个领域中数据往往以词向量表示，尽管在实践中 tf-idf 向量在预测时表现良好)的两大经典朴素贝叶斯算法之一。 分布参数由每类 y 的 $$ \theta_y=(\theta_{y1},\ldots,\theta_{yn}) $$ 向量决定， 式中 n 是特征的数量(对于文本分类，是词汇量的大小)\\( \theta_{yi}\\)是样本中属于类 y 中特征 i 概率\\(P(x_i \mid y)\\)。参数\\( \theta_y\\)使用平滑过的最大似然估计法来估计，即相对频率计数: 式中$$ N_{yi}=\sum_{x \in T} x_i$$ 是训练集 T 中 特征 i 在类 y 中出现的次数，$$ N_{yi}=\sum_{x \in T} y_i$$是类 y 中出现所有特征的计数总和。先验平滑因子\\(alpha \ge 0\\)应用于在学习样本中没有出现的特征，以防在将来的计算中出现0概率输出。 把 \\(\alpha = 1\\)被称为拉普拉斯平滑(Lapalce smoothing)，而 \\(\alpha < 1\\)被称为利德斯通(Lidstone smoothing)。 多项朴素贝叶斯实现方法代码： '''多项朴素贝叶斯''' def MyMultinomialNB(trainMat='',Classlabels='',testDoc=''): # -----sklearn MultinomialNB------- # 训练数据 X = np.array(trainMat) Y = np.array(Classlabels) # 多项朴素贝叶斯 clf = MultinomialNB() clf.fit(X, Y) # 测试预测结果 index = clf.predict(testDoc) # 返回索引 reslist = ['Art','Literature','Education','Philosophy','History'] print(reslist[index[0]]) 伯努利朴素贝叶斯BernoulliNB 实现了用于多重伯努利分布数据的朴素贝叶斯训练和分类算法，即有多个特征，但每个特征 都假设是一个二元 (Bernoulli, boolean) 变量。 因此，这类算法要求样本以二元值特征向量表示；如果样本含有其他类型的数据， 一个 BernoulliNB 实例会将其二值化(取决于 binarize 参数)。伯努利朴素贝叶斯的决策规则基于 与多项分布朴素贝叶斯的规则不同 伯努利朴素贝叶斯明确地惩罚类 y 中没有出现作为预测因子的特征 i ，而多项分布分布朴素贝叶斯只是简单地忽略没出现的特征。在文本分类的例子中，词频向量(word occurrence vectors)(而非词数向量(word count vectors))可能用于训练和用于这个分类器。 BernoulliNB 可能在一些数据集上可能表现得更好，特别是那些更短的文档。 如果时间允许，建议对两个模型都进行评估。伯努利朴素贝叶斯代码实现如下： ‘’’伯努利朴素贝叶斯’’’def MyBernoulliNB(trainMat=’’,Classlabels=’’,testDoc=’’): # -----sklearn BernoulliNB------- # 训练数据 X = np.array(trainMat) Y = np.array(Classlabels) # 多项朴素贝叶斯 clf = BernoulliNB() clf.fit(X, Y) # 测试预测结果 index = clf.predict(testDoc) # 返回索引 reslist = [&#39;Art&#39;,&#39;Literature&#39;,&#39;Education&#39;,&#39;Philosophy&#39;,&#39;History&#39;] print(reslist[index[0]]) &lt;/pre&gt; 各种贝叶斯模型分类测试代码实现如下： def testingNB(): # 加载数据集和单词集合 trainMat,Classlabels,myVocabList = grabdata() # 读取训练结果 # 测试数据 testEntry = textParse(open(&#39;./fudan/test/C6-2.txt&#39;,encoding=&#39;UTF-8&#39;).read()) testDoc = np.array(bagOfWords2VecMN(myVocabList, testEntry)) # 测试数据 # 测试预测结果 MyGaussianNB(trainMat,Classlabels,testDoc) MyMultinomialNB(trainMat,Classlabels,testDoc) MyBernoulliNB(trainMat,Classlabels,testDoc) &lt;/pre&gt;运行结果： Building prefix dict from the default dictionary ... Loading model from cache C:\Users\ADMINI~1\AppData\Local\Temp\jieba.cache Loading model cost 1.014 seconds. Prefix dict has been built succesfully. 高斯朴素贝叶斯：Education 多项朴素贝叶斯分类结果：Art 伯努利朴素贝叶斯分类结果：Literature 耗时：2.3996 s 参考文献 scikit中文社区：http://sklearn.apachecn.org/cn/0.19.0/ 中文维基百科：https://zh.wikipedia.org/wiki/ 文本分类特征选择：https://www.cnblogs.com/june0507/p/7601001.html GitHub：https://github.com/BaiNingchao/MachineLearning-1 图书：《机器学习实战》 图书：《自然语言处理理论与实战》 完整代码下载 源码请进【机器学习和自然语言QQ群：436303759】文件下载： 作者声明 本文版权归作者所有，旨在技术交流使用。未经作者同意禁止转载，转载后需在文章页面明显位置给出原文连接，否则相关责任自行承担。]]></content>
      <categories>
        <category>机器学习</category>
        <category>朴素贝叶斯</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Naive Bayes</tag>
        <tag>ML</tag>
        <tag>文本分类</tag>
        <tag>sklearn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一步步教你轻松学朴素贝叶斯模型算法实现篇2]]></title>
    <url>%2F2018%2F09%2F19%2F%E4%B8%80%E6%AD%A5%E6%AD%A5%E6%95%99%E4%BD%A0%E8%BD%BB%E6%9D%BE%E5%AD%A6%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%A8%A1%E5%9E%8B%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0%E7%AF%872%2F</url>
    <content type="text"><![CDATA[摘要：朴素贝叶斯模型是机器学习常用的模型算法之一，其在文本分类方面简单易行，且取得不错的分类效果。所以很受欢迎，对于朴素贝叶斯的学习，本文首先介绍理论知识即朴素贝叶斯相关概念和公式推导，为了加深理解，采用一个维基百科上面性别分类例子进行形式化描述。然后通过编程实现朴素贝叶斯分类算法，并在屏蔽社区言论、垃圾邮件、个人广告中获取区域倾向等几个方面进行应用，包括创建数据集、数据预处理、词集模型和词袋模型、朴素贝叶斯模型训练和优化等。然后结合复旦大学新闻语料进行朴素贝叶斯的应用。最后，大家熟悉其原理和实现之后，采用机器学习sklearn包进行实现和优化。由于篇幅较长，采用理论理解、案例实现、sklearn优化三个部分进行学习。（本文原创，转载必须注明出处.） 案例场景1: 屏蔽社区留言板的侮辱性言论项目概述构建一个快速过滤器来屏蔽在线社区留言板上的侮辱性言论。如果某条留言使用了负面或者侮辱性的语言，那么就将该留言标识为内容不当。对此问题建立两个类别: 侮辱类和非侮辱类，使用 1 和 0 分别表示。 本案例开发流程如下： 收集数据: 可以是文本数据、数据库数据、网络爬取的数据、自定义数据等等 数据预处理: 对采集数据进行格式化处理，文本数据的格式一致化，网络数据的分析抽取等，包括中文分词、停用词处理、词袋模型、构建词向量等。 分析数据: 检查词条确保解析的正确性，根据特征进行模型选择、特征抽取等。 训练算法: 从词向量计算概率 测试算法: 根据现实情况修改分类器 使用算法: 对社区留言板言论进行分类 收集数据本案例我们采用自定义的数据集，我们选择6条社区评论，然后进行数据处理后以list形式存储在文档列表postingList中。其中每个词代表一个特征。将每条评论进行分类（即1代表侮辱性文字,0代表非侮辱文字）存在在类别列表classVec中。最后返回数据集和类标签。代码实现如下： '''创建数据集：单词列表postingList, 所属类别classVec''' def loadDataSet(): postingList = [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'], ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'], ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'], ['stop', 'posting', 'stupid', 'worthless', 'garbage'], ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'], ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']] classVec = [0, 1, 0, 1, 0, 1] # 1代表侮辱性文字,0代表非侮辱文字 return postingList, classVec 代码分析：postingList列表存储6条评论信息，classVec列表存储每条信息类别（1代表侮辱性文字,0代表非侮辱文字）。最后返回文档列表和类别列表。 数据预处理数据预处理包括对样本进行分词、词性筛选、停用词处理等，最后形成规范化干净的数据样本。由于本案例收集数据时默认进行了数据预处理，所以本节不在介绍（复旦新闻语料文本分类案例会详细介绍）。目前，我们采集的数据还是文本类型，计算机还不能直接处理，需要将文本数据转化成词向量进行处理。这里面需要获取特征的词汇集合（如果暂时不理解，先看看代码实现，下面会进行形式化描述）。其实现过程如下： ‘’’获取所有单词的集合:返回不含重复元素的单词列表’’’def createVocabList(dataSet): vocabSet = set([]) for document in dataSet: vocabSet = vocabSet | set(document) # 操作符 | 用于求两个集合的并集 # print(vocabSet) return list(vocabSet) &lt;/pre&gt;代码分析：方法参数dataSet即加载数据集返回的文档列表。vocabSet是定义的不重复的数据集合。然后for循环对文档列表每条数据进行遍历处理，将不重复的词汇添加到vocabSet中，最终形成整个文档的词汇集，然后以list形式返回。 上面的方法已经获取了整个文档词汇集合，接着构建数据矩阵，代码实现如下： ‘’’词集模型构建数据矩阵’’’def setOfWords2Vec(vocabList, inputSet): # 创建一个和词汇表等长的向量，并将其元素都设置为0 returnVec = [0] * len(vocabList) # 遍历文档中的所有单词，如果出现了词汇表中的单词，则将输出的文档向量中的对应值设为1 for word in inputSet: if word in vocabList: returnVec[vocabList.index(word)] = 1 else: print(&quot;单词: %s 不在词汇表之中!&quot; % word) # print(returnVec) return returnVec &lt;/pre&gt;代码分析：本方法提供两个参数分别是整个训练文档词汇集（即全部训练文档6条评论不重复的单词集合），输入的数据列表。以整个词汇集等长的0向量。我们遍历输入数据列表，如果词特征在词汇集则标记1，不在词汇集保持为0.最后返回词向量矩阵。 与词集模型对应的，有个词袋模型。两者都是构建词向量，只是方式不一样，词袋模型也是推荐使用的词向量化方法，其实现如下： ‘’’文档词袋模型构建数据矩阵’’’def bagOfWords2VecMN(vocabList, inputSet): returnVec = [0] * len(vocabList) for word in inputSet: if word in vocabList: returnVec[vocabList.index(word)] += 1 # print(returnVec) return returnVec &lt;/pre&gt; 分析数据运行词集模型setOfWords2Vec(vocabList, dataSet[0])运行结果如下： [&#39;dog&#39;, &#39;to&#39;, &#39;take&#39;, &#39;park&#39;, &#39;licks&#39;, &#39;has&#39;, &#39;help&#39;, &#39;stupid&#39;, &#39;him&#39;, &#39;so&#39;, &#39;not&#39;, &#39;love&#39;, &#39;buying&#39;, &#39;problems&#39;, &#39;cute&#39;, &#39;stop&#39;, &#39;steak&#39;, &#39;how&#39;, &#39;flea&#39;, &#39;maybe&#39;, &#39;food&#39;, &#39;I&#39;, &#39;please&#39;, &#39;dalmation&#39;, &#39;mr&#39;, &#39;posting&#39;, &#39;ate&#39;, &#39;garbage&#39;, &#39;worthless&#39;, &#39;my&#39;, &#39;is&#39;, &#39;quit&#39;] [1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0] 结果分析：我们将dataSet[0]即第一条信息[‘my’, ‘dog’, ‘has’, ‘flea’, ‘problems’, ‘help’, ‘please’]构建词集模型，词特征集为[‘dog’, ‘to’, ‘take’, ‘park’, ‘licks’, ‘has’, ‘help’, ‘stupid’, ‘him’, ‘so’, ‘not’, ‘love’, ‘buying’, ‘problems’, ‘cute’, ‘stop’, ‘steak’, ‘how’, ‘flea’, ‘maybe’, ‘food’, ‘I’, ‘please’, ‘dalmation’, ‘mr’, ‘posting’, ‘ate’, ‘garbage’, ‘worthless’, ‘my’, ‘is’, ‘quit’]。结果显示[1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0]。即词特征集dog在dataSet[0]中，标记为1，to不在则保留原始的0.以此类推。我们也可以查看dataSet[1]等数据结果。 数据样本分析仅仅如上所述？当然不是，本例子中数据量比较小，容易分析。当数据量比较大，特征数以万计之时，人工分析就显得捉襟见肘了。我们可以采用图形化分析方法，根据具体业务需求，可以选择基于python自带的matplotlib可视化分析、或者其他图形可视化工具进行平面或多维数据分析，然后便于特征的选择。 如果是中文分词，我们还可以对词性进行分析，然后选择相应的词性特征，比如名词、动词、地名、人名、机构名等等，对虚词、助词等进行过滤，一方面达到数据降维另一方面防止模型训练拟合化等问题。 训练模型现在已经知道了一个词是否出现在一篇文档中，也知道该文档所属的类别。接下来我们重写贝叶斯准则，将之前的 x, y 替换为 w. 粗体的 w 表示这是一个向量，即它由多个值组成。在这个例子中，数值个数与词汇表中的词个数相同。 p(c_i|w)= \frac{p(w \mid c_i)p(c)} {p(w)}我们使用上述公式，对每个类计算该值，然后比较这两个概率值的大小。根据上述公式可知，我们右边的式子等同于左边的式子，由于对于每个\(c_i\)，\(P(w)\)是固定的。并且我们只需要比较左边式子值的大小来决策分类，那么我们就可以简化为通过比较右边分子值得大小来做决策分类。首先可以通过类别 i (侮辱性留言或者非侮辱性留言)中的文档数除以总的文档数来计算概率\(p(c_i)\)。接下来计算\(p(w|c_i)\)，这里就要用到朴素贝叶斯假设。如果将 w 展开为一个个独立特征，那么就可以将上述概率写作\(p(w_0,w_1,w_2…w_n|c_i)\)。这里假设所有词都互相独立，该假设也称作条件独立性假设（例如 A 和 B 两个人抛骰子，概率是互不影响的，也就是相互独立的，A 抛 2点的同时 B 抛 3 点的概率就是 1/6 * 1/6），它意味着可以使用\( p(w_0|c_i)p(w_1|c_i)p(w_2|;c_i)…p(w_n|c_i)\)来计算上述概率，这样就极大地简化了计算的过程。具体代码实现如下： ‘’’朴素贝叶斯分类器训练函数’’’def _trainNB0(trainMatrix, trainCategory): numTrainDocs = len(trainMatrix) # 文件数 numWords = len(trainMatrix[0]) # 单词数 # 侮辱性文件的出现概率，即trainCategory中所有的1的个数， # 代表的就是多少个侮辱性文件，与文件的总数相除就得到了侮辱性文件的出现概率 pAbusive = sum(trainCategory) / float(numTrainDocs) # 构造单词出现次数列表 p0Num = zeros(numWords) # [0,0,0,.....] p1Num = zeros(numWords) # [0,0,0,.....] p0Denom = 0.0;p1Denom = 0.0 # 整个数据集单词出现总数 for i in range(numTrainDocs): # 遍历所有的文件，如果是侮辱性文件，就计算此侮辱性文件中出现的侮辱性单词的个数 if trainCategory[i] == 1: p1Num += trainMatrix[i] #[0,1,1,....]-&gt;[0,1,1,...] p1Denom += sum(trainMatrix[i]) else: # 如果不是侮辱性文件，则计算非侮辱性文件中出现的侮辱性单词的个数 p0Num += trainMatrix[i] p0Denom += sum(trainMatrix[i]) # 类别1，即侮辱性文档的[P(F1|C1),P(F2|C1),P(F3|C1),P(F4|C1),P(F5|C1)....]列表 # 即 在1类别下，每个单词出现次数的占比 p1Vect = p1Num / p1Denom# [1,2,3,5]/90-&gt;[1/90,...] # 类别0，即正常文档的[P(F1|C0),P(F2|C0),P(F3|C0),P(F4|C0),P(F5|C0)....]列表 # 即 在0类别下，每个单词出现次数的占比 p0Vect = p0Num / p0Denom return p0Vect, p1Vect, pAbusive &lt;/pre&gt;代码分析：本方法参数分别是文档特征向量矩阵和文档类别向量矩阵。首先计算侮辱性文档占总文档的概率，然后计算正常文档下特征词的概率向量和侮辱性特征词的向量，为了更好理解上面的代码我们看下运行p0V,p1V,pAb=_trainNB0(trainMatrix,Classlabels)结果： 词汇表集 [&#39;I&#39;, &#39;cute&#39;, &#39;help&#39;, &#39;dalmation&#39;, &#39;please&#39;, &#39;has&#39;, &#39;my&#39;, &#39;him&#39;, &#39;worthless&#39;, &#39;problems&#39;, &#39;so&#39;, &#39;mr&#39;, &#39;flea&#39;, &#39;love&#39;, &#39;take&#39;, &#39;stupid&#39;, &#39;dog&#39;, &#39;park&#39;, &#39;how&#39;, &#39;quit&#39;, &#39;buying&#39;, &#39;posting&#39;, &#39;steak&#39;, &#39;maybe&#39;, &#39;to&#39;, &#39;is&#39;, &#39;ate&#39;, &#39;not&#39;, &#39;garbage&#39;, &#39;food&#39;, &#39;stop&#39;, &#39;licks&#39;] 各条评论特征向量，其中1,3,5条为类别0；2,4,6条为类别1 [0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0] [1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0] [0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1] [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0] 类别0下特征词条件概率 [0.04166667 0.04166667 0.04166667 0.04166667 0.04166667 0.04166667 0.125 0.08333333 0. 0.04166667 0.04166667 0.04166667 0.04166667 0.04166667 0. 0. 0.04166667 0. 0.04166667 0. 0. 0. 0.04166667 0. 0.04166667 0.04166667 0.04166667 0. 0. 0. 0.04166667 0.04166667] 类别1下特征词条件概率 [0. 0. 0. 0. 0. 0. 0. 0.05263158 0.10526316 0. 0. 0. 0. 0. 0.05263158 0.15789474 0.10526316 0.05263158 0. 0.05263158 0.05263158 0.05263158 0. 0.05263158 0.05263158 0. 0. 0.05263158 0.05263158 0.05263158 0.05263158 0. ] 0.5 结果分析：结合结果我们去理解上面的训练模型代码。首先最后一个是0.5代表侮辱性文档占全部文档的50%即一半，实际上我们标记3个正常评论词条，3个非正常的，这个显然正确。其次，第一次词I在类别1中出现0次，在类别0中出现1次。对应的条件概率分别是0.04166667和0 测试算法在利用贝叶斯分类器对文档进行分类时，要计算多个概率的乘积以获得文档属于某个类别的概率，即计算\(p(w_0|1)p(w_1|1)p(w_2|1)\)。如果其中一个概率值为 0，那么最后的乘积也为 0。为降低这种影响，可以将所有词的出现数初始化为 1，并将分母初始化为 2 （取1 或 2 的目的主要是为了保证分子和分母不为0，大家可以根据业务需求进行更改）。另一个遇到的问题是下溢出，这是由于太多很小的数相乘造成的。当计算乘积 \( p(w_0|c_i)p(w_1|c_i)p(w_2|c_i)…p(w_n|c_i)\)时，由于大部分因子都非常小，所以程序会下溢出或者得到不正确的答案。（用 Python 尝试相乘许多很小的数，最后四舍五入后会得到 0）。一种解决办法是对乘积取自然对数。在代数中有 ln(a * b) = ln(a) + ln(b), 于是通过求对数可以避免下溢出或者浮点数舍入导致的错误。同时，采用自然对数进行处理不会有任何损失。 下图给出了函数 f(x) 与 ln(f(x)) 的曲线。可以看出，它们在相同区域内同时增加或者减少，并且在相同点上取到极值。它们的取值虽然不同，但不影响最终结果。 根据朴素贝叶斯公式，我们观察分子\(p(w_i|c_i)P(c_i)\)进行条件概率连乘时候，由于有条件概率极小或者为0，最后导致结果为0 ，显然不符合我们预期结果，因此对训练模型进行优化，其优化代码如下： ‘’’训练数据优化版本’’’def trainNB0(trainMatrix, trainCategory): numTrainDocs = len(trainMatrix) # 总文件数 numWords = len(trainMatrix[0]) # 总单词数 pAbusive = sum(trainCategory) / float(numTrainDocs) # 侮辱性文件的出现概率 # 构造单词出现次数列表,p0Num 正常的统计,p1Num 侮辱的统计 # 避免单词列表中的任何一个单词为0，而导致最后的乘积为0，所以将每个单词的出现次数初始化为 1 p0Num = ones(numWords)#[0,0......]-&gt;[1,1,1,1,1.....],ones初始化1的矩阵 p1Num = ones(numWords) # 整个数据集单词出现总数，2.0根据样本实际调查结果调整分母的值（2主要是避免分母为0，当然值可以调整） # p0Denom 正常的统计 # p1Denom 侮辱的统计 p0Denom = 2.0 p1Denom = 2.0 for i in range(numTrainDocs): if trainCategory[i] == 1: p1Num += trainMatrix[i] # 累加辱骂词的频次 p1Denom += sum(trainMatrix[i]) # 对每篇文章的辱骂的频次 进行统计汇总 else: p0Num += trainMatrix[i] p0Denom += sum(trainMatrix[i]) # 类别1，即侮辱性文档的[log(P(F1|C1)),log(P(F2|C1)),log(P(F3|C1)),log(P(F4|C1)),log(P(F5|C1))....]列表,取对数避免下溢出或浮点舍入出错 p1Vect = log(p1Num / p1Denom) # 类别0，即正常文档的[log(P(F1|C0)),log(P(F2|C0)),log(P(F3|C0)),log(P(F4|C0)),log(P(F5|C0))....]列表 p0Vect = log(p0Num / p0Denom) return p0Vect, p1Vect, pAbusive &lt;/pre&gt;我们再看生成条件概率结果如下： [-2.56494936 -2.15948425 -3.25809654 -2.56494936 -3.25809654 -3.25809654 -2.56494936 -2.56494936 -3.25809654 -2.56494936 -3.25809654 -3.25809654 -2.56494936 -2.56494936 -2.56494936 -3.25809654 -2.56494936 -2.56494936 -2.56494936 -2.56494936 -1.87180218 -2.56494936 -3.25809654 -2.56494936 -2.56494936 -2.56494936 -2.56494936 -3.25809654 -3.25809654 -2.56494936 -3.25809654 -2.56494936] [-3.04452244 -2.35137526 -2.35137526 -3.04452244 -2.35137526 -2.35137526 -3.04452244 -3.04452244 -1.94591015 -2.35137526 -2.35137526 -2.35137526 -3.04452244 -2.35137526 -3.04452244 -1.65822808 -1.94591015 -3.04452244 -3.04452244 -3.04452244 -3.04452244 -3.04452244 -2.35137526 -3.04452244 -3.04452244 -3.04452244 -3.04452244 -2.35137526 -2.35137526 -3.04452244 -2.35137526 -3.04452244] 0.5 使用算法对社区留言板言论进行分类 构建朴素贝叶斯分类函数 将乘法转换为加法乘法： 加法： ‘’’def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1): # 计算公式 log(P(F1|C))+log(P(F2|C))+....+log(P(Fn|C))+log(P(C)) # 使用 NumPy 数组来计算两个向量相乘的结果，这里的相乘是指对应元素相乘，即先将两个向量中的第一个元素相乘，然后将第2个元素相乘，以此类推。这里的 vec2Classify * p1Vec 的意思就是将每个词与其对应的概率相关联起来 p1 = sum(vec2Classify * p1Vec) + log(pClass1) p0 = sum(vec2Classify * p0Vec) + log(1.0 - pClass1) if p1 &gt; p0: return 1 else: return 0 &lt;/pre&gt; 测试朴素贝叶斯算法 结合上面分析流程和实现方法，我们综合测试朴素贝叶斯对评论信息分类如下： ‘’’朴素贝叶斯算法屏蔽社区留言板的侮辱性言论的应用’’’def testingNB(): # 1. 加载数据集 dataSet, Classlabels = loadDataSet() # 2. 创建单词集合 myVocabList = createVocabList(dataSet) # 3. 计算单词是否出现并创建数据矩阵 trainMat = [] for postinDoc in dataSet: # 返回m*len(myVocabList)的矩阵， 记录的都是0，1信息 trainMat.append(setOfWords2Vec(myVocabList, postinDoc)) # print(&#39;test&#39;,len(array(trainMat)[0])) # 4. 训练数据 p0V, p1V, pAb = trainNB0(array(trainMat), array(Classlabels)) # 5. 测试数据 testEntry = [&#39;love&#39;, &#39;my&#39;, &#39;dalmation&#39;] thisDoc = array(setOfWords2Vec(myVocabList, testEntry)) print(testEntry, &#39;分类结果是: &#39;, classifyNB(thisDoc, p0V, p1V, pAb)) testEntry = [&#39;stupid&#39;, &#39;garbage&#39;] thisDoc = array(setOfWords2Vec(myVocabList, testEntry)) print(testEntry, &#39;分类结果是: &#39;, classifyNB(thisDoc, p0V, p1V, pAb)) &lt;/pre&gt;运行结果如下： [&#39;love&#39;, &#39;my&#39;, &#39;dalmation&#39;] 分类结果是: 0 [&#39;stupid&#39;, &#39;garbage&#39;] 分类结果是: 1 案例场景2: 对社区留言板言论进行分类项目概述我们运行朴素贝叶斯分类进行电子邮件垃圾过滤。在文档分类中，整个文档（如一封电子邮件）是实例，而电子邮件中的某些元素则构成特征。我们可以观察文档中出现的词，并把每个词作为一个特征，而每个词的出现或者不出现作为该特征的值，这样得到的特征数目就会跟词汇表中的词的数目一样多。 本项目开发流程如下： 收集数据: 提供文本文件 准备数据: 将文本文件解析成词条向量 分析数据: 检查词条确保解析的正确性 训练算法: 使用我们之前建立的 trainNB() 函数 测试算法: 使用朴素贝叶斯进行交叉验证 使用算法: 构建一个完整的程序对一组文档进行分类，将错分的文档输出到屏幕上 收集数据并预处理邮件格式内容如下： 对邮件进行读取实现如下： '''读取文本''' def testParseTest(): print(textParse(open('./email/ham/1.txt').read())) 准备数据对读取的文本进行词条向量化，其实现如下： ‘’’接收一个大字符串并将其解析为字符串列表’’’def textParse(bigString): import re # 使用正则表达式来切分句子，其中分隔符是除单词、数字外的任意字符串 listOfTokens = re.split(r&#39;\W*&#39;, bigString) return [tok.lower() for tok in listOfTokens if len(tok) &gt; 2] &lt;/pre&gt; 分析数据这个部分在案例场景1进行了详细描述，此处不在赘述。 训练算法此处，我们去调用在场景1优化过的朴素贝叶斯训练模型trainNB0() 函数，这里也是一劳永逸的方法。还可以对数据预处理等进行封装。 测试算法本测试方法中使用的数据集，即文档可以参见下文代码下载。采用方法跟场景1基本类似，这里不作代码解析。具体实现代码如下： ‘’’对贝叶斯垃圾邮件分类器进行自动化处理。’’’def spamTest(): docList = [];classList = [];fullText = [] # 文档列表、类别列表、文本特征 for i in range(1, 26): # 总共25个文档 # 切分，解析数据，并归类为 1 类别 wordList = textParse(open(&#39;./email/spam/%d.txt&#39; % i).read()) docList.append(wordList) classList.append(1) # 切分，解析数据，并归类为 0 类别 wordList = textParse(open(&#39;./email/ham/%d.txt&#39; % i,encoding=&#39;UTF-8&#39;).read()) docList.append(wordList) classList.append(0) fullText.extend(wordList) # 创建词汇表 vocabList = createVocabList(docList) trainingSet = list(range(50)) # 词汇表文档索引 testSet = [] # 随机取 10 个邮件用来测试 for i in range(10): # random.uniform(x, y) 随机生成一个范围为 x - y 的实数 randIndex = int(random.uniform(0, len(trainingSet))) testSet.append(trainingSet[randIndex]) # 随机抽取测试样本 del(trainingSet[randIndex]) # 训练集中删除选择为测试集的文档 trainMat = [];trainClasses = [] # 训练集合训练标签 for docIndex in trainingSet: trainMat.append(setOfWords2Vec(vocabList, docList[docIndex])) trainClasses.append(classList[docIndex]) p0V, p1V, pSpam = trainNB0(array(trainMat), array(trainClasses)) errorCount = 0 for docIndex in testSet: wordVector = setOfWords2Vec(vocabList, docList[docIndex]) if classifyNB(array(wordVector), p0V, p1V, pSpam) != classList[docIndex]: errorCount += 1 print(&#39;the errorCount is: &#39;, errorCount) print(&#39;the testSet length is :&#39;, len(testSet)) print(&#39;the error rate is :&#39;, float(errorCount)/len(testSet)) &lt;/pre&gt;运行结果如下： the errorCount is: 2 the testSet length is : 10 the error rate is : 0.2 案例场景3: 使用朴素贝叶斯分类器从个人广告中获取区域倾向项目概述广告商往往想知道关于一个人的一些特定人口统计信息，以便能更好地定向推销广告。我们将分别从美国的两个城市中选取一些人，通过分析这些人发布的信息，来比较这两个城市的人们在广告用词上是否不同。如果结论确实不同，那么他们各自常用的词是哪些，从人们的用词当中，我们能否对不同城市的人所关心的内容有所了解。 开发流程如下： 收集数据: 从 RSS 源收集内容，这里需要对 RSS 源构建一个接口 准备数据: 将文本文件解析成词条向量 分析数据: 检查词条确保解析的正确性 训练算法: 使用我们之前建立的 trainNB0() 函数 测试算法: 观察错误率，确保分类器可用。可以修改切分程序，以降低错误率，提高分类结果 使用算法: 构建一个完整的程序，封装所有内容。给定两个 RSS 源，改程序会显示最常用的公共词 收集数据从 RSS 源收集内容，这里需要对 RSS 源构建一个接口，也就是导入 RSS 源，我们使用 python 下载文本，在 点击下载地址 下浏览相关文档，安装 feedparse，首先解压下载的包，并将当前目录切换到解压文件所在的文件夹，然后在 python 提示符下输入：python setup.py install 准备数据 文档词袋模型 我们将每个词的出现与否作为一个特征，这可以被描述为 词集模型(set-of-words model)。如果一个词在文档中出现不止一次，这可能意味着包含该词是否出现在文档中所不能表达的某种信息，这种方法被称为 词袋模型(bag-of-words model)。在词袋中，每个单词可以出现多次，而在词集中，每个词只能出现一次。为适应词袋模型，需要对函数 setOfWords2Vec() 稍加修改，修改后的函数为 bagOfWords2Vec() 。 如下给出了基于词袋模型的朴素贝叶斯代码。它与函数 setOfWords2Vec() 几乎完全相同，唯一不同的是每当遇到一个单词时，它会增加词向量中的对应值，而不只是将对应的数值设为 1 。 这部分在场景1中已经构建完成，并进行了阐述。 分析数据这个部分在案例场景1进行了详细描述，此处不在赘述。 训练算法此处，我们去调用在场景1优化过的朴素贝叶斯训练模型trainNB0() 函数，这里也是一劳永逸的方法。还可以对数据预处理等进行封装。 测试算法观察错误率，确保分类器可用。可以修改切分程序，以降低错误率，提高分类结果。其具体实现如下： ‘’’RSS源分类器及高频词去除函数’’’def calcMostFreq(vocabList,fullText): import operator freqDict={} for token in vocabList: #遍历词汇表中的每个词 freqDict[token]=fullText.count(token) #统计每个词在文本中出现的次数 sortedFreq=sorted(freqDict.items(),key=operator.itemgetter(1),reverse=True) #根据每个词出现的次数从高到底对字典进行排序 return sortedFreq[:30] #返回出现次数最高的30个单词 def localWords(feed1,feed0): # import feedparser # feedparser是python中最常用的RSS程序库 docList=[];classList=[];fullText=[] minLen=min(len(feed1[&#39;entries&#39;]),len(feed0[&#39;entries&#39;])) # entries内容无法抓取，网站涉及反爬虫技术 print(len(feed1[&#39;entries&#39;]),len(feed0[&#39;entries&#39;])) for i in range(minLen): wordList=textParse(feed1[&#39;entries&#39;][i][&#39;summary&#39;]) #每次访问一条RSS源 docList.append(wordList) fullText.extend(wordList) classList.append(1) wordList=textParse(feed0[&#39;entries&#39;][i][&#39;summary&#39;]) docList.append(wordList) fullText.extend(wordList) classList.append(0) vocabList=createVocabList(docList) top30Words=calcMostFreq(vocabList,fullText) for pairW in top30Words: if pairW[0] in vocabList:vocabList.remove(pairW[0]) #去掉出现次数最高的那些词 trainingSet=range(2*minLen);testSet=[] for i in range(20): randIndex=int(random.uniform(0,len(trainingSet))) testSet.append(trainingSet[randIndex]) del(trainingSet[randIndex]) trainMat=[];trainClasses=[] for docIndex in trainingSet: trainMat.append(bagOfWords2VecMN(vocabList,docList[docIndex])) trainClasses.append(classList[docIndex]) p0V,p1V,pSpam=trainNB0(array(trainMat),array(trainClasses)) errorCount=0 for docIndex in testSet: wordVector=bagOfWords2VecMN(vocabList,docList[docIndex]) if classifyNB(array(wordVector),p0V,p1V,pSpam)!=classList[docIndex]: errorCount+=1 print(&#39;the error rate is:&#39;,float(errorCount)/len(testSet)) return vocabList,p0V,p1V &lt;/pre&gt;运行结果： ny = feedparser.parse(&#39;http://newyork.craigslist.org/stp/index.rss&#39;) sf = feedparser.parse(&#39;http://sfbay.craigslist.org/stp/index.rss&#39;) # print(ny) vocabList,pSF,pNY=localWords(ny,sf) 由于如上两个地址抓取，得到feed0[‘entries’]为空，所以没有进行结果分析，读者可以试用其他rss地址进行处理。如下是采用之前网站反爬虫抓取前的分析结果： vocabList,pSF,pNY=bayes.localWords(ny,sf) the error rate is: 0.2 vocabList,pSF,pNY=bayes.localWords(ny,sf) the error rate is: 0.3 vocabList,pSF,pNY=bayes.localWords(ny,sf) the error rate is: 0.55 参考文献 scikit中文社区：http://sklearn.apachecn.org/cn/0.19.0/ 中文维基百科：https://zh.wikipedia.org/wiki/ 文本分类特征选择：https://www.cnblogs.com/june0507/p/7601001.html GitHub：https://github.com/BaiNingchao/MachineLearning-1 图书：《机器学习实战》 图书：《自然语言处理理论与实战》 完整代码下载 源码请进【机器学习和自然语言QQ群：436303759】文件下载： 作者声明 本文版权归作者所有，旨在技术交流使用。未经作者同意禁止转载，转载后需在文章页面明显位置给出原文连接，否则相关责任自行承担。]]></content>
      <categories>
        <category>机器学习</category>
        <category>朴素贝叶斯</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Naive Bayes</tag>
        <tag>ML</tag>
        <tag>文本分类</tag>
        <tag>实战案例</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一步步教你轻松学朴素贝叶斯模型算法理论篇1]]></title>
    <url>%2F2018%2F09%2F19%2F%E4%B8%80%E6%AD%A5%E6%AD%A5%E6%95%99%E4%BD%A0%E8%BD%BB%E6%9D%BE%E5%AD%A6%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%A8%A1%E5%9E%8B%E7%AE%97%E6%B3%95%E7%90%86%E8%AE%BA%E7%AF%871%2F</url>
    <content type="text"><![CDATA[摘要：朴素贝叶斯模型是机器学习常用的模型算法之一，其在文本分类方面简单易行，且取得不错的分类效果。所以很受欢迎，对于朴素贝叶斯的学习，本文首先介绍理论知识即朴素贝叶斯相关概念和公式推导，为了加深理解，采用一个维基百科上面性别分类例子进行形式化描述。然后通过编程实现朴素贝叶斯分类算法，并在屏蔽社区言论、垃圾邮件、个人广告中获取区域倾向等几个方面进行应用，包括创建数据集、数据预处理、词集模型和词袋模型、朴素贝叶斯模型训练和优化等。然后结合复旦大学新闻语料进行朴素贝叶斯的应用。最后，大家熟悉其原理和实现之后，采用机器学习sklearn包进行实现和优化。由于篇幅较长，采用理论理解、案例实现、sklearn优化三个部分进行学习。（本文原创，转载必须注明出处.） 朴素贝叶斯理论朴素贝叶斯概述朴素贝叶斯是一种构建分类器的简单方法。该分类器模型会给问题实例分配用特征值表示的类标签，类标签取自有限集合。所有朴素贝叶斯分类器都假定样本每个特征与其他特征都不相关。 特征独立理解的例子：如果一种水果其具有红，圆，直径大概3英寸等特征，该水果可以被判定为是苹果。尽管这些特征相互依赖或者有些特征由其他特征决定，然而朴素贝叶斯分类器认为这些属性在判定该水果是否为苹果的概率分布上独立的。 尽管是带着这些朴素思想和过于简单化的假设，但朴素贝叶斯分类器在很多复杂的现实情形中仍能够获取相当好的效果。朴素贝叶斯分类器的一个优势在于只需要根据少量的训练数据估计出必要的参数（变量的均值和方差）。 朴素贝叶斯模型朴素贝叶斯方法是基于贝叶斯定理的一组有监督学习算法，即“简单”地假设每对特征之间相互独立。 给定一个类别 \(y\) 和一个从\(x_1\)&gt;到\(x_n\) 的相关的特征向量，贝叶斯定理阐述了以下关系: 使用简单(naive)的假设-每对特征之间都相互独立: 对于所有的 math: i ，这个关系式可以简化为 由于在给定的输入中\( P(x_1, \dots, x_n)\) 是一个常量，我们使用下面的分类规则: 我们可以使用最大后验概率(Maximum A Posteriori, MAP) 来估计 \(P(y)\)和\(P(x_i\mid y)\); 前者是训练集中类别 y 的相对频率。各种各样的的朴素贝叶斯分类器的差异大部分来自于处理 \( P(x_i \mid y)\) 分布时的所做的假设不同。尽管其假设过于简单，在很多实际情况下，朴素贝叶斯工作得很好，特别是文档分类和垃圾邮件过滤。相比于其他更复杂的方法，朴素贝叶斯学习器和分类器非常快。 朴素贝叶斯算法思想假设有一个数据集，它由两类数据组成，数据分布如下图所示： 我们现在用\(p_1(x,y)\) 表示数据点 (x,y) 属于类别 1（图中用圆点表示的类别）的概率，用 \(p_2(x,y)\) 表示数据点 (x,y) 属于类别 2（图中三角形表示的类别）的概率，那么对于一个新数据点 (x,y)，可以用下面的规则来判断它的类别： 如果\( p_1(x,y)&gt;p_2(x,y)\)，那么类别为1 如果 \( p_1(x,y)&lt;p_2(x,y)\)，那么类别为2 也就是说，我们会选择高概率对应的类别。这就是贝叶斯决策理论的核心思想，即选择具有最高概率的决策。 朴素贝叶斯工作原理提取所有文档中的词条并进行去重 获取文档的所有类别 计算每个类别中的文档数目 对每篇训练文档: 对每个类别: 如果词条出现在文档中--&gt;增加该词条的计数值（for循环或者矩阵相加） 增加所有词条的计数值（此类别下词条总数） 对每个类别: 对每个词条: 将该词条的数目除以总词条数目得到的条件概率（P(词条|类别)） 返回该文档属于每个类别的条件概率（P(类别|文档的所有词条)） 朴素贝叶斯算法流程收集数据: 可以使用任何方法。 准备数据: 需要数值型或者布尔型数据。 分析数据: 有大量特征时，绘制特征作用不大，此时使用直方图效果更好。 训练算法: 计算不同的独立特征的条件概率。 测试算法: 计算错误率。 使用算法: 一个常见的朴素贝叶斯应用是文档分类。可以在任意的分类场景中使用朴素贝叶斯分类器，不一定非要是文本。 朴素贝叶斯优缺点优点: 在数据较少的情况下仍然有效，可以处理多类别问题。 缺点: 对于输入数据的准备方式较为敏感。 适用数据类型: 标称型数据。 案例描述：形式化理解朴素贝叶斯性别分类问题描述通过一些测量的特征，包括身高、体重、脚的尺寸，判定一个人是男性还是女性。 训练数据 性别 身高(英尺) 体重(磅) 脚的尺寸(英寸) 男 6 180 12 男 5.92 190 11 男 5.58 170 12 男 5.92 165 10 女 5 100 6 女 5.5 150 8 女 5.42 130 7 女 5.75 150 9 假设训练集样本的特征满足高斯分布，得到下表： 性别 均值(身高) 方差(身高) 均值(体重) 方差(体重) 均值(脚的尺寸) 方差(脚的尺寸) 男性 5.855 3.5033e-02 176.25 1.2292e+02 11.25 9.1667e-01 女性 5.4175 9.7225e-02 132.5 5.5833e+02 7.5 1.6667e+00 我们认为两种类别是等概率的，也就是P(male)= P(female) = 0.5。在没有做辨识的情况下就做这样的假设并不是一个好的点子。但我们通过数据集中两类样本出现的频率来确定P(C)，我们得到的结果也是一样的。 测试数据以下给出一个待分类是男性还是女性的样本。 性别 身高(英尺) 体重(磅) 脚的尺寸(英尺) sample 6 130 8 我们希望得到的是男性还是女性哪类的后验概率大。男性的后验概率通过下面式子来求取 女性的后验概率通过下面式子来求取 证据因子（通常是常数）用来对各类的后验概率之和进行归一化. 证据因子是一个常数（在正态分布中通常是正数），所以可以忽略。接下来我们来判定这样样本的性别。 其中 是训练集样本的正态分布参数. 注意，这里的值大于1也是允许的 – 这里是概率密度而不是概率，因为身高是一个连续的变量. 集样本的正态分布参数. 注意，这里的值大于1也是允许的 – 这里是概率密度而不是概率，因为身高是一个连续的变量. 模型预测结果由于女性后验概率的分子比较大，所以我们预计这个样本是女性。 参考文献 scikit中文社区：http://sklearn.apachecn.org/cn/0.19.0/ 中文维基百科：https://zh.wikipedia.org/wiki/ 文本分类特征选择：https://www.cnblogs.com/june0507/p/7601001.html GitHub：https://github.com/BaiNingchao/MachineLearning-1 图书：《机器学习实战》 图书：《自然语言处理理论与实战》 完整代码下载 源码请进【机器学习和自然语言QQ群：436303759】文件下载： 作者声明 本文版权归作者所有，旨在技术交流使用。未经作者同意禁止转载，转载后需在文章页面明显位置给出原文连接，否则相关责任自行承担。]]></content>
      <categories>
        <category>机器学习</category>
        <category>朴素贝叶斯</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>自然语言处理</tag>
        <tag>Naive Bayes</tag>
        <tag>ML</tag>
        <tag>文本分类</tag>
        <tag>理论学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo-GitHub搭建个人博客配置教程]]></title>
    <url>%2F2018%2F09%2F19%2FHexo-GitHub%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E9%85%8D%E7%BD%AE%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[摘要：创建一个自定义个人博客，即可以便于知识的记录，又可以分享知识。然而，诸多技术社区虽有便捷的优点，同时也存在限制性和专业化的问题。自己从头写网站成本较高，故而作者采用github建站，既可以支持markdown文档编写，又可以与github账号打通，便于资料上传保存。仅仅建站完成还是不够美观和功能扩展。这里github结合诸多插件可以实现，本文重点介绍Hexo的配置操作，具体以下实现可以根据需要选择。（本文原创，转载注明出处.） 前提准备 安装Node.js Node.js下载地址：https://nodejs.org/en/download/ 安装Git软件 Git软件下载地址：https://git-scm.com/download 安装hexo框架 初始化hexo Hexo官方网站：https://hexo.io/zh-cn/在电脑合适的位置新建一个文件夹存放博客。本文中取名为MyBlog文件夹。控制台命令行使用cd命令进入到Blog文件夹，输入以下命令进行初始化： hexo init # 初始化 安装依赖包: hexo install # 安装依赖包 测试本地运行 hexo g # 等同于hexo generate，生成静态文件 hexo s # 等同于hexo server，在本地服务器运行 部署到Coding以及GitHub上 注册、登录、创建仓库、打开_config.yml到最后deploy选项： # Deployment ## Docs: https://hexo.io/docs/deployment.html deploy: type: git repo: github: https://github.com/DimpleFeng/dimplefeng.github.io.git,master coding: https://git.coding.net/DimpleFeng/test.git,master 部署部署之前需要安装git部署插件，否则会提示Deployer not found错误。 npm install hexo-deployer-git --save 安装完毕后控制台输入: hexo g -d hexo s 随后访问你的以下网址（注意替换）：yourName.github.io 写一篇新的博文 两种方法： 在博文根目录的Source文件夹的post文件夹下直接新建一个md文件 在博文根目录打来PowerShell，然后输入hexo new ‘你的标题’回车在你的post文件夹下就新建了一个博文，打开编辑即可。然后使用hexo g -d部署到线上。 $ hexo clean $ hexo g -d 修改主题 主题官网：https://hexo.io/themes/ Administrator@PC-20160724ASED MINGW64 /e/MyBlog $ git clone https://github.com/theme-next/hexo-theme-next themes/next 修改站点配置文件_config.yml # Extensions ## Plugins: https://hexo.io/plugins/ ## Themes: https://hexo.io/themes/ theme: next next四种主题选择，打开 主题配置文件 找到Scheme Settings # Schemes # scheme: Muse # scheme: Mist # scheme: Pisces scheme: Gemini 设置语言 修改站点配置文件_config.yml:language: zh-Hans 目录 新建一个页面，命名为tags。命令如下：hexo new page “tags”。在myBlog/source下会新生成一个新的文件夹tags，在该文件夹下会有一个index.md文件 --- title: 标签测试文章 date: 2018-09-17 20:31:20 type: tags tags: - Test - Tag --- 新建一个页面，命名为categories。命令如下：hexo new page “categories”。在myBlog/source下会新生成一个新的文件夹categories，在该文件夹下会有一个index.md文件 --- title: categories date: 2018-09-17 20:31:40 type: categories --- 新建一个页面，命名为comments。命令如下：hexo new page “comments”。在myBlog/source下会新生成一个新的文件夹comments，在该文件夹下会有一个index.md文件 --- title: comments date: 2018年9月18日16:16:39 type: &quot;comments&quot; comments: true --- 在菜单中添加链接。编辑主题的 themes/next/_config.yml ，添加tags到menu中，如下: menu: home: / archives: /archives/ categories: /categories/ tags: /tags/ 高级设置 设置现在更多 &lt;!--more--&gt; 头像设置 打开 主题配置文件_config.yml 找到Sidebar Avatar字段 # Sidebar Avatar url: ../images/header.jpg 这是头像的路径，只需把你的头像命名为header.jpg（随便命名）放入themes/next/source/images中，将avatar的路径名改成你的头像名就OK啦！ 设置RSS 1、先安装 hexo-generator-feed 插件 $ npm install hexo-generator-feed --save 2、打开 站点配置文件 找到Extensions在下面添加 # RSS订阅 feed: type: atom path: atom.xml limit: 20 hub: content: content_limit: 140 content_limit_delim: &#39; &#39; 3、打开 主题配置文件 找到rss，设置为 rss: /atom.xml 添加搜索功能 1、安装 hexo-generator-searchdb 插件 $ npm install hexo-generator-searchdb --save 2、打开 站点配置文件 找到Extensions在下面添加 # 搜索 search: path: search.xml field: post format: html limit: 10000 3、打开 主题配置文件 找到Local search，将enable设置为true 修改文章内链接文本样式 打开文件 themes/next/source/css/_common/components/post/post.styl，在末尾添加 .post-body p a { color: #0593d3; border-bottom: none; border-bottom: 1px solid #0593d3; &amp;:hover { color: #fc6423; border-bottom: none; border-bottom: 1px solid #fc6423; } } 其中选择 .post-body 是为了不影响标题，选择 p 是为了不影响首页“阅读全文”的显示样式,颜色可以自己定义。 设置网站缩略图标 把logo图片（png或jpg格式，不是favicon.ico）放在themes/next/source/images里，然后打开 主题配置文件 找到favicon，将small、medium、apple_touch_icon三个字段的值都设置成/images/图片名.jpg就可以了，其他字段都注释掉。 favicon: small: /images/logo.png medium: /images/logo.png apple_touch_icon: /images/logo.png 添加评论 添加站点访问计数 去掉文章目录标题的自动编号 我们自己写文章的时候一般都会自己带上标题编号，但是默认的主题会给我们带上编号，很是别扭，如何去掉呢？打开主题配置文件，找到 # Table Of Contents in the Sidebar toc: enable: true # Automatically add list number to toc. number: false # If true, all words will placed on next lines if header width longer then sidebar width. wrap: false 设置Fork me on Github 第一步选取适合自己的样式代码:https://github.com/blog/273-github-ribbons &lt;a href=&quot;https://github.com/you&quot;&gt;&lt;img style=&quot;position: absolute; top: 0; right: 0; border: 0;&quot; src=&quot;https://s3.amazonaws.com/github/ribbons/forkme_right_red_aa0000.png&quot; alt=&quot;Fork me on GitHub&quot;&gt;&lt;/a&gt; 在主题中进行配置/root/blog/themes/next/layout/_layout.swig文件中进行配置 /root表示的是根目录. next表示的是当前你使用的主题的样式. 具体的配置直接图片中展示: 注意的是: href 后面是自己的github的地址,记得修改哦! 文章的输入密码访问 第一步修改主题下面的文件,主要的是修改的主题下面的文件:themes-&gt;next-&gt;layout-&gt;_partials-&gt;head-&gt;head.swig &lt;script&gt; (function(){ if(&#39;{{ page.password }}&#39;){ if (prompt(&#39;请输入文章密码&#39;) !== &#39;{{ page.password }}&#39;){ alert(&#39;密码错误！&#39;); history.back(); } } })(); &lt;/script&gt; 放置位置 实现点击的桃心效果 添加打js代码地址: http://7u2ss1.com1.z0.glb.clouddn.com/love.js新建js代码放置,在/theme/next/source/js/src这个路劲下面,新建love.js,将上面的代码复制进去.配置 _layout.swig文件,在 themes/next/layout/_layout.swig 文件, 最后部分添加: &lt;!-- 页面点击小红心 --&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;/js/src/love.js&quot;&gt;&lt;/script&gt; 在_layout.swig位置的设置展示: Hexo之站点地图的搭建sitemap.xml 创建sitemap站点：hexo new page sitemap npm install hexo-generator-sitemap --save npm install hexo-generator-baidu-sitemap --save 如上设置,出现,sitemap.xml和baidusitemap.xml 表示站点文件生成. 之后就是百度站点地图的验证百度站长平台进行验证: https://ziyuan.baidu.com/dashboard/index 第一次会输入一些信息,姓名,职位等信息. 后面就是添加站点:网站的根目录在那里呢？ 在你的博客的本地根目录的Source文件夹内。 具体参照：https://blog.csdn.net/qq_32454537/article/details/79482914 以下功能配置及其参照文章 设置代码高亮主题 设置字体 侧边栏社交链接 开启打赏功能 友情链接 订阅微信公众号具体配置参照文章：http://theme-next.iissnan.com/theme-settings.html 修改文章后面标签的图标在主题下: themes/next/layout/_macro/post.swig文件:修改模板 /themes/next/layout/_macro/post.swig，搜索 rel=”tag”&gt;#，将 # 换成Hexo的个性化配置(一) https://blog.csdn.net/kunkun5love/article/details/79130956 配置文章分享功能https://blog.csdn.net/lanuage/article/details/78991798 配置访问统计 主题中查找 busuanzi_count 如果你使用的是NexT主题（其他主题类似），打开/theme/next/layout/_partial/footer.swig文件，拷贝下面的代码至文件的开头。 fatal: bad config file line 1 in .git/config 删除hexo路径下的.deploy_git后，问题解决了。http://ishareflower.com/2015-11-18/Git-deployment.html 配置ssh https://blog.csdn.net/Greenovia/article/details/60576985 代码高亮显示 https://blog.csdn.net/u011240016/article/details/79422448 分享功能https://blog.csdn.net/cl534854121/article/details/76121105 作者声明 本文版权归作者所有，旨在技术交流使用。未经作者同意禁止转载，转载后需在文章页面明显位置给出原文连接，否则相关责任自行承担。]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
</search>
